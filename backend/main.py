import os
import sys
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.middleware.trustedhost import TrustedHostMiddleware
from fastapi.responses import JSONResponse
import uvicorn
from motor.motor_asyncio import AsyncIOMotorClient
from pydantic import BaseModel
from bson import ObjectId
from typing import List, Optional, Dict, Any
import locale
import codecs
from datetime import datetime
from datetime import timedelta
import csv
from chatbot import chatbot_router, langgraph_router
from github import router as github_router
from routers.upload import router as upload_router

from routers.pdf_ocr import router as pdf_ocr_router


from similarity_service import SimilarityService
from embedding_service import EmbeddingService
from vector_service import VectorService

# Python í™˜ê²½ ì¸ì½”ë”© ì„¤ì •
# ì‹œìŠ¤í…œ ê¸°ë³¸ ì¸ì½”ë”©ì„ UTF-8ë¡œ ì„¤ì •
if sys.platform.startswith('win'):
    # Windows í™˜ê²½ì—ì„œ UTF-8 ê°•ì œ ì„¤ì •
    os.environ['PYTHONIOENCODING'] = 'utf-8'
    # ì½˜ì†” ì¶œë ¥ ì¸ì½”ë”© ì„¤ì •
    sys.stdout = codecs.getwriter('utf-8')(sys.stdout.detach())
    sys.stderr = codecs.getwriter('utf-8')(sys.stderr.detach())

# FastAPI ì•± ìƒì„±
app = FastAPI(title="AI ì±„ìš© ê´€ë¦¬ ì‹œìŠ¤í…œ API", version="1.0.0")

# CORS ì„¤ì •
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# í•œê¸€ ì¸ì½”ë”©ì„ ìœ„í•œ ë¯¸ë“¤ì›¨ì–´
@app.middleware("http")
async def add_charset_header(request, call_next):
    response = await call_next(request)
    
    # ëª¨ë“  JSON ì‘ë‹µì— UTF-8 ì¸ì½”ë”© ëª…ì‹œ
    if response.headers.get("content-type", "").startswith("application/json"):
        response.headers["content-type"] = "application/json; charset=utf-8"
    
    # í…ìŠ¤íŠ¸ ì‘ë‹µì—ë„ UTF-8 ì¸ì½”ë”© ëª…ì‹œ
    elif response.headers.get("content-type", "").startswith("text/"):
        if "charset" not in response.headers.get("content-type", ""):
            current_content_type = response.headers.get("content-type", "")
            response.headers["content-type"] = f"{current_content_type}; charset=utf-8"
    
    return response

# ë¼ìš°í„° ë“±ë¡
app.include_router(chatbot_router, prefix="/api/chatbot", tags=["chatbot"])
app.include_router(langgraph_router, prefix="/api/langgraph", tags=["langgraph"])
# í”„ë¡ íŠ¸ì—”ë“œ í˜¸í™˜ì„ ìœ„í•´ /api/langgraph-agent í”„ë¦¬í”½ìŠ¤ë„ ë™ì¼ ë¼ìš°í„°ë¡œ ë§ˆìš´íŠ¸
app.include_router(langgraph_router, prefix="/api/langgraph-agent", tags=["langgraph-agent"])
app.include_router(github_router, prefix="/api", tags=["github"])
app.include_router(upload_router, tags=["upload"])

app.include_router(pdf_ocr_router, prefix="/api/pdf-ocr", tags=["pdf_ocr"])


# MongoDB ì—°ê²°
MONGODB_URI = os.getenv("MONGODB_URI", "mongodb://localhost:27017/hireme")
client = AsyncIOMotorClient(MONGODB_URI)
db = client.hireme

# í™˜ê²½ ë³€ìˆ˜ì—ì„œ API í‚¤ ë¡œë“œ
GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY")
PINECONE_API_KEY = os.getenv("PINECONE_API_KEY") 
PINECONE_INDEX_NAME = os.getenv("PINECONE_INDEX_NAME", "resume-vectors")

# ì„œë¹„ìŠ¤ ì´ˆê¸°í™”
embedding_service = EmbeddingService()
vector_service = VectorService(
    api_key=PINECONE_API_KEY or "dummy-key",  # API í‚¤ê°€ ì—†ì–´ë„ ì„œë²„ ì‹œì‘ì€ ê°€ëŠ¥
    index_name=PINECONE_INDEX_NAME
)
similarity_service = SimilarityService(embedding_service, vector_service)

# Pydantic ëª¨ë¸ë“¤
class User(BaseModel):
    id: Optional[str] = None
    username: str
    email: str
    role: str = "user"
    created_at: Optional[datetime] = None

class Resume(BaseModel):
    id: Optional[str] = None
    resume_id: Optional[str] = None
    name: str
    position: str
    department: str
    experience: str
    skills: str
    growthBackground: str
    motivation: str
    careerHistory: str
    analysisScore: int = 0
    analysisResult: str = ""
    status: str = "pending"
    created_at: Optional[datetime] = None

class ResumeChunk(BaseModel):
    id: Optional[str] = None
    resume_id: str
    chunk_id: str
    text: str
    start_pos: int
    end_pos: int
    chunk_index: int
    field_name: Optional[str] = None  # growthBackground, motivation, careerHistory
    metadata: Optional[Dict[str, Any]] = None
    vector_id: Optional[str] = None  # Pinecone ë²¡í„° ID
    created_at: Optional[datetime] = None

class Interview(BaseModel):
    id: Optional[str] = None
    user_id: str
    company: str
    position: str
    date: datetime
    status: str = "scheduled"
    created_at: Optional[datetime] = None

# ì´ˆê¸° ë°ì´í„° ë¡œë”© ìœ í‹¸ë¦¬í‹°: DBê°€ ë¹„ì–´ìˆìœ¼ë©´ ë£¨íŠ¸ CSVì—ì„œ ì„í¬íŠ¸
async def seed_applicants_from_csv_if_empty() -> None:
    try:
        total_documents = await db.applicants.count_documents({})
        if total_documents > 0:
            return

        project_root_csv_path = os.path.abspath(
            os.path.join(os.path.dirname(__file__), "..", "hireme.applicants.csv")
        )
        if not os.path.exists(project_root_csv_path):
            return

        documents_to_insert = []
        with open(project_root_csv_path, mode="r", encoding="utf-8") as csv_file:
            reader = csv.DictReader(csv_file)
            for row in reader:
                document: Dict[str, Any] = {}

                # _id ì²˜ë¦¬: ê°€ëŠ¥í•˜ë©´ ObjectIdë¡œ ì €ì¥
                raw_id = row.get("_id")
                if raw_id and isinstance(raw_id, str) and len(raw_id) == 24:
                    try:
                        document["_id"] = ObjectId(raw_id)
                    except Exception:
                        document["_id"] = raw_id

                # resume_id ì²˜ë¦¬
                raw_resume_id = row.get("resume_id")
                if raw_resume_id and isinstance(raw_resume_id, str) and len(raw_resume_id) == 24:
                    try:
                        document["resume_id"] = ObjectId(raw_resume_id)
                    except Exception:
                        document["resume_id"] = raw_resume_id
                elif raw_resume_id:
                    document["resume_id"] = raw_resume_id

                # ë¬¸ìì—´ í•„ë“œë“¤: í•­ìƒ ë¬¸ìì—´ë¡œ ìºìŠ¤íŒ…
                string_fields = [
                    "name",
                    "position",
                    "department",
                    "experience",
                    "skills",
                    "growthBackground",
                    "motivation",
                    "careerHistory",
                    "analysisResult",
                    "status",
                ]
                for field_name in string_fields:
                    value = row.get(field_name, "")
                    document[field_name] = "" if value is None else str(value)

                # ìˆ«ì í•„ë“œ
                try:
                    document["analysisScore"] = int(row.get("analysisScore", "0") or 0)
                except Exception:
                    document["analysisScore"] = 0

                # created_at ì²˜ë¦¬
                created_at_raw = row.get("created_at")
                if created_at_raw:
                    try:
                        iso_candidate = created_at_raw.replace("Z", "+00:00")
                        document["created_at"] = datetime.fromisoformat(iso_candidate)
                    except Exception:
                        document["created_at"] = datetime.now()

                documents_to_insert.append(document)

        if documents_to_insert:
            print(f"ğŸ” ì‹œë“œ ëŒ€ìƒ ë¬¸ì„œ ìˆ˜: {len(documents_to_insert)}")
            await db.applicants.insert_many(documents_to_insert)
            new_count = await db.applicants.count_documents({})
            print(f"ğŸ“¥ CSVì—ì„œ {len(documents_to_insert)}ê±´ ì„í¬íŠ¸ ì™„ë£Œ â†’ í˜„ì¬ ì´ ë¬¸ì„œ ìˆ˜: {new_count}")
    except Exception as seed_error:
                    print(f"[ERROR] CSV ì„í¬íŠ¸ ì‹¤íŒ¨: {seed_error}")


def load_applicants_from_csv() -> List[Dict[str, Any]]:
    """DB ë¯¸ê°€ë™/ë¹„ì–´ìˆì„ ë•Œ CSVë¥¼ ì§ì ‘ ì½ì–´ ë°˜í™˜"""
    try:
        project_root_csv_path = os.path.abspath(
            os.path.join(os.path.dirname(__file__), "..", "hireme.applicants.csv")
        )
        if not os.path.exists(project_root_csv_path):
            return []

        applicants: List[Dict[str, Any]] = []
        with open(project_root_csv_path, mode="r", encoding="utf-8") as csv_file:
            reader = csv.DictReader(csv_file)
            for row in reader:
                item: Dict[str, Any] = {}
                # id/_id
                raw_id = row.get("_id") or row.get("id")
                if raw_id:
                    item["id"] = str(raw_id)

                # ê¸°ë³¸ ë¬¸ìì—´ í•„ë“œ
                for field_name in [
                    "name",
                    "position",
                    "department",
                    "experience",
                    "skills",
                    "growthBackground",
                    "motivation",
                    "careerHistory",
                    "analysisResult",
                    "status",
                ]:
                    value = row.get(field_name, "")
                    item[field_name] = "" if value is None else str(value)

                # score
                try:
                    item["analysisScore"] = int(row.get("analysisScore", "0") or 0)
                except Exception:
                    item["analysisScore"] = 0

                # created_at
                created_at_raw = row.get("created_at")
                if created_at_raw:
                    try:
                        iso_candidate = created_at_raw.replace("Z", "+00:00")
                        item["created_at"] = datetime.fromisoformat(iso_candidate)
                    except Exception:
                        item["created_at"] = datetime.now()

                applicants.append(item)
        return applicants
    except Exception:
        return []

# API ë¼ìš°íŠ¸ë“¤
@app.get("/")
async def root():
    return {"message": "AI ì±„ìš© ê´€ë¦¬ ì‹œìŠ¤í…œ APIê°€ ì‹¤í–‰ ì¤‘ì…ë‹ˆë‹¤."}

@app.get("/health")
async def health_check():
    return {"status": "healthy", "message": "ì„œë²„ê°€ ì •ìƒì ìœ¼ë¡œ ì‘ë™ ì¤‘ì…ë‹ˆë‹¤."}

# ì‚¬ìš©ì ê´€ë ¨ API
@app.get("/api/users", response_model=List[User])
async def get_users():
    users = await db.users.find().to_list(1000)
    # MongoDBì˜ _idë¥¼ idë¡œ ë³€í™˜
    for user in users:
        user["id"] = str(user["_id"])
        del user["_id"]
    return [User(**user) for user in users]

@app.post("/api/users", response_model=User)
async def create_user(user: User):
    user_dict = user.dict()
    user_dict["created_at"] = datetime.now()
    result = await db.users.insert_one(user_dict)
    user_dict["id"] = str(result.inserted_id)
    return User(**user_dict)

# ì´ë ¥ì„œ ê´€ë ¨ API
@app.get("/api/resumes", response_model=List[Resume])
async def get_resumes():
    resumes = await db.resumes.find().to_list(1000)
    # MongoDBì˜ _idë¥¼ idë¡œ ë³€í™˜
    for resume in resumes:
        resume["id"] = str(resume["_id"])
        del resume["_id"]
    return [Resume(**resume) for resume in resumes]

@app.post("/api/resumes", response_model=Resume)
async def create_resume(resume: Resume):
    resume_dict = resume.dict()
    resume_dict["created_at"] = datetime.now()
    result = await db.resumes.insert_one(resume_dict)
    resume_dict["id"] = str(result.inserted_id)
    return Resume(**resume_dict)

# ë©´ì ‘ ê´€ë ¨ API
@app.get("/api/interviews", response_model=List[Interview])
async def get_interviews():
    interviews = await db.interviews.find().to_list(1000)
    # MongoDBì˜ _idë¥¼ idë¡œ ë³€í™˜
    for interview in interviews:
        interview["id"] = str(interview["_id"])
        del interview["_id"]
    return [Interview(**interview) for interview in interviews]

@app.post("/api/interviews", response_model=Interview)
async def create_interview(interview: Interview):
    interview_dict = interview.dict()
    interview_dict["created_at"] = datetime.now()
    result = await db.interviews.insert_one(interview_dict)
    interview_dict["id"] = str(result.inserted_id)
    return Interview(**interview_dict)

# ì§€ì›ì ê´€ë ¨ API
@app.get("/api/applicants")
async def get_applicants(skip: int = 0, limit: int = 20):
    try:
        # DBê°€ ë¹„ì–´ìˆìœ¼ë©´ CSVì—ì„œ ìë™ ì„í¬íŠ¸
        await seed_applicants_from_csv_if_empty()
        # ì´ ë¬¸ì„œ ìˆ˜
        total_count = await db.applicants.count_documents({})

        if total_count == 0:
            # DBê°€ ì™„ì „ ë¹„ì–´ìˆì„ ë•Œ CSVë¥¼ ê°€ìƒ DBì²˜ëŸ¼ ë°˜í™˜
            csv_applicants = load_applicants_from_csv()
            items = csv_applicants[skip:skip+limit]
            return {
                "applicants": [Resume(**a) for a in items],
                "total_count": len(csv_applicants),
                "skip": skip,
                "limit": limit,
                "has_more": (skip + limit) < len(csv_applicants)
            }

        # í˜ì´ì§•ìœ¼ë¡œ ì´ë ¥ì„œ(ì§€ì›ì) ëª©ë¡ ì¡°íšŒ
        applicants = await db.applicants.find().skip(skip).limit(limit).to_list(limit)

        # MongoDBì˜ _idë¥¼ idë¡œ ë³€í™˜ ë° ObjectId í•„ë“œë“¤ì„ ë¬¸ìì—´ë¡œ ë³€í™˜
        for applicant in applicants:
            applicant["id"] = str(applicant["_id"])
            del applicant["_id"]
            if "resume_id" in applicant and applicant["resume_id"]:
                applicant["resume_id"] = str(applicant["resume_id"])

        return {
            "applicants": [Resume(**applicant) for applicant in applicants],
            "total_count": total_count,
            "skip": skip,
            "limit": limit,
            "has_more": (skip + limit) < total_count
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ì§€ì›ì ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")

# ì§€ì›ì í†µê³„ API
@app.get("/api/applicants/stats/overview")
async def get_applicant_stats():
    try:
        # DBê°€ ë¹„ì–´ìˆìœ¼ë©´ CSVì—ì„œ ìë™ ì„í¬íŠ¸
        await seed_applicants_from_csv_if_empty()
        # ì´ ì§€ì›ì ìˆ˜ (resumes ì»¬ë ‰ì…˜ ê¸°ì¤€)
        total_applicants = await db.resumes.count_documents({})

        if total_applicants == 0:
            # CSV ê¸°ë°˜ ê°€ìƒ í†µê³„
            csv_applicants = load_applicants_from_csv()
            total = len(csv_applicants)
            status_counts = {"pending": 0, "approved": 0, "rejected": 0}
            for a in csv_applicants:
                s = (a.get("status") or "").lower()
                if s in status_counts:
                    status_counts[s] += 1
            return {
                "total_applicants": total,
                "status_breakdown": status_counts,
                "recent_applicants_30_days": total,
                "success_rate": round((status_counts.get("approved", 0) / total * 100) if total > 0 else 0, 2)
            }

        # ìƒíƒœë³„ ì§€ì›ì ìˆ˜
        pending_count = await db.resumes.count_documents({"status": "pending"})
        approved_count = await db.resumes.count_documents({"status": "approved"})
        rejected_count = await db.resumes.count_documents({"status": "rejected"})
        
        # ìµœê·¼ 30ì¼ê°„ ì§€ì›ì ìˆ˜ (ì›” ê²½ê³„/ìœ¤ë‹¬ ì´ìŠˆ ì—†ì´ ì•ˆì „í•˜ê²Œ ê³„ì‚°)
        thirty_days_ago = datetime.now() - timedelta(days=30)
        recent_applicants = await db.resumes.count_documents({"created_at": {"$gte": thirty_days_ago}})
        
        return {
            "total_applicants": total_applicants,
            "status_breakdown": {
                "pending": pending_count,
                "approved": approved_count,
                "rejected": rejected_count
            },
            "recent_applicants_30_days": recent_applicants,
            "success_rate": round((approved_count / total_applicants * 100) if total_applicants > 0 else 0, 2)
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ì§€ì›ì í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")

# Vector Service API
@app.post("/api/vector/create")
async def create_vector(data: Dict[str, Any]):
    """í…ìŠ¤íŠ¸ë¥¼ ë²¡í„°ë¡œ ë³€í™˜í•˜ì—¬ ì €ì¥"""
    try:
        text = data.get("text", "")
        document_id = data.get("document_id")
        metadata = data.get("metadata", {})
        
        # ì—¬ê¸°ì„œ ì‹¤ì œ ë²¡í„°í™” ë¡œì§ êµ¬í˜„
        # ì˜ˆ: embedding_modelì„ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ë¥¼ ë²¡í„°ë¡œ ë³€í™˜
        
        # ì„ì‹œë¡œ ì„±ê³µ ì‘ë‹µ ë°˜í™˜
        return {
            "message": "Vector created successfully",
            "document_id": document_id,
            "vector_dimension": 384,  # ì˜ˆì‹œ ì°¨ì›
            "status": "success"
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ë²¡í„° ìƒì„± ì‹¤íŒ¨: {str(e)}")

@app.post("/api/vector/search")
async def search_vectors(data: Dict[str, Any]):
    """ë²¡í„° ìœ ì‚¬ë„ ê²€ìƒ‰"""
    try:
        query_text = data.get("query", "")
        top_k = data.get("top_k", 5)
        threshold = data.get("threshold", 0.7)
        
        # ì—¬ê¸°ì„œ ì‹¤ì œ ë²¡í„° ê²€ìƒ‰ ë¡œì§ êµ¬í˜„
        
        # ì„ì‹œë¡œ ê²€ìƒ‰ ê²°ê³¼ ë°˜í™˜
        return {
            "results": [
                {
                    "document_id": "doc_001",
                    "score": 0.95,
                    "text": "ê²€ìƒ‰ëœ í…ìŠ¤íŠ¸ ìƒ˜í”Œ 1",
                    "metadata": {"type": "resume", "applicant_id": "app_001"}
                },
                {
                    "document_id": "doc_002", 
                    "score": 0.87,
                    "text": "ê²€ìƒ‰ëœ í…ìŠ¤íŠ¸ ìƒ˜í”Œ 2",
                    "metadata": {"type": "cover_letter", "applicant_id": "app_002"}
                }
            ],
            "total_found": 2
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ë²¡í„° ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}")

# Chunking Service API
@app.post("/api/chunking/split")
async def split_text(data: Dict[str, Any]):
    """í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë¶„í• í•˜ê³  DBì— ì €ì¥"""
    try:
        text = data.get("text", "")
        resume_id = data.get("resume_id")
        field_name = data.get("field_name", "")  # growthBackground, motivation, careerHistory
        chunk_size = data.get("chunk_size", 1000)
        chunk_overlap = data.get("chunk_overlap", 200)
        split_type = data.get("split_type", "recursive")
        
        if not resume_id:
            raise HTTPException(status_code=400, detail="resume_idê°€ í•„ìš”í•©ë‹ˆë‹¤.")
        
        # í…ìŠ¤íŠ¸ ë¶„í•  ë¡œì§
        chunks = []
        text_length = len(text)
        start = 0
        chunk_index = 0
        
        while start < text_length:
            end = min(start + chunk_size, text_length)
            chunk_text = text[start:end]
            
            if chunk_text.strip():  # ë¹ˆ ì²­í¬ëŠ” ì œì™¸
                chunk_id = f"chunk_{chunk_index:03d}"
                vector_id = f"resume_{resume_id}_{chunk_id}"
                
                chunk_doc = {
                    "resume_id": resume_id,
                    "chunk_id": chunk_id,
                    "text": chunk_text,
                    "start_pos": start,
                    "end_pos": end,
                    "chunk_index": chunk_index,
                    "field_name": field_name,
                    "vector_id": vector_id,
                    "metadata": {
                        "length": len(chunk_text),
                        "split_type": split_type,
                        "chunk_size": chunk_size,
                        "chunk_overlap": chunk_overlap
                    },
                    "created_at": datetime.now()
                }
                
                # MongoDBì— ì²­í¬ ì €ì¥
                result = await db.resume_chunks.insert_one(chunk_doc)
                chunk_doc["id"] = str(result.inserted_id)
                
                chunks.append(chunk_doc)
                chunk_index += 1
            
            start = end - chunk_overlap if chunk_overlap > 0 else end
            
            if start >= text_length:
                break
        
        return {
            "chunks": chunks,
            "total_chunks": len(chunks),
            "original_length": text_length,
            "resume_id": resume_id,
            "field_name": field_name,
            "split_config": {
                "chunk_size": chunk_size,
                "chunk_overlap": chunk_overlap,
                "split_type": split_type
            }
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"í…ìŠ¤íŠ¸ ë¶„í•  ì‹¤íŒ¨: {str(e)}")

@app.get("/api/chunking/resume/{resume_id}")
async def get_resume_chunks(resume_id: str):
    """íŠ¹ì • ì´ë ¥ì„œì˜ ëª¨ë“  ì²­í¬ ì¡°íšŒ"""
    try:
        chunks = await db.resume_chunks.find({"resume_id": resume_id}).to_list(1000)
        
        # MongoDBì˜ _idë¥¼ idë¡œ ë³€í™˜
        for chunk in chunks:
            chunk["id"] = str(chunk["_id"])
            del chunk["_id"]
        
        return {
            "resume_id": resume_id,
            "chunks": [ResumeChunk(**chunk) for chunk in chunks],
            "total_chunks": len(chunks)
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ì²­í¬ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")

@app.post("/api/chunking/process-resume")
async def process_resume_with_chunking(data: Dict[str, Any]):
    """ì´ë ¥ì„œ ì „ì²´ë¥¼ í•„ë“œë³„ë¡œ ì²­í‚¹ ì²˜ë¦¬"""
    try:
        resume_id = data.get("resume_id")
        if not resume_id:
            raise HTTPException(status_code=400, detail="resume_idê°€ í•„ìš”í•©ë‹ˆë‹¤.")
        
        # ì´ë ¥ì„œ ì •ë³´ ì¡°íšŒ
        resume = await db.resumes.find_one({"_id": ObjectId(resume_id)})
        if not resume:
            raise HTTPException(status_code=404, detail="ì´ë ¥ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        chunk_size = data.get("chunk_size", 800)
        chunk_overlap = data.get("chunk_overlap", 150)
        
        # ì²­í‚¹í•  í•„ë“œë“¤
        fields_to_chunk = ["growthBackground", "motivation", "careerHistory"]
        all_chunks = []
        
        for field_name in fields_to_chunk:
            field_text = resume.get(field_name, "")
            if field_text and field_text.strip():
                # í•„ë“œë³„ ì²­í‚¹ ì²˜ë¦¬
                field_chunks = []
                text_length = len(field_text)
                start = 0
                chunk_index = 0
                
                while start < text_length:
                    end = min(start + chunk_size, text_length)
                    chunk_text = field_text[start:end]
                    
                    if chunk_text.strip():
                        chunk_id = f"{field_name}_chunk_{chunk_index:03d}"
                        vector_id = f"resume_{resume_id}_{chunk_id}"
                        
                        chunk_doc = {
                            "resume_id": resume_id,
                            "chunk_id": chunk_id,
                            "text": chunk_text,
                            "start_pos": start,
                            "end_pos": end,
                            "chunk_index": chunk_index,
                            "field_name": field_name,
                            "vector_id": vector_id,
                            "metadata": {
                                "applicant_name": resume.get("name", ""),
                                "position": resume.get("position", ""),
                                "department": resume.get("department", ""),
                                "length": len(chunk_text)
                            },
                            "created_at": datetime.now()
                        }
                        
                        result = await db.resume_chunks.insert_one(chunk_doc)
                        chunk_doc["id"] = str(result.inserted_id)
                        field_chunks.append(chunk_doc)
                        chunk_index += 1
                    
                    start = end - chunk_overlap if chunk_overlap > 0 else end
                    if start >= text_length:
                        break
                
                all_chunks.extend(field_chunks)
        
        return {
            "resume_id": resume_id,
            "applicant_name": resume.get("name", ""),
            "processed_fields": fields_to_chunk,
            "total_chunks": len(all_chunks),
            "chunks_by_field": {
                field: len([c for c in all_chunks if c["field_name"] == field])
                for field in fields_to_chunk
            },
            "chunks": all_chunks
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ì´ë ¥ì„œ ì²­í‚¹ ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")

@app.post("/api/chunking/merge")
async def merge_chunks(data: Dict[str, Any]):
    """ì²­í¬ë“¤ì„ ë³‘í•©"""
    try:
        chunks = data.get("chunks", [])
        separator = data.get("separator", "\n\n")
        
        # ì²­í¬ ë³‘í•©
        merged_text = separator.join([chunk.get("text", "") for chunk in chunks])
        
        return {
            "merged_text": merged_text,
            "total_length": len(merged_text),
            "chunks_merged": len(chunks),
            "separator_used": separator
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ì²­í¬ ë³‘í•© ì‹¤íŒ¨: {str(e)}")

# Similarity Service API
@app.post("/api/similarity/compare")
async def compare_similarity(data: Dict[str, Any]):
    """ë‘ í…ìŠ¤íŠ¸ ê°„ì˜ ìœ ì‚¬ë„ ê³„ì‚°"""
    try:
        text1 = data.get("text1", "")
        text2 = data.get("text2", "")
        method = data.get("method", "cosine")  # cosine, jaccard, levenshtein
        
        # ì—¬ê¸°ì„œ ì‹¤ì œ ìœ ì‚¬ë„ ê³„ì‚° ë¡œì§ êµ¬í˜„
        # ì˜ˆ: sentence-transformersì˜ cosine similarity
        
        # ì„ì‹œë¡œ ìœ ì‚¬ë„ ì ìˆ˜ ë°˜í™˜
        import random
        similarity_score = random.uniform(0.3, 0.95)  # ì„ì‹œ ì ìˆ˜
        
        return {
            "similarity_score": round(similarity_score, 4),
            "method": method,
            "text1_length": len(text1),
            "text2_length": len(text2),
            "comparison_result": {
                "highly_similar": similarity_score > 0.8,
                "moderately_similar": 0.5 < similarity_score <= 0.8,
                "low_similar": similarity_score <= 0.5
            }
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ìœ ì‚¬ë„ ê³„ì‚° ì‹¤íŒ¨: {str(e)}")

@app.post("/api/similarity/batch")
async def batch_similarity(data: Dict[str, Any]):
    """ì—¬ëŸ¬ í…ìŠ¤íŠ¸ë“¤ ê°„ì˜ ì¼ê´„ ìœ ì‚¬ë„ ê³„ì‚°"""
    try:
        texts = data.get("texts", [])
        reference_text = data.get("reference_text", "")
        method = data.get("method", "cosine")
        threshold = data.get("threshold", 0.7)
        
        # ë°°ì¹˜ ìœ ì‚¬ë„ ê³„ì‚°
        results = []
        import random
        
        for i, text in enumerate(texts):
            similarity_score = random.uniform(0.2, 0.95)  # ì„ì‹œ ì ìˆ˜
            results.append({
                "index": i,
                "text_preview": text[:100] + "..." if len(text) > 100 else text,
                "similarity_score": round(similarity_score, 4),
                "above_threshold": similarity_score >= threshold
            })
        
        # ì„ê³„ê°’ ì´ìƒì¸ ê²°ê³¼ë“¤ í•„í„°ë§
        filtered_results = [r for r in results if r["above_threshold"]]
        
        return {
            "results": results,
            "filtered_results": filtered_results,
            "total_compared": len(texts),
            "above_threshold_count": len(filtered_results),
            "method": method,
            "threshold": threshold,
            "reference_text_length": len(reference_text)
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ë°°ì¹˜ ìœ ì‚¬ë„ ê³„ì‚° ì‹¤íŒ¨: {str(e)}")

@app.get("/api/similarity/metrics")
async def get_similarity_metrics():
    """ìœ ì‚¬ë„ ì„œë¹„ìŠ¤ ë©”íŠ¸ë¦­ ì¡°íšŒ"""
    try:
        # ì„ì‹œ ë©”íŠ¸ë¦­ ë°ì´í„°
        return {
            "total_comparisons": 1250,
            "average_similarity": 0.67,
            "supported_methods": ["cosine", "jaccard", "levenshtein", "semantic"],
            "performance_stats": {
                "average_processing_time_ms": 45,
                "comparisons_per_second": 220,
                "cache_hit_rate": 0.78
            },
            "usage_by_method": {
                "cosine": 850,
                "semantic": 300,
                "jaccard": 70,
                "levenshtein": 30
            }
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ë©”íŠ¸ë¦­ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")

# ì´ë ¥ì„œ ìœ ì‚¬ë„ ì²´í¬ API
@app.post("/api/resume/similarity-check/{resume_id}")
async def check_resume_similarity(resume_id: str):
    """íŠ¹ì • ì´ë ¥ì„œì˜ ìœ ì‚¬ë„ ì²´í¬ (ë‹¤ë¥¸ ëª¨ë“  ì´ë ¥ì„œì™€ ë¹„êµ)"""
    try:
        print(f"[INFO] ìœ ì‚¬ë„ ì²´í¬ ìš”ì²­ - resume_id: {resume_id}")
        
        # SimilarityServiceë¥¼ í†µí•œ ì²­í‚¹ ê¸°ë°˜ ìœ ì‚¬ë„ ë¶„ì„
        result = await similarity_service.find_similar_resumes_by_chunks(resume_id, db.applicants, limit=50)
        
        # í˜„ì¬ ì´ë ¥ì„œ ì •ë³´ ì¡°íšŒ
        current_resume = await db.applicants.find_one({"_id": object_id})
        print(f"[INFO] ë°ì´í„°ë² ì´ìŠ¤ ì¡°íšŒ ê²°ê³¼: {current_resume is not None}")
        
        # ì²­í‚¹ ê¸°ë°˜ API ì‘ë‹µ í˜•ì‹ì— ë§ê²Œ ë³€í™˜
        similarity_results = []
        for similar in result["data"]["similar_resumes"]:
            # ì²­í‚¹ ìƒì„¸ ì •ë³´ì—ì„œ í•„ë“œë³„ ìœ ì‚¬ë„ ì¶”ì¶œ
            chunk_details = similar.get("chunk_details", {})
            field_similarities = {
                "growthBackground": 0.0,
                "motivation": 0.0,
                "careerHistory": 0.0
            }
            
            # ì²­í¬ ë§¤ì¹­ì—ì„œ í•„ë“œë³„ ìµœê³  ì ìˆ˜ ì¶”ì¶œ
            for chunk_key, chunk_info in chunk_details.items():
                if "growth_background" in chunk_key:
                    field_similarities["growthBackground"] = max(field_similarities["growthBackground"], chunk_info["score"])
                elif "motivation" in chunk_key:
                    field_similarities["motivation"] = max(field_similarities["motivation"], chunk_info["score"])
                elif "career_history" in chunk_key:
                    field_similarities["careerHistory"] = max(field_similarities["careerHistory"], chunk_info["score"])
            
            similarity_result = {
                "resume_id": similar["resume"]["_id"],
                "applicant_name": similar["resume"].get("name", "ì•Œ ìˆ˜ ì—†ìŒ"),
                "position": similar["resume"].get("position", ""),
                "department": similar["resume"].get("department", ""),
                "overall_similarity": round(similar["similarity_score"], 4),
                "field_similarities": {
                    "growthBackground": round(field_similarities["growthBackground"], 4),
                    "motivation": round(field_similarities["motivation"], 4),
                    "careerHistory": round(field_similarities["careerHistory"], 4)
                },
                "chunk_matches": similar.get("chunk_matches", 0),
                "chunk_details": chunk_details,
                "is_high_similarity": similar["similarity_score"] > 0.7,
                "is_moderate_similarity": 0.4 <= similar["similarity_score"] <= 0.7,
                "is_low_similarity": similar["similarity_score"] < 0.4,
                "llm_analysis": similar.get("llm_analysis")
            }
            similarity_results.append(similarity_result)
        
        # ë‹¤ë¥¸ ëª¨ë“  ì´ë ¥ì„œ ì¡°íšŒ (í˜„ì¬ ì´ë ¥ì„œ ì œì™¸)
        other_resumes = await db.applicants.find({"_id": {"$ne": ObjectId(resume_id)}}).to_list(1000)
        
        # í˜„ì¬ ì´ë ¥ì„œì˜ ë¹„êµ í…ìŠ¤íŠ¸ (ìœ ì‚¬ë„ ê³„ì‚° í•„ë“œ)
        current_fields = {
            "growthBackground": current_resume.get("growthBackground", ""),
            "motivation": current_resume.get("motivation", ""),
            "careerHistory": current_resume.get("careerHistory", "")
        }
        
        # ì „ì²´ í…ìŠ¤íŠ¸ ì¡°í•©
        current_text = " ".join([text for text in current_fields.values() if text])
        
        similarity_results = []
        
        for other_resume in other_resumes:
            other_id = str(other_resume["_id"])
            
            # ë‹¤ë¥¸ ì´ë ¥ì„œì˜ ë¹„êµ í…ìŠ¤íŠ¸
            other_fields = {
                "growthBackground": other_resume.get("growthBackground", ""),
                "motivation": other_resume.get("motivation", ""), 
                "careerHistory": other_resume.get("careerHistory", "")
            }
            other_text = " ".join([text for text in other_fields.values() if text])
            
            # ì‹¤ì œ ìœ ì‚¬ë„ ê³„ì‚° ì‚¬ìš©
            try:
                print(f"ğŸ’« ì´ë ¥ì„œ ê°„ ìœ ì‚¬ë„ ê³„ì‚° ì‹œì‘: {resume_id} vs {other_id}")
                
                # SimilarityServiceì˜ í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ ê³„ì‚° ë©”ì„œë“œ ì§ì ‘ í˜¸ì¶œ
                text_similarity = similarity_service._calculate_text_similarity(current_resume, other_resume)
                overall_similarity = text_similarity if text_similarity is not None else 0.0
                
                print(f"ğŸ“Š í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ ê²°ê³¼: {overall_similarity:.3f}")
                
                # í•„ë“œë³„ ìœ ì‚¬ë„ ê³„ì‚°
                field_similarities = {}
                for field_name in current_fields.keys():
                    if current_fields[field_name] and other_fields[field_name]:
                        # í•„ë“œë³„ ê°œë³„ í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ ê³„ì‚°
                        field_sim = similarity_service._calculate_text_similarity(
                            {field_name: current_fields[field_name]},
                            {field_name: other_fields[field_name]}
                        )
                        field_similarities[field_name] = field_sim if field_sim is not None else 0.0
                        print(f"ğŸ“‹ {field_name} ìœ ì‚¬ë„: {field_similarities[field_name]:.3f}")
                    else:
                        field_similarities[field_name] = 0.0
                        
            except Exception as e:
                print(f"[ERROR] ìœ ì‚¬ë„ ê³„ì‚° ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                import traceback
                traceback.print_exc()
                
                # ì˜¤ë¥˜ ì‹œ ê¸°ë³¸ê°’ ì‚¬ìš©
                import random
                overall_similarity = random.uniform(0.1, 0.9)
                field_similarities = {}
                for field_name in current_fields.keys():
                    if current_fields[field_name] and other_fields[field_name]:
                        field_similarities[field_name] = random.uniform(0.0, 1.0)
                    else:
                        field_similarities[field_name] = 0.0
            
            # LLM ë¶„ì„ ì¶”ê°€ (ìœ ì‚¬ë„ê°€ ì¼ì • ìˆ˜ì¤€ ì´ìƒì¼ ë•Œë§Œ)
            llm_analysis = None
            
            if overall_similarity >= 0.3:  # 30% ì´ìƒ ìœ ì‚¬í•  ë•Œë§Œ LLM ë¶„ì„
                try:
                    print(f"[API] LLM ë¶„ì„ ì‹œì‘ - ìœ ì‚¬ë„: {overall_similarity:.3f}")
                    llm_analysis = await similarity_service.llm_service.analyze_similarity_reasoning(
                        original_resume=current_resume,
                        similar_resume=other_resume,
                        similarity_score=overall_similarity
                    )
                    print(f"[API] LLM ë¶„ì„ ì™„ë£Œ")
                except Exception as llm_error:
                    print(f"[API] LLM ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {llm_error}")
                    llm_analysis = {
                        "success": False,
                        "error": str(llm_error),
                        "analysis": "LLM ë¶„ì„ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤."
                    }
            
            similarity_result = {
                "resume_id": other_id,
                "applicant_name": other_resume.get("name", "ì•Œ ìˆ˜ ì—†ìŒ"),
                "position": other_resume.get("position", ""),
                "department": other_resume.get("department", ""),
                "overall_similarity": round(overall_similarity, 4),
                "field_similarities": {
                    "growthBackground": round(field_similarities["growthBackground"], 4),
                    "motivation": round(field_similarities["motivation"], 4),
                    "careerHistory": round(field_similarities["careerHistory"], 4)
                },
                "is_high_similarity": overall_similarity > 0.7,
                "is_moderate_similarity": 0.4 <= overall_similarity <= 0.7,
                "is_low_similarity": overall_similarity < 0.4,
                "llm_analysis": llm_analysis
            }
            
            similarity_results.append(similarity_result)
        
        # ìœ ì‚¬ë„ ë†’ì€ ìˆœìœ¼ë¡œ ì •ë ¬
        similarity_results.sort(key=lambda x: x["overall_similarity"], reverse=True)
        
        # ì „ì²´ í‘œì ˆ ìœ„í—˜ë„ ë¶„ì„ ì¶”ê°€
        plagiarism_analysis = None
        high_similarity_results = [r for r in similarity_results if r["overall_similarity"] >= 0.3]
        
        if high_similarity_results:
            try:
                print(f"[API] í‘œì ˆ ìœ„í—˜ë„ ë¶„ì„ ì‹œì‘")
                plagiarism_analysis = await similarity_service.llm_service.analyze_plagiarism_risk(
                    original_resume=current_resume,
                    similar_resumes=high_similarity_results
                )
                print(f"[API] í‘œì ˆ ìœ„í—˜ë„ ë¶„ì„ ì™„ë£Œ")
            except Exception as plag_error:
                print(f"[API] í‘œì ˆ ìœ„í—˜ë„ ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {plag_error}")
                plagiarism_analysis = {
                    "success": False,
                    "error": str(plag_error),
                    "risk_level": "UNKNOWN",
                    "analysis": "í‘œì ˆ ìœ„í—˜ë„ ë¶„ì„ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤."
                }
        
        # í†µê³„ ì •ë³´
        high_similarity_count = len([r for r in similarity_results if r["is_high_similarity"]])
        moderate_similarity_count = len([r for r in similarity_results if r["is_moderate_similarity"]])
        low_similarity_count = len([r for r in similarity_results if r["is_low_similarity"]])
        
        return {
            "current_resume": {
                "id": resume_id,
                "name": current_resume.get("name", ""),
                "position": current_resume.get("position", ""),
                "department": current_resume.get("department", "")
            },
            "similarity_results": similarity_results,
            "statistics": {
                "total_compared": len(similarity_results),
                "high_similarity_count": high_similarity_count,
                "moderate_similarity_count": moderate_similarity_count,
                "low_similarity_count": low_similarity_count,
                "average_similarity": round(sum([r["overall_similarity"] for r in similarity_results]) / len(similarity_results) if similarity_results else 0, 4)
            },
            "top_similar": similarity_results[:5] if similarity_results else [],
            "plagiarism_analysis": plagiarism_analysis,
            "analysis_timestamp": datetime.now().isoformat()
        }
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ìœ ì‚¬ë„ ì²´í¬ ì‹¤íŒ¨: {str(e)}")

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8000)