import os
import sys
from dotenv import load_dotenv
from fastapi import FastAPI, HTTPException, File, UploadFile, Form
from fastapi.middleware.cors import CORSMiddleware
from fastapi.middleware.trustedhost import TrustedHostMiddleware
from fastapi.responses import JSONResponse
import uvicorn
from motor.motor_asyncio import AsyncIOMotorClient
from pydantic import BaseModel
from bson import ObjectId
from typing import List, Optional, Dict, Any
import locale
import codecs
from datetime import datetime
import io
import json
import re
import random
# from sentence_transformers import SentenceTransformer
# import numpy as np

# ìì†Œì„œ ë¶„ì„ ê´€ë ¨ ë¼ì´ë¸ŒëŸ¬ë¦¬
try:
    import pdfplumber
    import docx
    PDF_AVAILABLE = True
except ImportError:
    PDF_AVAILABLE = False

# .env íŒŒì¼ ë¡œë“œ (í˜„ì¬ ë””ë ‰í† ë¦¬ì—ì„œ)
print(f"ğŸ” í˜„ì¬ ì‘ì—… ë””ë ‰í† ë¦¬: {os.getcwd()}")
print(f"ğŸ” .env íŒŒì¼ ì¡´ì¬ ì—¬ë¶€: {os.path.exists('.env')}")
load_dotenv('.env')
print(f"ğŸ” GOOGLE_API_KEY ë¡œë“œ í›„: {os.getenv('GOOGLE_API_KEY')}")

# Gemini í´ë¼ì´ì–¸íŠ¸ (í™˜ê²½ë³€ìˆ˜ì—ì„œ API í‚¤ ê°€ì ¸ì˜¤ê¸°)
try:
    import google.generativeai as genai
    api_key = os.getenv("GOOGLE_API_KEY")
    if api_key:
        genai.configure(api_key=api_key)
        GEMINI_AVAILABLE = True
        print("âœ… Gemini APIê°€ ì„¤ì •ë˜ì—ˆìŠµë‹ˆë‹¤.")
    else:
        GEMINI_AVAILABLE = False
        print("âš ï¸  GOOGLE_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ìì†Œì„œ ë¶„ì„ ê¸°ëŠ¥ì´ ì œí•œë©ë‹ˆë‹¤.")
except ImportError:
    GEMINI_AVAILABLE = False
    print("âš ï¸  Gemini ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ìì†Œì„œ ë¶„ì„ ê¸°ëŠ¥ì´ ì œí•œë©ë‹ˆë‹¤.")

# Python í™˜ê²½ ì¸ì½”ë”© ì„¤ì •
# ì‹œìŠ¤í…œ ê¸°ë³¸ ì¸ì½”ë”©ì„ UTF-8ë¡œ ì„¤ì •
if sys.platform.startswith('win'):
    # Windows í™˜ê²½ì—ì„œ UTF-8 ê°•ì œ ì„¤ì •
    os.environ['PYTHONIOENCODING'] = 'utf-8'
    # ì½˜ì†” ì¶œë ¥ ì¸ì½”ë”© ì„¤ì •
    sys.stdout = codecs.getwriter('utf-8')(sys.stdout.detach())
    sys.stderr = codecs.getwriter('utf-8')(sys.stderr.detach())

# FastAPI ì•± ìƒì„±
app = FastAPI(title="AI ì±„ìš© ê´€ë¦¬ ì‹œìŠ¤í…œ API", version="1.0.0")

# CORS ì„¤ì •
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# í•œê¸€ ì¸ì½”ë”©ì„ ìœ„í•œ ë¯¸ë“¤ì›¨ì–´
@app.middleware("http")
async def add_charset_header(request, call_next):
    response = await call_next(request)
    
    # ëª¨ë“  JSON ì‘ë‹µì— UTF-8 ì¸ì½”ë”© ëª…ì‹œ
    if response.headers.get("content-type", "").startswith("application/json"):
        response.headers["content-type"] = "application/json; charset=utf-8"
    
    # í…ìŠ¤íŠ¸ ì‘ë‹µì—ë„ UTF-8 ì¸ì½”ë”© ëª…ì‹œ
    elif response.headers.get("content-type", "").startswith("text/"):
        if "charset" not in response.headers.get("content-type", ""):
            current_content_type = response.headers.get("content-type", "")
            response.headers["content-type"] = f"{current_content_type}; charset=utf-8"
    
    return response

# ë¼ìš°í„° ë“±ë¡
# chatbot_routerì™€ langgraph_routerëŠ” í˜„ì¬ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ
# app.include_router(chatbot_router, prefix="/api/chatbot", tags=["chatbot"])
# app.include_router(langgraph_router, prefix="/api/langgraph", tags=["langgraph"])

# Upload ë¼ìš°í„° ë“±ë¡
from routers.upload import router as upload_router
app.include_router(upload_router, prefix="/api/upload", tags=["upload"])

# ì§€ì›ì ë¼ìš°í„° ë“±ë¡
from routers.applicants import router as applicants_router
app.include_router(applicants_router, prefix="/api/applicants", tags=["applicants"])

# MongoDB ì—°ê²°
MONGODB_URI = os.getenv("MONGODB_URI", "mongodb://localhost:27017/hireme")
client = AsyncIOMotorClient(MONGODB_URI)
db = client.hireme

# ìì†Œì„œ ë¶„ì„ ê´€ë ¨ í•¨ìˆ˜ë“¤
def extract_text_from_pdf(file_bytes: bytes) -> str:
    """PDF íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
    if not PDF_AVAILABLE:
        raise HTTPException(status_code=400, detail="PDF ì²˜ë¦¬ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    
    text = []
    try:
        with pdfplumber.open(io.BytesIO(file_bytes)) as pdf:
            for page in pdf.pages:
                page_text = page.extract_text()
                if page_text:
                    text.append(page_text)
        return "\n".join(text)
    except Exception as e:
        raise HTTPException(status_code=400, detail=f"PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}")

def extract_text_from_docx(file_bytes: bytes) -> str:
    """DOCX íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
    if not PDF_AVAILABLE:
        raise HTTPException(status_code=400, detail="DOCX ì²˜ë¦¬ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    
    try:
        doc = docx.Document(io.BytesIO(file_bytes))
        text = []
        for paragraph in doc.paragraphs:
            if paragraph.text.strip():
                text.append(paragraph.text)
        return "\n".join(text)
    except Exception as e:
        raise HTTPException(status_code=400, detail=f"DOCX í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}")

def extract_text_from_file(filename: str, file_bytes: bytes) -> str:
    """íŒŒì¼ í™•ì¥ìì— ë”°ë¼ ì ì ˆí•œ í…ìŠ¤íŠ¸ ì¶”ì¶œ í•¨ìˆ˜ í˜¸ì¶œ"""
    file_ext = filename.lower()
    
    if file_ext.endswith('.pdf'):
        return extract_text_from_pdf(file_bytes)
    elif file_ext.endswith('.docx'):
        return extract_text_from_docx(file_bytes)
    elif file_ext.endswith('.txt'):
        return file_bytes.decode('utf-8', errors='ignore')
    else:
        raise HTTPException(status_code=400, detail="ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹ì…ë‹ˆë‹¤. PDF, DOCX, TXT íŒŒì¼ë§Œ ì§€ì›í•©ë‹ˆë‹¤.")

def split_into_paragraphs(text: str) -> List[str]:
    """í…ìŠ¤íŠ¸ë¥¼ ë¬¸ë‹¨ìœ¼ë¡œ ë¶„ë¦¬"""
    paragraphs = [p.strip() for p in text.split('\n\n') if p.strip()]
    if not paragraphs:
        # ë¬¸ë‹¨ êµ¬ë¶„ì´ ì—†ëŠ” ê²½ìš° ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„ë¦¬
        sentences = re.split(r'[.!?]+', text)
        paragraphs = [s.strip() for s in sentences if s.strip()]
    return paragraphs

def split_into_sentences(text: str) -> List[str]:
    """í…ìŠ¤íŠ¸ë¥¼ ë¬¸ì¥ìœ¼ë¡œ ë¶„ë¦¬"""
    sentences = re.split(r'[.!?]+', text)
    return [s.strip() for s in sentences if s.strip()]

async def analyze_cover_letter_basic(text: str, job_description: str = "") -> Dict[str, Any]:
    """ê¸°ë³¸ ìì†Œì„œ ë¶„ì„ (Geminiê°€ ì‚¬ìš© ë¶ˆê°€ëŠ¥í•  ë•Œ)"""
    try:
        # ê°„ë‹¨í•œ í…ìŠ¤íŠ¸ ë¶„ì„
        words = text.split()
        sentences = re.split(r'[.!?]+', text)
        
        return {
            "summary": f"ì´ {len(words)}ë‹¨ì–´, {len(sentences)}ë¬¸ì¥ìœ¼ë¡œ êµ¬ì„±ëœ ìì†Œì„œì…ë‹ˆë‹¤.",
            "top_strengths": [
                {"strength": "í…ìŠ¤íŠ¸ ê¸¸ì´", "evidence": f"ì´ {len(words)}ë‹¨ì–´", "confidence": 0.8}
            ],
            "star_cases": [],
            "job_fit_score": 50,
            "matched_skills": [],
            "missing_skills": [],
            "grammar_suggestions": [],
            "improvement_suggestions": [],
            "overall_score": 50
        }
    except Exception as e:
        print(f"ê¸°ë³¸ ë¶„ì„ ì‹¤íŒ¨: {e}")
        return {
            "summary": "ë¶„ì„ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.",
            "top_strengths": [],
            "star_cases": [],
            "job_fit_score": 0,
            "matched_skills": [],
            "missing_skills": [],
            "grammar_suggestions": [],
            "improvement_suggestions": [],
            "overall_score": 0
        }

async def analyze_cover_letter_with_llm(text: str, job_description: str = "") -> Dict[str, Any]:
    """Geminië¥¼ ì‚¬ìš©í•˜ì—¬ ìì†Œì„œ ë¶„ì„"""
    if not GEMINI_AVAILABLE:
        # Geminiê°€ ì‚¬ìš© ë¶ˆê°€ëŠ¥í•  ë•Œ ê¸°ë³¸ ë¶„ì„ ì œê³µ
        return await analyze_cover_letter_basic(text, job_description)
    
    try:
        # Gemini ëª¨ë¸ ì´ˆê¸°í™”
        model = genai.GenerativeModel('gemini-1.5-flash')
        
        # 1. ìš”ì•½ ë° í•µì‹¬ ê°•ì  ì¶”ì¶œ
        summary_prompt = f"""ë‹¹ì‹ ì€ ì±„ìš©ë‹´ë‹¹ì ì—­í• ì„ í•œë‹¤. ì§€ì›ìê°€ ì œì¶œí•œ ìì†Œì„œë¥¼ ì½ê³ , 3ë¬¸ì¥ ë‚´ë¡œ í•µì‹¬ ìš”ì•½ê³¼ ê°€ì¥ ê°•í•œ 'í•µì‹¬ ê°•ì ' 3ê°€ì§€ë¥¼ ì¶”ì¶œí•´ì¤˜. í•µì‹¬ ê°•ì ì€ êµ¬ì²´ì  ê·¼ê±°(ë¬¸ì¥ ìœ„ì¹˜/ë¬¸ì¥ ì¼ë¶€)ì™€ í•¨ê»˜ í‘œê¸°í•´ë¼.

ìì†Œì„œ:
{text}

ë°˜ë“œì‹œ ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”. ë‹¤ë¥¸ í…ìŠ¤íŠ¸ëŠ” í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”:
{{
  "summary": "í•œ ë¬¸ì¥ ìš”ì•½...",
  "top_strengths": [
    {{"strength":"íŒ€ ë¦¬ë”© ê²½í—˜", "evidence":"3ë²ˆì§¸ ë¬¸ë‹¨: 'íŒ€ì„ ì´ëŒë©°...'", "confidence": 0.92}},
    {{"strength":"ê¸°ìˆ  ìŠ¤íƒ", "evidence":"2ë²ˆì§¸ ë¬¸ë‹¨: 'Pythonê³¼...'", "confidence": 0.88}},
    {{"strength":"ë¬¸ì œ í•´ê²°", "evidence":"4ë²ˆì§¸ ë¬¸ë‹¨: 'í”„ë¡œì íŠ¸ì—ì„œ...'", "confidence": 0.85}}
  ]
}}"""

        summary_response = model.generate_content(summary_prompt)
        summary_result = json.loads(summary_response.text)
        
        # 2. STAR ì‚¬ë¡€ ì¶”ì¶œ
        star_prompt = f"""ë‹¤ìŒ í…ìŠ¤íŠ¸ì—ì„œ STAR(ìƒí™©, ê³¼ì œ, í–‰ë™, ê²°ê³¼) êµ¬ì¡°ì˜ ì‚¬ë¡€ë¥¼ ì°¾ì•„ ê°ê°ì„ ë¶„ë¦¬í•´ì„œ ë°˜í™˜í•´ë¼. ì°¾ì„ ìˆ˜ ì—†ìœ¼ë©´ ë¹ˆ ë°°ì—´ [] ë°˜í™˜.

í…ìŠ¤íŠ¸:
{text}

ë°˜ë“œì‹œ ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”. ë‹¤ë¥¸ í…ìŠ¤íŠ¸ëŠ” í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”:
[
  {{"s":"ìƒí™© ì„¤ëª…", "t":"ê³¼ì œ ì„¤ëª…", "a":"í–‰ë™ ì„¤ëª…", "r":"ê²°ê³¼ ì„¤ëª…", "evidence_sentence_indices":[2,3]}}
]"""

        star_response = model.generate_content(star_prompt)
        star_result = json.loads(star_response.text)
        
        # 3. ì§ë¬´ ì í•©ì„± ì ìˆ˜ ë° í‚¤ì›Œë“œ ë§¤ì¹­
        job_fit_prompt = f"""ë‹¹ì‹ ì€ ITê¸°ì—… ì±„ìš©ë‹´ë‹¹ì ì—­í• ì„ í•œë‹¤. ì±„ìš©ê³µê³ (ì§ë¬´ ì„¤ëª…)ë¥¼ ì£¼ë©´ ìì†Œì„œì˜ 'ì§ë¬´ ì í•©ì„±'ì„ 0~100ìœ¼ë¡œ ì ìˆ˜í™”í•˜ê³ , ì–´ë–¤ ìŠ¤í‚¬/ê²½í—˜ì´ ì¼ì¹˜í•˜ëŠ”ì§€, ë¶€ì¡±í•œ í‚¤ì›Œë“œëŠ” ë¬´ì—‡ì¸ì§€ ì •ë¦¬í•œë‹¤.

[í‰ê°€ ê¸°ì¤€]
- ê¸°ìˆ  ì í•©ì„±: ì‚¬ìš© ê¸°ìˆ  ìŠ¤íƒ, í”„ë¡œì íŠ¸ ê²½í—˜, ë¬¸ì œ í•´ê²° ëŠ¥ë ¥
- ì§ë¬´ ì´í•´ë„: í•´ë‹¹ í¬ì§€ì…˜ì˜ ì—­í• Â·ì±…ì„ì— ëŒ€í•œ ëª…í™•í•œ ì´í•´
- ì„±ì¥ ê°€ëŠ¥ì„±: í•™ìŠµ íƒœë„, ìƒˆë¡œìš´ ê¸°ìˆ  ìŠµë“ ê²½í—˜
- íŒ€ì›Œí¬/ì»¤ë®¤ë‹ˆì¼€ì´ì…˜: í˜‘ì—… ê²½í—˜, ê°ˆë“± í•´ê²° ì‚¬ë¡€
- ë™ê¸°/íšŒì‚¬ ì´í•´ë„: ì§€ì› ë™ê¸°, íšŒì‚¬ì™€ì˜ ê°€ì¹˜ê´€ ì¼ì¹˜ ì—¬ë¶€

ì§ë¬´ ì„¤ëª…: {job_description if job_description else "ì§ë¬´ ì„¤ëª…ì´ ì œê³µë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."}
ìì†Œì„œ: {text}

ë°˜ë“œì‹œ ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”. ë‹¤ë¥¸ í…ìŠ¤íŠ¸ëŠ” í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”:
{{
 "score": 78,
 "matched_skills":["Python", "íŒ€ë¦¬ë”©"],
 "missing_skills":["í´ë¼ìš°ë“œ","ë°ì´í„° íŒŒì´í”„ë¼ì¸"],
 "explanation":"ì§€ì›ìëŠ” Pythonê³¼ íŒ€ë¦¬ë”© ê²½í—˜ì´ ìš°ìˆ˜í•˜ì§€ë§Œ, í´ë¼ìš°ë“œ ê¸°ìˆ ê³¼ ë°ì´í„° íŒŒì´í”„ë¼ì¸ ê²½í—˜ì´ ë¶€ì¡±í•©ë‹ˆë‹¤."
}}"""

        job_fit_response = model.generate_content(job_fit_prompt)
        job_fit_result = json.loads(job_fit_response.text)
        
        # 4. ë¬¸ì¥ë³„ ê°œì„  ì œì•ˆ
        improvement_prompt = f"""ê° ë¬¸ì¥ì„ ë” ê°„ê²°í•˜ê³  ì ê·¹ì ìœ¼ë¡œ ë°”ê¿”ë¼. ì›ë˜ ë¬¸ì¥ê³¼ ê°œì„  ë¬¸ì¥ì„ ì§ì§€ì–´ ë°˜í™˜.

ìì†Œì„œ:
{text}

ë°˜ë“œì‹œ ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”. ë‹¤ë¥¸ í…ìŠ¤íŠ¸ëŠ” í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”:
[{{"original":"ì›ë˜ ë¬¸ì¥", "improved":"ê°œì„ ëœ ë¬¸ì¥"}}]"""

        improvement_response = model.generate_content(improvement_prompt)
        improvement_result = json.loads(improvement_response.text)
        
        # 5. ë¬¸ë²• ê²€ì‚¬ ë° êµì • ì œì•ˆ
        grammar_prompt = f"""ë‹¤ìŒ ìì†Œì„œì˜ ë¬¸ë²• ì˜¤ë¥˜ë¥¼ ì°¾ì•„ êµì • ì œì•ˆì„ í•´ì£¼ì„¸ìš”. ê° ì˜¤ë¥˜ì— ëŒ€í•´ ì›ë˜ ë¬¸ì¥ê³¼ êµì •ëœ ë¬¸ì¥ì„ ì§ì§€ì–´ ë°˜í™˜í•˜ì„¸ìš”.

ìì†Œì„œ:
{text}

ë°˜ë“œì‹œ ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”. ë‹¤ë¥¸ í…ìŠ¤íŠ¸ëŠ” í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”:
[{{"original":"ì›ë˜ ë¬¸ì¥", "corrected":"êµì •ëœ ë¬¸ì¥", "explanation":"ì˜¤ë¥˜ ì„¤ëª…"}}]"""

        grammar_response = model.generate_content(grammar_prompt)
        grammar_result = json.loads(grammar_response.text)
        
        # ì¢…í•© ì ìˆ˜ ê³„ì‚°
        overall_score = calculate_overall_score(summary_result, star_result, job_fit_result, improvement_result, grammar_result)
        
        return {
            "summary": summary_result.get("summary", ""),
            "top_strengths": summary_result.get("top_strengths", []),
            "star_cases": star_result if isinstance(star_result, list) else [],
            "job_fit_score": job_fit_result.get("score", 0),
            "matched_skills": job_fit_result.get("matched_skills", []),
            "missing_skills": job_fit_result.get("missing_skills", []),
            "grammar_suggestions": grammar_result if isinstance(grammar_result, list) else [],
            "improvement_suggestions": improvement_result if isinstance(improvement_result, list) else [],
            "overall_score": overall_score
        }
        
    except Exception as e:
        print(f"Gemini ë¶„ì„ ì‹¤íŒ¨, ê¸°ë³¸ ë¶„ì„ìœ¼ë¡œ ëŒ€ì²´: {e}")
        return await analyze_cover_letter_basic(text, job_description)

def calculate_overall_score(summary_result: Dict, star_result: List, job_fit_result: Dict, improvement_result: List, grammar_result: List) -> int:
    """ì¢…í•© ì ìˆ˜ ê³„ì‚° (0-100)"""
    score = 0
    
    # ìš”ì•½ ë° í•µì‹¬ ê°•ì  (20ì )
    if summary_result.get("summary") and summary_result.get("top_strengths"):
        score += 20
    
    # STAR ì‚¬ë¡€ (25ì )
    if star_result and len(star_result) > 0:
        score += min(25, len(star_result) * 8)
    
    # ì§ë¬´ ì í•©ì„± (30ì )
    job_fit_score = job_fit_result.get("score", 0)
    score += int(job_fit_score * 0.3)
    
    # ê°œì„  ì œì•ˆ (15ì )
    if improvement_result and len(improvement_result) > 0:
        score += min(15, len(improvement_result) * 2)
    
    # ë¬¸ë²• ê²€ì‚¬ (10ì )
    if grammar_result and len(grammar_result) > 0:
        score += min(10, len(grammar_result) * 1.5)
    
    return min(100, score)

# def generate_embedding(text: str) -> List[float]:
#     """í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„±"""
#     try:
#         model = SentenceTransformer('all-MiniLM-L6-v2')
#         embedding = model.encode([text])[0].tolist()
#         return embedding
#     except Exception as e:
#         print(f"ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {e}")
#         return []


# í™˜ê²½ ë³€ìˆ˜ì—ì„œ API í‚¤ ë¡œë“œ
GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY")
# PINECONE_API_KEY = os.getenv("PINECONE_API_KEY") 
# PINECONE_INDEX_NAME = os.getenv("PINECONE_INDEX_NAME", "resume-vectors")

# ì„œë¹„ìŠ¤ ì´ˆê¸°í™” (í˜„ì¬ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ)
# embedding_service = EmbeddingService()
# vector_service = VectorService(
#     api_key=PINECONE_API_KEY or "dummy-key",  # API í‚¤ê°€ ì—†ì–´ë„ ì„œë²„ ì‹œì‘ì€ ê°€ëŠ¥
#     index_name=PINECONE_INDEX_NAME
# )
# similarity_service = SimilarityService(embedding_service, vector_service)

# Pydantic ëª¨ë¸ë“¤
class User(BaseModel):
    id: Optional[str] = None
    username: str
    email: str
    role: str = "user"
    created_at: Optional[datetime] = None

class Resume(BaseModel):
    id: Optional[str] = None
    resume_id: Optional[str] = None
    name: str
    position: str
    department: str
    experience: str
    skills: str
    growthBackground: str
    motivation: str
    careerHistory: str
    analysisScore: int = 0
    analysisResult: str = ""
    status: str = "pending"
    created_at: Optional[datetime] = None

class ResumeChunk(BaseModel):
    id: Optional[str] = None
    resume_id: str
    chunk_id: str
    text: str
    start_pos: int
    end_pos: int
    chunk_index: int
    field_name: Optional[str] = None  # growthBackground, motivation, careerHistory
    metadata: Optional[Dict[str, Any]] = None
    vector_id: Optional[str] = None  # Pinecone ë²¡í„° ID
    created_at: Optional[datetime] = None

class Interview(BaseModel):
    id: Optional[str] = None
    user_id: str
    company: str
    position: str
    date: datetime
    status: str = "scheduled"
    created_at: Optional[datetime] = None

# ìì†Œì„œ ë¶„ì„ ê´€ë ¨ ëª¨ë¸ë“¤
class CoverLetterAnalysis(BaseModel):
    id: Optional[str] = None
    filename: str
    original_text: str
    summary: str
    top_strengths: List[Dict[str, Any]]
    star_cases: List[Dict[str, Any]]
    job_fit_score: int
    matched_skills: List[str]
    missing_skills: List[str]
    grammar_suggestions: List[Dict[str, str]]
    improvement_suggestions: List[Dict[str, str]]
    overall_score: int
    analysis_date: Optional[datetime] = None
    job_description: Optional[str] = None
    # embedding: Optional[List[float]] = None

class JobDescription(BaseModel):
    title: str
    description: str
    required_skills: List[str]
    preferred_skills: List[str]
    company: str
    position_level: str

# API ë¼ìš°íŠ¸ë“¤
@app.get("/")
async def root():
    return {"message": "AI ì±„ìš© ê´€ë¦¬ ì‹œìŠ¤í…œ APIê°€ ì‹¤í–‰ ì¤‘ì…ë‹ˆë‹¤."}

@app.get("/health")
async def health_check():
    return {"status": "healthy", "timestamp": datetime.now().isoformat()}

# ì´ë ¥ì„œ ë¶„ì„ API ì—”ë“œí¬ì¸íŠ¸ (ë ˆê±°ì‹œ: ìƒì„¸ ë¶„ì„ì€ routers.uploadì˜ /api/upload/analyze ì‚¬ìš©)
@app.post("/api/upload/analyze-legacy")
async def analyze_resume_legacy(
    file: UploadFile = File(...),
    job_description: str = Form(""),
    company: str = Form(""),
    position: str = Form("")
):
    """ì´ë ¥ì„œ íŒŒì¼ ì—…ë¡œë“œ ë° ë¶„ì„"""
    try:
        # íŒŒì¼ í¬ê¸° ì œí•œ (10MB)
        if file.size > 10 * 1024 * 1024:
            raise HTTPException(status_code=400, detail="íŒŒì¼ í¬ê¸°ëŠ” 10MBë¥¼ ì´ˆê³¼í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        # íŒŒì¼ ë‚´ìš© ì½ê¸°
        content = await file.read()
        
        # í…ìŠ¤íŠ¸ ì¶”ì¶œ
        extracted_text = extract_text_from_file(file.filename, content)
        
        # ë¬¸ë‹¨ ë° ë¬¸ì¥ ë¶„ë¦¬
        paragraphs = split_into_paragraphs(extracted_text)
        sentences = split_into_sentences(extracted_text)
        
        # LLMì„ ì‚¬ìš©í•œ ì´ë ¥ì„œ ë¶„ì„
        analysis_result = await analyze_cover_letter_with_llm(extracted_text, job_description)
        
        # ë¶„ì„ ê²°ê³¼ë¥¼ MongoDBì— ì €ì¥
        resume_doc = {
            "filename": file.filename,
            "original_text": extracted_text,
            "paragraphs": paragraphs,
            "sentences": sentences,
            "summary": analysis_result["summary"],
            "top_strengths": analysis_result["top_strengths"],
            "star_cases": analysis_result["star_cases"],
            "job_fit_score": analysis_result["job_fit_score"],
            "matched_skills": analysis_result["matched_skills"],
            "missing_skills": analysis_result["missing_skills"],
            "grammar_suggestions": analysis_result["grammar_suggestions"],
            "improvement_suggestions": analysis_result["improvement_suggestions"],
            "overall_score": analysis_result["overall_score"],
            "analysis_date": datetime.now(),
            "job_description": job_description,
            "company": company,
            "position": position
        }
        
        result = await db.resumes.insert_one(resume_doc)
        resume_doc["id"] = str(result.inserted_id)
        
        return {
            "message": "ì´ë ¥ì„œ ë¶„ì„(ë ˆê±°ì‹œ)ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.",
            "analysis_id": str(result.inserted_id),
            "analysis_result": analysis_result,
            "overall_summary": {
                "total_score": analysis_result["overall_score"] / 10,  # 0-100ì„ 0-10ìœ¼ë¡œ ë³€í™˜
                "summary": analysis_result["summary"],
                "top_strengths": analysis_result["top_strengths"],
                "star_cases": analysis_result["star_cases"],
                "job_fit_score": analysis_result["job_fit_score"],
                "matched_skills": analysis_result["matched_skills"],
                "missing_skills": analysis_result["missing_skills"],
                "grammar_suggestions": analysis_result["grammar_suggestions"],
                "improvement_suggestions": analysis_result["improvement_suggestions"]
            },
            "paragraphs_count": len(paragraphs),
            "sentences_count": len(sentences)
        }
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ì´ë ¥ì„œ ë¶„ì„ ì‹¤íŒ¨: {str(e)}")

# ìì†Œì„œ ë¶„ì„ ê´€ë ¨ API ì—”ë“œí¬ì¸íŠ¸ë“¤
@app.post("/api/cover-letter/upload")
async def upload_cover_letter(
    file: UploadFile = File(...),
    job_description: str = Form(""),
    company: str = Form(""),
    position: str = Form("")
):
    """ìì†Œì„œ íŒŒì¼ ì—…ë¡œë“œ ë° ë¶„ì„"""
    try:
        # íŒŒì¼ í¬ê¸° ì œí•œ (10MB)
        if file.size > 10 * 1024 * 1024:
            raise HTTPException(status_code=400, detail="íŒŒì¼ í¬ê¸°ëŠ” 10MBë¥¼ ì´ˆê³¼í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        # íŒŒì¼ ë‚´ìš© ì½ê¸°
        content = await file.read()
        
        # í…ìŠ¤íŠ¸ ì¶”ì¶œ
        extracted_text = extract_text_from_file(file.filename, content)
        
        # ë¬¸ë‹¨ ë° ë¬¸ì¥ ë¶„ë¦¬
        paragraphs = split_into_paragraphs(extracted_text)
        sentences = split_into_sentences(extracted_text)
        
        # LLMì„ ì‚¬ìš©í•œ ìì†Œì„œ ë¶„ì„
        analysis_result = await analyze_cover_letter_with_llm(extracted_text, job_description)
        
        # ì„ë² ë”© ìƒì„± (í˜„ì¬ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ)
        # embedding = generate_embedding(extracted_text)
        
        # ë¶„ì„ ê²°ê³¼ë¥¼ MongoDBì— ì €ì¥
        cover_letter_doc = {
            "filename": file.filename,
            "original_text": extracted_text,
            "paragraphs": paragraphs,
            "sentences": sentences,
            "summary": analysis_result["summary"],
            "top_strengths": analysis_result["top_strengths"],
            "star_cases": analysis_result["star_cases"],
            "job_fit_score": analysis_result["job_fit_score"],
            "matched_skills": analysis_result["matched_skills"],
            "missing_skills": analysis_result["missing_skills"],
            "grammar_suggestions": analysis_result["grammar_suggestions"],
            "improvement_suggestions": analysis_result["improvement_suggestions"],
            "overall_score": analysis_result["overall_score"],
            "analysis_date": datetime.now(),
            "job_description": job_description,
            "company": company,
            "position": position,
            # "embedding": embedding  # í˜„ì¬ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ
        }
        
        result = await db.cover_letters.insert_one(cover_letter_doc)
        cover_letter_doc["id"] = str(result.inserted_id)
        
        return {
            "message": "ìì†Œì„œ ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.",
            "analysis_id": str(result.inserted_id),
            "analysis_result": analysis_result,
            "overall_summary": {
                "total_score": analysis_result["overall_score"] / 10,  # 0-100ì„ 0-10ìœ¼ë¡œ ë³€í™˜
                "summary": analysis_result["summary"],
                "top_strengths": analysis_result["top_strengths"],
                "star_cases": analysis_result["star_cases"],
                "job_fit_score": analysis_result["job_fit_score"],
                "matched_skills": analysis_result["matched_skills"],
                "missing_skills": analysis_result["missing_skills"],
                "grammar_suggestions": analysis_result["grammar_suggestions"],
                "improvement_suggestions": analysis_result["improvement_suggestions"]
            },
            "paragraphs_count": len(paragraphs),
            "sentences_count": len(sentences)
        }
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ìì†Œì„œ ë¶„ì„ ì‹¤íŒ¨: {str(e)}")

@app.get("/api/cover-letter/{analysis_id}")
async def get_cover_letter_analysis(analysis_id: str):
    """ìì†Œì„œ ë¶„ì„ ê²°ê³¼ ì¡°íšŒ"""
    try:
        from bson import ObjectId
        analysis = await db.cover_letters.find_one({"_id": ObjectId(analysis_id)})
        
        if not analysis:
            raise HTTPException(status_code=404, detail="ë¶„ì„ ê²°ê³¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        analysis["id"] = str(analysis["_id"])
        del analysis["_id"]
        
        return analysis
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ë¶„ì„ ê²°ê³¼ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")

@app.get("/api/cover-letter/list")
async def list_cover_letter_analyses(skip: int = 0, limit: int = 20):
    """ìì†Œì„œ ë¶„ì„ ê²°ê³¼ ëª©ë¡ ì¡°íšŒ"""
    try:
        analyses = await db.cover_letters.find().skip(skip).limit(limit).to_list(limit)
        
        for analysis in analyses:
            analysis["id"] = str(analysis["_id"])
            del analysis["_id"]
        
        total_count = await db.cover_letters.count_documents({})
        
        return {
            "analyses": analyses,
            "total_count": total_count,
            "skip": skip,
            "limit": limit,
            "has_more": (skip + limit) < total_count
        }
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ë¶„ì„ ê²°ê³¼ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")

@app.post("/api/cover-letter/similar")
async def find_similar_cover_letters(
    text: str = Form(...),
    limit: int = Form(5)
):
    """ìœ ì‚¬í•œ ìì†Œì„œ ê²€ìƒ‰ (ë²¡í„° ìœ ì‚¬ë„ ê¸°ë°˜)"""
    try:
        # ì…ë ¥ í…ìŠ¤íŠ¸ì˜ ì„ë² ë”© ìƒì„± (ë¹„í™œì„±í™”)
        query_embedding = []
        
        # MongoDBì—ì„œ ìœ ì‚¬í•œ ì„ë² ë”© ê²€ìƒ‰ (ê°„ë‹¨í•œ ìœ í´ë¦¬ë“œ ê±°ë¦¬ ì‚¬ìš©)
        all_analyses = await db.cover_letters.find({"embedding": {"$exists": True}}).to_list(1000)
        
        # ìœ ì‚¬ë„ ê³„ì‚° ë° ì •ë ¬
        similarities = []
        for analysis in all_analyses:
            if analysis.get("embedding"):
                # ì„ì‹œ ìœ ì‚¬ë„ ê°’ (ì„ë² ë”© ë¹„í™œì„±í™” ìƒíƒœ)
                import random
                distance = 1.0 - random.uniform(0.0, 1.0)
                similarities.append((distance, analysis))
        
        # ê±°ë¦¬ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬ (ê°€ê¹Œìš¸ìˆ˜ë¡ ìœ ì‚¬)
        similarities.sort(key=lambda x: x[0])
        
        # ìƒìœ„ ê²°ê³¼ ë°˜í™˜
        top_results = []
        for distance, analysis in similarities[:limit]:
            analysis["id"] = str(analysis["_id"])
            analysis["similarity_score"] = 1.0 / (1.0 + distance)  # 0-1 ë²”ìœ„ì˜ ìœ ì‚¬ë„ ì ìˆ˜
            del analysis["_id"]
            del analysis["embedding"]  # ì„ë² ë”©ì€ ì œì™¸
            top_results.append(analysis)
        
        return {
            "query_text": text,
            "similar_analyses": top_results,
            "total_found": len(top_results)
        }
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ìœ ì‚¬ ìì†Œì„œ ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}")

@app.post("/api/cover-letter/improve")
async def get_improvement_suggestions(
    text: str = Form(...),
    focus_area: str = Form("all")  # "grammar", "style", "content", "all"
):
    """ìì†Œì„œ ê°œì„  ì œì•ˆ ìƒì„±"""
    try:
        if focus_area == "grammar":
            prompt = f"""ë‹¤ìŒ ìì†Œì„œì˜ ë¬¸ë²• ì˜¤ë¥˜ë¥¼ ì°¾ì•„ êµì • ì œì•ˆì„ í•´ì£¼ì„¸ìš”. ê° ì˜¤ë¥˜ì— ëŒ€í•´ ì›ë˜ ë¬¸ì¥ê³¼ êµì •ëœ ë¬¸ì¥ì„ ì§ì§€ì–´ ë°˜í™˜í•˜ì„¸ìš”.

ìì†Œì„œ:
{text}

Response: [{{"original":"...", "corrected":"...", "explanation":"ì˜¤ë¥˜ ì„¤ëª…"}}]"""
        elif focus_area == "style":
            prompt = f"""ë‹¤ìŒ ìì†Œì„œë¥¼ ë” ê°„ê²°í•˜ê³  ì ê·¹ì ìœ¼ë¡œ ë°”ê¿”ë¼. ì›ë˜ ë¬¸ì¥ê³¼ ê°œì„  ë¬¸ì¥ì„ ì§ì§€ì–´ ë°˜í™˜.

ìì†Œì„œ:
{text}

Response: [{{"original":"...", "improved":"... (í•œ ì¤„)"}}]"""
        elif focus_area == "content":
            prompt = f"""ë‹¤ìŒ ìì†Œì„œì˜ ë‚´ìš©ì„ ê°œì„ í•˜ì—¬ ë” êµ¬ì²´ì ì´ê³  ì„¤ë“ë ¥ ìˆê²Œ ë§Œë“¤ì–´ì£¼ì„¸ìš”. ê° ë¬¸ì¥ì— ëŒ€í•œ ê°œì„  ì œì•ˆì„ ì œê³µí•˜ì„¸ìš”.

ìì†Œì„œ:
{text}

Response: [{{"original":"...", "improved":"...", "explanation":"ê°œì„  ì´ìœ "}}]"""
        else:
            prompt = f"""ë‹¤ìŒ ìì†Œì„œë¥¼ ì¢…í•©ì ìœ¼ë¡œ ê°œì„ í•´ì£¼ì„¸ìš”. ë¬¸ë²•, ìŠ¤íƒ€ì¼, ë‚´ìš©ì„ ëª¨ë‘ ê³ ë ¤í•˜ì—¬ ê°œì„  ì œì•ˆì„ ì œê³µí•˜ì„¸ìš”.

ìì†Œì„œ:
{text}

Response: [{{"original":"...", "improved":"...", "explanation":"ê°œì„  ì´ìœ "}}]"""
        
        if not GEMINI_AVAILABLE:
            raise HTTPException(status_code=400, detail="Gemini APIê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        # Gemini ëª¨ë¸ ì´ˆê¸°í™”
        model = genai.GenerativeModel('gemini-1.5-flash')
        response = model.generate_content(prompt)
        
        improvement_result = json.loads(response.text)
        
        return {
            "focus_area": focus_area,
            "original_text": text,
            "improvements": improvement_result if isinstance(improvement_result, list) else []
        }
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ê°œì„  ì œì•ˆ ìƒì„± ì‹¤íŒ¨: {str(e)}")

# ì‚¬ìš©ì ê´€ë ¨ API
@app.get("/api/users", response_model=List[User])
async def get_users():
    users = await db.users.find().to_list(1000)
    # MongoDBì˜ _idë¥¼ idë¡œ ë³€í™˜
    for user in users:
        user["id"] = str(user["_id"])
        del user["_id"]
    return [User(**user) for user in users]

@app.post("/api/users", response_model=User)
async def create_user(user: User):
    user_dict = user.dict()
    user_dict["created_at"] = datetime.now()
    result = await db.users.insert_one(user_dict)
    user_dict["id"] = str(result.inserted_id)
    return User(**user_dict)

# ì´ë ¥ì„œ ê´€ë ¨ API
@app.get("/api/resumes", response_model=List[Resume])
async def get_resumes():
    resumes = await db.resumes.find().to_list(1000)
    # MongoDBì˜ _idë¥¼ idë¡œ ë³€í™˜
    for resume in resumes:
        resume["id"] = str(resume["_id"])
        del resume["_id"]
    return [Resume(**resume) for resume in resumes]

@app.post("/api/resumes", response_model=Resume)
async def create_resume(resume: Resume):
    resume_dict = resume.dict()
    resume_dict["created_at"] = datetime.now()
    result = await db.resumes.insert_one(resume_dict)
    resume_dict["id"] = str(result.inserted_id)
    return Resume(**resume_dict)

# ë©´ì ‘ ê´€ë ¨ API
@app.get("/api/interviews", response_model=List[Interview])
async def get_interviews():
    interviews = await db.interviews.find().to_list(1000)
    # MongoDBì˜ _idë¥¼ idë¡œ ë³€í™˜
    for interview in interviews:
        interview["id"] = str(interview["_id"])
        del interview["_id"]
    return [Interview(**interview) for interview in interviews]

@app.post("/api/interviews", response_model=Interview)
async def create_interview(interview: Interview):
    interview_dict = interview.dict()
    interview_dict["created_at"] = datetime.now()
    result = await db.interviews.insert_one(interview_dict)
    interview_dict["id"] = str(result.inserted_id)
    return Interview(**interview_dict)

# ì§€ì›ì ê´€ë ¨ API
@app.get("/api/applicants")
async def get_applicants(skip: int = 0, limit: int = 20):
    try:
        # í˜ì´ì§•ìœ¼ë¡œ ì§€ì›ì ëª©ë¡ ì¡°íšŒ (applicants ì»¬ë ‰ì…˜ ì‚¬ìš©)
        applicants = await db.applicants.find().skip(skip).limit(limit).to_list(limit)
        
        # MongoDBì˜ _idë¥¼ idë¡œ ë³€í™˜ ë° í•„ìš”í•œ í•„ë“œë“¤ ì¶”ê°€
        for applicant in applicants:
            applicant["id"] = str(applicant["_id"])
            del applicant["_id"]
            
            # í•„ìˆ˜ í•„ë“œë“¤ì´ ì—†ëŠ” ê²½ìš° ê¸°ë³¸ê°’ ì„¤ì •
            if "email" not in applicant:
                applicant["email"] = "ì´ë©”ì¼ ì •ë³´ ì—†ìŒ"
            if "phone" not in applicant:
                applicant["phone"] = "ì „í™”ë²ˆí˜¸ ì •ë³´ ì—†ìŒ"
            if "appliedDate" not in applicant:
                applicant["appliedDate"] = applicant.get("created_at", "ì§€ì›ì¼ ì •ë³´ ì—†ìŒ")
            if "skills" not in applicant:
                applicant["skills"] = applicant.get("skills", "ê¸°ìˆ  ì •ë³´ ì—†ìŒ")
        
        # ì´ ì§€ì›ì ìˆ˜
        total_count = await db.applicants.count_documents({})
        
        return {
            "applicants": applicants,
            "total_count": total_count,
            "skip": skip,
            "limit": limit,
            "has_more": (skip + limit) < total_count
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ì§€ì›ì ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")

# ì§€ì›ì í†µê³„ API
@app.get("/api/applicants/stats/overview")
async def get_applicant_stats():
    try:
        # ì´ ì§€ì›ì ìˆ˜ (applicants ì»¬ë ‰ì…˜ ê¸°ì¤€)
        total_applicants = await db.applicants.count_documents({})
        
        # ìƒíƒœë³„ ì§€ì›ì ìˆ˜
        pending_count = await db.applicants.count_documents({"status": "pending"})
        approved_count = await db.applicants.count_documents({"status": "approved"})
        rejected_count = await db.applicants.count_documents({"status": "rejected"})
        
        # ìµœê·¼ 30ì¼ê°„ ì§€ì›ì ìˆ˜
        thirty_days_ago = datetime.now().replace(day=datetime.now().day-30) if datetime.now().day > 30 else datetime.now().replace(month=datetime.now().month-1, day=1)
        recent_applicants = await db.applicants.count_documents({
            "created_at": {"$gte": thirty_days_ago}
        })
        
        return {
            "total_applicants": total_applicants,
            "status_breakdown": {
                "pending": pending_count,
                "approved": approved_count,
                "rejected": rejected_count
            },
            "recent_applicants_30_days": recent_applicants,
            "success_rate": round((approved_count / total_applicants * 100) if total_applicants > 0 else 0, 2)
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ì§€ì›ì í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")

# Vector Service API
@app.post("/api/vector/create")
async def create_vector(data: Dict[str, Any]):
    """í…ìŠ¤íŠ¸ë¥¼ ë²¡í„°ë¡œ ë³€í™˜í•˜ì—¬ ì €ì¥"""
    try:
        text = data.get("text", "")
        document_id = data.get("document_id")
        metadata = data.get("metadata", {})
        
        # ì—¬ê¸°ì„œ ì‹¤ì œ ë²¡í„°í™” ë¡œì§ êµ¬í˜„
        # ì˜ˆ: embedding_modelì„ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ë¥¼ ë²¡í„°ë¡œ ë³€í™˜
        
        # ì„ì‹œë¡œ ì„±ê³µ ì‘ë‹µ ë°˜í™˜
        return {
            "message": "Vector created successfully",
            "document_id": document_id,
            "vector_dimension": 384,  # ì˜ˆì‹œ ì°¨ì›
            "status": "success"
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ë²¡í„° ìƒì„± ì‹¤íŒ¨: {str(e)}")

@app.post("/api/vector/search")
async def search_vectors(data: Dict[str, Any]):
    """ë²¡í„° ìœ ì‚¬ë„ ê²€ìƒ‰"""
    try:
        query_text = data.get("query", "")
        top_k = data.get("top_k", 5)
        threshold = data.get("threshold", 0.7)
        
        # ì—¬ê¸°ì„œ ì‹¤ì œ ë²¡í„° ê²€ìƒ‰ ë¡œì§ êµ¬í˜„
        
        # ì„ì‹œë¡œ ê²€ìƒ‰ ê²°ê³¼ ë°˜í™˜
        return {
            "results": [
                {
                    "document_id": "doc_001",
                    "score": 0.95,
                    "text": "ê²€ìƒ‰ëœ í…ìŠ¤íŠ¸ ìƒ˜í”Œ 1",
                    "metadata": {"type": "resume", "applicant_id": "app_001"}
                },
                {
                    "document_id": "doc_002", 
                    "score": 0.87,
                    "text": "ê²€ìƒ‰ëœ í…ìŠ¤íŠ¸ ìƒ˜í”Œ 2",
                    "metadata": {"type": "cover_letter", "applicant_id": "app_002"}
                }
            ],
            "total_found": 2
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ë²¡í„° ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}")

# Chunking Service API
@app.post("/api/chunking/split")
async def split_text(data: Dict[str, Any]):
    """í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë¶„í• í•˜ê³  DBì— ì €ì¥"""
    try:
        text = data.get("text", "")
        resume_id = data.get("resume_id")
        field_name = data.get("field_name", "")  # growthBackground, motivation, careerHistory
        chunk_size = data.get("chunk_size", 1000)
        chunk_overlap = data.get("chunk_overlap", 200)
        split_type = data.get("split_type", "recursive")
        
        if not resume_id:
            raise HTTPException(status_code=400, detail="resume_idê°€ í•„ìš”í•©ë‹ˆë‹¤.")
        
        # í…ìŠ¤íŠ¸ ë¶„í•  ë¡œì§
        chunks = []
        text_length = len(text)
        start = 0
        chunk_index = 0
        
        while start < text_length:
            end = min(start + chunk_size, text_length)
            chunk_text = text[start:end]
            
            if chunk_text.strip():  # ë¹ˆ ì²­í¬ëŠ” ì œì™¸
                chunk_id = f"chunk_{chunk_index:03d}"
                vector_id = f"resume_{resume_id}_{chunk_id}"
                
                chunk_doc = {
                    "resume_id": resume_id,
                    "chunk_id": chunk_id,
                    "text": chunk_text,
                    "start_pos": start,
                    "end_pos": end,
                    "chunk_index": chunk_index,
                    "field_name": field_name,
                    "vector_id": vector_id,
                    "metadata": {
                        "length": len(chunk_text),
                        "split_type": split_type,
                        "chunk_size": chunk_size,
                        "chunk_overlap": chunk_overlap
                    },
                    "created_at": datetime.now()
                }
                
                # MongoDBì— ì²­í¬ ì €ì¥
                result = await db.resume_chunks.insert_one(chunk_doc)
                chunk_doc["id"] = str(result.inserted_id)
                
                chunks.append(chunk_doc)
                chunk_index += 1
            
            start = end - chunk_overlap if chunk_overlap > 0 else end
            
            if start >= text_length:
                break
        
        return {
            "chunks": chunks,
            "total_chunks": len(chunks),
            "original_length": text_length,
            "resume_id": resume_id,
            "field_name": field_name,
            "split_config": {
                "chunk_size": chunk_size,
                "chunk_overlap": chunk_overlap,
                "split_type": split_type
            }
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"í…ìŠ¤íŠ¸ ë¶„í•  ì‹¤íŒ¨: {str(e)}")

@app.get("/api/chunking/resume/{resume_id}")
async def get_resume_chunks(resume_id: str):
    """íŠ¹ì • ì´ë ¥ì„œì˜ ëª¨ë“  ì²­í¬ ì¡°íšŒ"""
    try:
        chunks = await db.resume_chunks.find({"resume_id": resume_id}).to_list(1000)
        
        # MongoDBì˜ _idë¥¼ idë¡œ ë³€í™˜
        for chunk in chunks:
            chunk["id"] = str(chunk["_id"])
            del chunk["_id"]
        
        return {
            "resume_id": resume_id,
            "chunks": [ResumeChunk(**chunk) for chunk in chunks],
            "total_chunks": len(chunks)
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ì²­í¬ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")

@app.post("/api/chunking/process-resume")
async def process_resume_with_chunking(data: Dict[str, Any]):
    """ì´ë ¥ì„œ ì „ì²´ë¥¼ í•„ë“œë³„ë¡œ ì²­í‚¹ ì²˜ë¦¬"""
    try:
        resume_id = data.get("resume_id")
        if not resume_id:
            raise HTTPException(status_code=400, detail="resume_idê°€ í•„ìš”í•©ë‹ˆë‹¤.")
        
        # ì´ë ¥ì„œ ì •ë³´ ì¡°íšŒ
        resume = await db.resumes.find_one({"_id": ObjectId(resume_id)})
        if not resume:
            raise HTTPException(status_code=404, detail="ì´ë ¥ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        chunk_size = data.get("chunk_size", 800)
        chunk_overlap = data.get("chunk_overlap", 150)
        
        # ì²­í‚¹í•  í•„ë“œë“¤
        fields_to_chunk = ["growthBackground", "motivation", "careerHistory"]
        all_chunks = []
        
        for field_name in fields_to_chunk:
            field_text = resume.get(field_name, "")
            if field_text and field_text.strip():
                # í•„ë“œë³„ ì²­í‚¹ ì²˜ë¦¬
                field_chunks = []
                text_length = len(field_text)
                start = 0
                chunk_index = 0
                
                while start < text_length:
                    end = min(start + chunk_size, text_length)
                    chunk_text = field_text[start:end]
                    
                    if chunk_text.strip():
                        chunk_id = f"{field_name}_chunk_{chunk_index:03d}"
                        vector_id = f"resume_{resume_id}_{chunk_id}"
                        
                        chunk_doc = {
                            "resume_id": resume_id,
                            "chunk_id": chunk_id,
                            "text": chunk_text,
                            "start_pos": start,
                            "end_pos": end,
                            "chunk_index": chunk_index,
                            "field_name": field_name,
                            "vector_id": vector_id,
                            "metadata": {
                                "applicant_name": resume.get("name", ""),
                                "position": resume.get("position", ""),
                                "department": resume.get("department", ""),
                                "length": len(chunk_text)
                            },
                            "created_at": datetime.now()
                        }
                        
                        result = await db.resume_chunks.insert_one(chunk_doc)
                        chunk_doc["id"] = str(result.inserted_id)
                        field_chunks.append(chunk_doc)
                        chunk_index += 1
                    
                    start = end - chunk_overlap if chunk_overlap > 0 else end
                    if start >= text_length:
                        break
                
                all_chunks.extend(field_chunks)
        
        return {
            "resume_id": resume_id,
            "applicant_name": resume.get("name", ""),
            "processed_fields": fields_to_chunk,
            "total_chunks": len(all_chunks),
            "chunks_by_field": {
                field: len([c for c in all_chunks if c["field_name"] == field])
                for field in fields_to_chunk
            },
            "chunks": all_chunks
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ì´ë ¥ì„œ ì²­í‚¹ ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")

@app.post("/api/chunking/merge")
async def merge_chunks(data: Dict[str, Any]):
    """ì²­í¬ë“¤ì„ ë³‘í•©"""
    try:
        chunks = data.get("chunks", [])
        separator = data.get("separator", "\n\n")
        
        # ì²­í¬ ë³‘í•©
        merged_text = separator.join([chunk.get("text", "") for chunk in chunks])
        
        return {
            "merged_text": merged_text,
            "total_length": len(merged_text),
            "chunks_merged": len(chunks),
            "separator_used": separator
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ì²­í¬ ë³‘í•© ì‹¤íŒ¨: {str(e)}")

# Similarity Service API
@app.post("/api/similarity/compare")
async def compare_similarity(data: Dict[str, Any]):
    """ë‘ í…ìŠ¤íŠ¸ ê°„ì˜ ìœ ì‚¬ë„ ê³„ì‚°"""
    try:
        text1 = data.get("text1", "")
        text2 = data.get("text2", "")
        method = data.get("method", "cosine")  # cosine, jaccard, levenshtein
        
        # ì—¬ê¸°ì„œ ì‹¤ì œ ìœ ì‚¬ë„ ê³„ì‚° ë¡œì§ êµ¬í˜„
        # ì˜ˆ: sentence-transformersì˜ cosine similarity
        
        # ì„ì‹œë¡œ ìœ ì‚¬ë„ ì ìˆ˜ ë°˜í™˜
        import random
        similarity_score = random.uniform(0.3, 0.95)  # ì„ì‹œ ì ìˆ˜
        
        return {
            "similarity_score": round(similarity_score, 4),
            "method": method,
            "text1_length": len(text1),
            "text2_length": len(text2),
            "comparison_result": {
                "highly_similar": similarity_score > 0.8,
                "moderately_similar": 0.5 < similarity_score <= 0.8,
                "low_similar": similarity_score <= 0.5
            }
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ìœ ì‚¬ë„ ê³„ì‚° ì‹¤íŒ¨: {str(e)}")

@app.post("/api/similarity/batch")
async def batch_similarity(data: Dict[str, Any]):
    """ì—¬ëŸ¬ í…ìŠ¤íŠ¸ë“¤ ê°„ì˜ ì¼ê´„ ìœ ì‚¬ë„ ê³„ì‚°"""
    try:
        texts = data.get("texts", [])
        reference_text = data.get("reference_text", "")
        method = data.get("method", "cosine")
        threshold = data.get("threshold", 0.7)
        
        # ë°°ì¹˜ ìœ ì‚¬ë„ ê³„ì‚°
        results = []
        import random
        
        for i, text in enumerate(texts):
            similarity_score = random.uniform(0.2, 0.95)  # ì„ì‹œ ì ìˆ˜
            results.append({
                "index": i,
                "text_preview": text[:100] + "..." if len(text) > 100 else text,
                "similarity_score": round(similarity_score, 4),
                "above_threshold": similarity_score >= threshold
            })
        
        # ì„ê³„ê°’ ì´ìƒì¸ ê²°ê³¼ë“¤ í•„í„°ë§
        filtered_results = [r for r in results if r["above_threshold"]]
        
        return {
            "results": results,
            "filtered_results": filtered_results,
            "total_compared": len(texts),
            "above_threshold_count": len(filtered_results),
            "method": method,
            "threshold": threshold,
            "reference_text_length": len(reference_text)
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ë°°ì¹˜ ìœ ì‚¬ë„ ê³„ì‚° ì‹¤íŒ¨: {str(e)}")

@app.get("/api/similarity/metrics")
async def get_similarity_metrics():
    """ìœ ì‚¬ë„ ì„œë¹„ìŠ¤ ë©”íŠ¸ë¦­ ì¡°íšŒ"""
    try:
        # ì„ì‹œ ë©”íŠ¸ë¦­ ë°ì´í„°
        return {
            "total_comparisons": 1250,
            "average_similarity": 0.67,
            "supported_methods": ["cosine", "jaccard", "levenshtein", "semantic"],
            "performance_stats": {
                "average_processing_time_ms": 45,
                "comparisons_per_second": 220,
                "cache_hit_rate": 0.78
            },
            "usage_by_method": {
                "cosine": 850,
                "semantic": 300,
                "jaccard": 70,
                "levenshtein": 30
            }
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ë©”íŠ¸ë¦­ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")

# ì´ë ¥ì„œ ìœ ì‚¬ë„ ì²´í¬ API
@app.post("/api/resume/similarity-check/{resume_id}")
async def check_resume_similarity(resume_id: str):
    """íŠ¹ì • ì´ë ¥ì„œì˜ ìœ ì‚¬ë„ ì²´í¬ (ë‹¤ë¥¸ ëª¨ë“  ì´ë ¥ì„œì™€ ë¹„êµ)"""
    try:
        print(f"ğŸ” ìœ ì‚¬ë„ ì²´í¬ ìš”ì²­ - resume_id: {resume_id}")
        
        # ObjectId ë³€í™˜ ì‹œë„
        try:
            object_id = ObjectId(resume_id)
            print(f"âœ… ObjectId ë³€í™˜ ì„±ê³µ: {object_id}")
        except Exception as oid_error:
            print(f"âŒ ObjectId ë³€í™˜ ì‹¤íŒ¨: {oid_error}")
            raise HTTPException(status_code=400, detail=f"ì˜ëª»ëœ resume_id í˜•ì‹: {resume_id}")
        
        # í˜„ì¬ ì´ë ¥ì„œ ì •ë³´ ì¡°íšŒ
        current_resume = await db.resumes.find_one({"_id": object_id})
        print(f"ğŸ” ë°ì´í„°ë² ì´ìŠ¤ ì¡°íšŒ ê²°ê³¼: {current_resume is not None}")
        
        if not current_resume:
            # ìƒì„¸ ë¤í”„ ëŒ€ì‹  ê°„ë‹¨í•œ ì§„ë‹¨ë§Œ ë‚¨ê¸°ê³  404 ë°˜í™˜
            raise HTTPException(status_code=404, detail="ì´ë ¥ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        # ë‹¤ë¥¸ ëª¨ë“  ì´ë ¥ì„œ ì¡°íšŒ (í˜„ì¬ ì´ë ¥ì„œ ì œì™¸)
        other_resumes = await db.resumes.find({"_id": {"$ne": ObjectId(resume_id)}}).to_list(1000)
        
        # í˜„ì¬ ì´ë ¥ì„œì˜ ë¹„êµ í…ìŠ¤íŠ¸ (ìœ ì‚¬ë„ ê³„ì‚° í•„ë“œ)
        current_fields = {
            "growthBackground": current_resume.get("growthBackground", ""),
            "motivation": current_resume.get("motivation", ""),
            "careerHistory": current_resume.get("careerHistory", "")
        }
        
        # ì „ì²´ í…ìŠ¤íŠ¸ ì¡°í•©
        current_text = " ".join([text for text in current_fields.values() if text])
        
        similarity_results = []
        
        for other_resume in other_resumes:
            other_id = str(other_resume["_id"])
            
            # ë‹¤ë¥¸ ì´ë ¥ì„œì˜ ë¹„êµ í…ìŠ¤íŠ¸
            other_fields = {
                "growthBackground": other_resume.get("growthBackground", ""),
                "motivation": other_resume.get("motivation", ""), 
                "careerHistory": other_resume.get("careerHistory", "")
            }
            other_text = " ".join([text for text in other_fields.values() if text])
            
            # ì‹¤ì œ ìœ ì‚¬ë„ ê³„ì‚° ì‚¬ìš©
            try:
                print(f"ğŸ’« ì´ë ¥ì„œ ê°„ ìœ ì‚¬ë„ ê³„ì‚° ì‹œì‘: {resume_id} vs {other_id}")
                
                # SimilarityServiceì˜ í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ ê³„ì‚° ë©”ì„œë“œ ì§ì ‘ í˜¸ì¶œ (í˜„ì¬ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ)
                # text_similarity = similarity_service._calculate_text_similarity(current_resume, other_resume)
                # overall_similarity = text_similarity if text_similarity is not None else 0.0
                
                # ì„ì‹œë¡œ ëœë¤ ê°’ ì‚¬ìš©
                overall_similarity = random.uniform(0.1, 0.9)
                
                print(f"ğŸ“Š í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ ê²°ê³¼: {overall_similarity:.3f}")
                
                # í•„ë“œë³„ ìœ ì‚¬ë„ ê³„ì‚° (ì„ì‹œë¡œ ëœë¤ ê°’ ì‚¬ìš©)
                field_similarities = {}
                for field_name in current_fields.keys():
                    if current_fields[field_name] and other_fields[field_name]:
                        # í•„ë“œë³„ ê°œë³„ í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ ê³„ì‚° (ì„ì‹œë¡œ ëœë¤ ê°’ ì‚¬ìš©)
                        # field_sim = similarity_service._calculate_text_similarity(
                        #     {field_name: current_fields[field_name]},
                        #     {field_name: other_fields[field_name]}
                        # )
                        field_similarities[field_name] = random.uniform(0.0, 1.0)
                        print(f"ğŸ“‹ {field_name} ìœ ì‚¬ë„: {field_similarities[field_name]:.3f}")
                    else:
                        field_similarities[field_name] = 0.0
                        
            except Exception as e:
                print(f"âŒ ìœ ì‚¬ë„ ê³„ì‚° ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                import traceback
                traceback.print_exc()
                
                # ì˜¤ë¥˜ ì‹œ ê¸°ë³¸ê°’ ì‚¬ìš©
                overall_similarity = random.uniform(0.1, 0.9)
                field_similarities = {}
                for field_name in current_fields.keys():
                    if current_fields[field_name] and other_fields[field_name]:
                        field_similarities[field_name] = random.uniform(0.0, 1.0)
                    else:
                        field_similarities[field_name] = 0.0
            
            similarity_result = {
                "resume_id": other_id,
                "applicant_name": other_resume.get("name", "ì•Œ ìˆ˜ ì—†ìŒ"),
                "position": other_resume.get("position", ""),
                "department": other_resume.get("department", ""),
                "overall_similarity": round(overall_similarity, 4),
                "field_similarities": {
                    "growthBackground": round(field_similarities["growthBackground"], 4),
                    "motivation": round(field_similarities["motivation"], 4),
                    "careerHistory": round(field_similarities["careerHistory"], 4)
                },
                "is_high_similarity": overall_similarity > 0.7,
                "is_moderate_similarity": 0.4 <= overall_similarity <= 0.7,
                "is_low_similarity": overall_similarity < 0.4
            }
            
            similarity_results.append(similarity_result)
        
        # ìœ ì‚¬ë„ ë†’ì€ ìˆœìœ¼ë¡œ ì •ë ¬
        similarity_results.sort(key=lambda x: x["overall_similarity"], reverse=True)
        
        # í†µê³„ ì •ë³´
        high_similarity_count = len([r for r in similarity_results if r["is_high_similarity"]])
        moderate_similarity_count = len([r for r in similarity_results if r["is_moderate_similarity"]])
        low_similarity_count = len([r for r in similarity_results if r["is_low_similarity"]])
        
        return {
            "current_resume": {
                "id": resume_id,
                "name": current_resume.get("name", ""),
                "position": current_resume.get("position", ""),
                "department": current_resume.get("department", "")
            },
            "similarity_results": similarity_results,
            "statistics": {
                "total_compared": len(similarity_results),
                "high_similarity_count": high_similarity_count,
                "moderate_similarity_count": moderate_similarity_count,
                "low_similarity_count": low_similarity_count,
                "average_similarity": round(sum([r["overall_similarity"] for r in similarity_results]) / len(similarity_results) if similarity_results else 0, 4)
            },
            "top_similar": similarity_results[:5] if similarity_results else [],
            "analysis_timestamp": datetime.now().isoformat()
        }
        
    except HTTPException:
        # ëª…ì‹œì ìœ¼ë¡œ ë°œìƒì‹œí‚¨ 4xxëŠ” ê·¸ëŒ€ë¡œ ì „ë‹¬
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ìœ ì‚¬ë„ ì²´í¬ ì‹¤íŒ¨: {str(e)}")

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8000) 