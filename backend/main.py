import os
import sys
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.middleware.trustedhost import TrustedHostMiddleware
from fastapi.responses import JSONResponse
import uvicorn
from motor.motor_asyncio import AsyncIOMotorClient
from pydantic import BaseModel
from bson import ObjectId
from typing import List, Optional, Dict, Any
import locale
import codecs
from datetime import datetime
from datetime import timedelta
import csv
from chatbot import chatbot_router, langgraph_router
from github import router as github_router
from routers.upload import router as upload_router
from routers.pick_chatbot import router as pick_chatbot_router
from routers.integrated_ocr import router as integrated_ocr_router

from routers.pdf_ocr import router as pdf_ocr_router


from similarity_service import SimilarityService
from embedding_service import EmbeddingService
from vector_service import VectorService
from services.mongo_service import MongoService

# Python í™˜ê²½ ì¸ì½”ë”© ì„¤ì •
# ì‹œìŠ¤í…œ ê¸°ë³¸ ì¸ì½”ë”©ì„ UTF-8ë¡œ ì„¤ì •
if sys.platform.startswith('win'):
    # Windows í™˜ê²½ì—ì„œ UTF-8 ê°•ì œ ì„¤ì •
    os.environ['PYTHONIOENCODING'] = 'utf-8'
    # ì½˜ì†” ì¶œë ¥ ì¸ì½”ë”© ì„¤ì •
    sys.stdout = codecs.getwriter('utf-8')(sys.stdout.detach())
    sys.stderr = codecs.getwriter('utf-8')(sys.stderr.detach())

# FastAPI ì•± ìƒì„±
app = FastAPI(title="AI ì±„ìš© ê´€ë¦¬ ì‹œìŠ¤í…œ API", version="1.0.0")

# CORS ì„¤ì •
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# í•œê¸€ ì¸ì½”ë”©ì„ ìœ„í•œ ë¯¸ë“¤ì›¨ì–´
@app.middleware("http")
async def add_charset_header(request, call_next):
    response = await call_next(request)
    
    # ëª¨ë“  JSON ì‘ë‹µì— UTF-8 ì¸ì½”ë”© ëª…ì‹œ
    if response.headers.get("content-type", "").startswith("application/json"):
        response.headers["content-type"] = "application/json; charset=utf-8"
    
    # í…ìŠ¤íŠ¸ ì‘ë‹µì—ë„ UTF-8 ì¸ì½”ë”© ëª…ì‹œ
    elif response.headers.get("content-type", "").startswith("text/"):
        if "charset" not in response.headers.get("content-type", ""):
            current_content_type = response.headers.get("content-type", "")
            response.headers["content-type"] = f"{current_content_type}; charset=utf-8"
    
    return response

# ë¼ìš°í„° ë“±ë¡
app.include_router(chatbot_router, prefix="/api/chatbot", tags=["chatbot"])
app.include_router(langgraph_router, prefix="/api/langgraph", tags=["langgraph"])
# í”„ë¡ íŠ¸ì—”ë“œ í˜¸í™˜ì„ ìœ„í•´ /api/langgraph-agent í”„ë¦¬í”½ìŠ¤ë„ ë™ì¼ ë¼ìš°í„°ë¡œ ë§ˆìš´íŠ¸
app.include_router(langgraph_router, prefix="/api/langgraph-agent", tags=["langgraph-agent"])
app.include_router(github_router, prefix="/api", tags=["github"])
app.include_router(upload_router, tags=["upload"])
app.include_router(pick_chatbot_router, prefix="/api", tags=["pick-chatbot"])
app.include_router(integrated_ocr_router, prefix="/api/integrated-ocr", tags=["integrated-ocr"])
# í”„ë¡ íŠ¸ì—”ë“œ í˜¸í™˜ì„ ìœ„í•´ /upload ê²½ë¡œì—ë„ ë“±ë¡
app.include_router(integrated_ocr_router, prefix="/upload", tags=["upload-compat"])

app.include_router(pdf_ocr_router, prefix="/api/pdf-ocr", tags=["pdf_ocr"])


# MongoDB ì—°ê²°
MONGODB_URI = os.getenv("MONGODB_URI", "mongodb://localhost:27017/hireme")
client = AsyncIOMotorClient(MONGODB_URI)
db = client.hireme

# í™˜ê²½ ë³€ìˆ˜ì—ì„œ API í‚¤ ë¡œë“œ
GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY")
PINECONE_API_KEY = os.getenv("PINECONE_API_KEY") 
PINECONE_INDEX_NAME = os.getenv("PINECONE_INDEX_NAME", "resume-vectors")

# ì„œë¹„ìŠ¤ ì´ˆê¸°í™”
embedding_service = EmbeddingService()
vector_service = VectorService(
    api_key=PINECONE_API_KEY or "dummy-key",  # API í‚¤ê°€ ì—†ì–´ë„ ì„œë²„ ì‹œì‘ì€ ê°€ëŠ¥
    index_name=PINECONE_INDEX_NAME
)
similarity_service = SimilarityService(embedding_service, vector_service)

# Pydantic ëª¨ë¸ë“¤
class User(BaseModel):
    id: Optional[str] = None
    username: str
    email: str
    role: str = "user"
    created_at: Optional[datetime] = None

class Resume(BaseModel):
    id: Optional[str] = None
    resume_id: Optional[str] = None
    name: str
    position: str
    department: Optional[str] = ""
    experience: int
    skills: List[str]
    growthBackground: Optional[str] = ""
    motivation: Optional[str] = ""
    careerHistory: Optional[str] = ""
    analysisScore: int = 0
    analysisResult: str = ""
    status: str = "pending"
    created_at: Optional[datetime] = None

class ResumeChunk(BaseModel):
    id: Optional[str] = None
    resume_id: str
    chunk_id: str
    text: str
    start_pos: int
    end_pos: int
    chunk_index: int
    field_name: Optional[str] = None  # growthBackground, motivation, careerHistory
    metadata: Optional[Dict[str, Any]] = None
    vector_id: Optional[str] = None  # Pinecone ë²¡í„° ID
    created_at: Optional[datetime] = None

class Interview(BaseModel):
    id: Optional[str] = None
    user_id: str
    company: str
    position: str
    date: datetime
    status: str = "scheduled"
    created_at: Optional[datetime] = None

# ì´ˆê¸° ë°ì´í„° ë¡œë”© ìœ í‹¸ë¦¬í‹°: DBê°€ ë¹„ì–´ìˆìœ¼ë©´ ë£¨íŠ¸ CSVì—ì„œ ì„í¬íŠ¸
async def seed_applicants_from_csv_if_empty() -> None:
    try:
        total_documents = await db.applicants.count_documents({})
        if total_documents > 0:
            return

        project_root_csv_path = os.path.abspath(
            os.path.join(os.path.dirname(__file__), "..", "hireme.applicants.csv")
        )
        if not os.path.exists(project_root_csv_path):
            return

        documents_to_insert = []
        with open(project_root_csv_path, mode="r", encoding="utf-8") as csv_file:
            reader = csv.DictReader(csv_file)
            for row in reader:
                document: Dict[str, Any] = {}

                # _id ì²˜ë¦¬: ê°€ëŠ¥í•˜ë©´ ObjectIdë¡œ ì €ì¥
                raw_id = row.get("_id")
                if raw_id and isinstance(raw_id, str) and len(raw_id) == 24:
                    try:
                        document["_id"] = ObjectId(raw_id)
                    except Exception:
                        document["_id"] = raw_id

                # resume_id ì²˜ë¦¬
                raw_resume_id = row.get("resume_id")
                if raw_resume_id and isinstance(raw_resume_id, str) and len(raw_resume_id) == 24:
                    try:
                        document["resume_id"] = ObjectId(raw_resume_id)
                    except Exception:
                        document["resume_id"] = raw_resume_id
                elif raw_resume_id:
                    document["resume_id"] = raw_resume_id

                # ë¬¸ìì—´ í•„ë“œë“¤: í•­ìƒ ë¬¸ìì—´ë¡œ ìºìŠ¤íŒ…
                string_fields = [
                    "name",
                    "position",
                    "department",
                    "growthBackground",
                    "motivation",
                    "careerHistory",
                    "analysisResult",
                    "status",
                ]
                for field_name in string_fields:
                    value = row.get(field_name, "")
                    document[field_name] = "" if value is None else str(value)
                
                # experience í•„ë“œ: ì •ìˆ˜ë¡œ ì²˜ë¦¬
                try:
                    experience_value = row.get("experience", "0")
                    document["experience"] = int(experience_value) if experience_value else 0
                except (ValueError, TypeError):
                    document["experience"] = 0
                
                # skills í•„ë“œ: ë¦¬ìŠ¤íŠ¸ë¡œ ì²˜ë¦¬
                skills_value = row.get("skills", "")
                if skills_value:
                    # ì‰¼í‘œë¡œ êµ¬ë¶„ëœ ë¬¸ìì—´ì„ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
                    skills_list = [skill.strip() for skill in str(skills_value).split(",") if skill.strip()]
                    document["skills"] = skills_list
                else:
                    document["skills"] = []

                # ìˆ«ì í•„ë“œ
                try:
                    document["analysisScore"] = int(row.get("analysisScore", "0") or 0)
                except Exception:
                    document["analysisScore"] = 0

                # created_at ì²˜ë¦¬
                created_at_raw = row.get("created_at")
                if created_at_raw:
                    try:
                        iso_candidate = created_at_raw.replace("Z", "+00:00")
                        document["created_at"] = datetime.fromisoformat(iso_candidate)
                    except Exception:
                        document["created_at"] = datetime.now()

                documents_to_insert.append(document)

        if documents_to_insert:
            print(f"ğŸ” ì‹œë“œ ëŒ€ìƒ ë¬¸ì„œ ìˆ˜: {len(documents_to_insert)}")
            
            # ì¤‘ë³µ ì²´í¬ ë° ì—…ë°ì´íŠ¸
            inserted_count = 0
            updated_count = 0
            
            for document in documents_to_insert:
                try:
                    # ê¸°ì¡´ ë¬¸ì„œê°€ ìˆëŠ”ì§€ í™•ì¸
                    existing = await db.applicants.find_one({"name": document.get("name"), "position": document.get("position")})
                    if existing:
                        # ê¸°ì¡´ ë¬¸ì„œ ì—…ë°ì´íŠ¸
                        await db.applicants.update_one(
                            {"_id": existing["_id"]},
                            {"$set": document}
                        )
                        updated_count += 1
                    else:
                        # ìƒˆ ë¬¸ì„œ ì‚½ì…
                        await db.applicants.insert_one(document)
                        inserted_count += 1
                except Exception as e:
                    print(f"ë¬¸ì„œ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                    continue
            
            new_count = await db.applicants.count_documents({})
            print(f"ğŸ“¥ CSV ì²˜ë¦¬ ì™„ë£Œ â†’ ì‚½ì…: {inserted_count}ê±´, ì—…ë°ì´íŠ¸: {updated_count}ê±´, ì´ ë¬¸ì„œ ìˆ˜: {new_count}")
    except Exception as seed_error:
                    print(f"[ERROR] CSV ì„í¬íŠ¸ ì‹¤íŒ¨: {seed_error}")


def load_applicants_from_csv() -> List[Dict[str, Any]]:
    """DB ë¯¸ê°€ë™/ë¹„ì–´ìˆì„ ë•Œ CSVë¥¼ ì§ì ‘ ì½ì–´ ë°˜í™˜"""
    try:
        project_root_csv_path = os.path.abspath(
            os.path.join(os.path.dirname(__file__), "..", "hireme.applicants.csv")
        )
        if not os.path.exists(project_root_csv_path):
            return []

        applicants: List[Dict[str, Any]] = []
        with open(project_root_csv_path, mode="r", encoding="utf-8") as csv_file:
            reader = csv.DictReader(csv_file)
            for row in reader:
                item: Dict[str, Any] = {}
                # id/_id
                raw_id = row.get("_id") or row.get("id")
                if raw_id:
                    item["id"] = str(raw_id)

                # ê¸°ë³¸ ë¬¸ìì—´ í•„ë“œ
                for field_name in [
                    "name",
                    "position",
                    "department",
                    "growthBackground",
                    "motivation",
                    "careerHistory",
                    "analysisResult",
                    "status",
                ]:
                    value = row.get(field_name, "")
                    item[field_name] = "" if value is None else str(value)
                
                # experience í•„ë“œ: ì •ìˆ˜ë¡œ ì²˜ë¦¬
                try:
                    experience_value = row.get("experience", "0")
                    item["experience"] = int(experience_value) if experience_value else 0
                except (ValueError, TypeError):
                    item["experience"] = 0
                
                # skills í•„ë“œ: ë¦¬ìŠ¤íŠ¸ë¡œ ì²˜ë¦¬
                skills_value = row.get("skills", "")
                if skills_value:
                    # ì‰¼í‘œë¡œ êµ¬ë¶„ëœ ë¬¸ìì—´ì„ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
                    skills_list = [skill.strip() for skill in str(skills_value).split(",") if skill.strip()]
                    item["skills"] = skills_list
                else:
                    item["skills"] = []

                # score
                try:
                    item["analysisScore"] = int(row.get("analysisScore", "0") or 0)
                except Exception:
                    item["analysisScore"] = 0

                # created_at
                created_at_raw = row.get("created_at")
                if created_at_raw:
                    try:
                        iso_candidate = created_at_raw.replace("Z", "+00:00")
                        item["created_at"] = datetime.fromisoformat(iso_candidate)
                    except Exception:
                        item["created_at"] = datetime.now()

                applicants.append(item)
        return applicants
    except Exception:
        return []

# API ë¼ìš°íŠ¸ë“¤
@app.get("/")
async def root():
    return {"message": "AI ì±„ìš© ê´€ë¦¬ ì‹œìŠ¤í…œ APIê°€ ì‹¤í–‰ ì¤‘ì…ë‹ˆë‹¤."}

@app.get("/health")
async def health_check():
    return {"status": "healthy", "message": "ì„œë²„ê°€ ì •ìƒì ìœ¼ë¡œ ì‘ë™ ì¤‘ì…ë‹ˆë‹¤."}

# ì‚¬ìš©ì ê´€ë ¨ API
@app.get("/api/users", response_model=List[User])
async def get_users():
    users = await db.users.find().to_list(1000)
    # MongoDBì˜ _idë¥¼ idë¡œ ë³€í™˜
    for user in users:
        user["id"] = str(user["_id"])
        del user["_id"]
    return [User(**user) for user in users]

@app.post("/api/users", response_model=User)
async def create_user(user: User):
    user_dict = user.dict()
    user_dict["created_at"] = datetime.now()
    result = await db.users.insert_one(user_dict)
    user_dict["id"] = str(result.inserted_id)
    return User(**user_dict)

# ì´ë ¥ì„œ ê´€ë ¨ API
@app.get("/api/resumes", response_model=List[Resume])
async def get_resumes():
    resumes = await db.resumes.find().to_list(1000)
    # MongoDBì˜ _idë¥¼ idë¡œ ë³€í™˜
    for resume in resumes:
        resume["id"] = str(resume["_id"])
        del resume["_id"]
    return [Resume(**resume) for resume in resumes]

@app.post("/api/resumes", response_model=Resume)
async def create_resume(resume: Resume):
    resume_dict = resume.dict()
    resume_dict["created_at"] = datetime.now()
    result = await db.resumes.insert_one(resume_dict)
    resume_dict["id"] = str(result.inserted_id)
    return Resume(**resume_dict)

# ë©´ì ‘ ê´€ë ¨ API
@app.get("/api/interviews", response_model=List[Interview])
async def get_interviews():
    interviews = await db.interviews.find().to_list(1000)
    # MongoDBì˜ _idë¥¼ idë¡œ ë³€í™˜
    for interview in interviews:
        interview["id"] = str(interview["_id"])
        del interview["_id"]
    return [Interview(**interview) for interview in interviews]

@app.post("/api/interviews", response_model=Interview)
async def create_interview(interview: Interview):
    interview_dict = interview.dict()
    interview_dict["created_at"] = datetime.now()
    result = await db.interviews.insert_one(interview_dict)
    interview_dict["id"] = str(result.inserted_id)
    return Interview(**interview_dict)

# ì§€ì›ì ê´€ë ¨ API
@app.get("/api/applicants")
async def get_applicants(skip: int = 0, limit: int = 20):
    try:
        # DBê°€ ë¹„ì–´ìˆìœ¼ë©´ CSVì—ì„œ ìë™ ì„í¬íŠ¸
        await seed_applicants_from_csv_if_empty()
        # ì´ ë¬¸ì„œ ìˆ˜
        total_count = await db.applicants.count_documents({})

        if total_count == 0:
            # DBê°€ ì™„ì „ ë¹„ì–´ìˆì„ ë•Œ CSVë¥¼ ê°€ìƒ DBì²˜ëŸ¼ ë°˜í™˜
            csv_applicants = load_applicants_from_csv()
            items = csv_applicants[skip:skip+limit]
            return {
                "applicants": [Resume(**a) for a in items],
                "total_count": len(csv_applicants),
                "skip": skip,
                "limit": limit,
                "has_more": (skip + limit) < len(csv_applicants)
            }

        # í˜ì´ì§•ìœ¼ë¡œ ì´ë ¥ì„œ(ì§€ì›ì) ëª©ë¡ ì¡°íšŒ
        applicants = await db.applicants.find().skip(skip).limit(limit).to_list(limit)

        # MongoDBì˜ _idë¥¼ idë¡œ ë³€í™˜ ë° ObjectId í•„ë“œë“¤ì„ ë¬¸ìì—´ë¡œ ë³€í™˜
        for applicant in applicants:
            applicant["id"] = str(applicant["_id"])
            del applicant["_id"]
            if "resume_id" in applicant and applicant["resume_id"]:
                applicant["resume_id"] = str(applicant["resume_id"])

        return {
            "applicants": [Resume(**applicant) for applicant in applicants],
            "total_count": total_count,
            "skip": skip,
            "limit": limit,
            "has_more": (skip + limit) < total_count
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ì§€ì›ì ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")

# ê°œë³„ ì§€ì›ì ì¡°íšŒ API
@app.get("/api/applicants/{applicant_id}")
async def get_applicant(applicant_id: str):
    try:
        # MongoDB ObjectIdë¡œ ì¡°íšŒ ì‹œë„
        try:
            applicant = await db.applicants.find_one({"_id": ObjectId(applicant_id)})
        except:
            # ObjectId ë³€í™˜ ì‹¤íŒ¨ì‹œ ë¬¸ìì—´ë¡œ ì¡°íšŒ
            applicant = await db.applicants.find_one({"_id": applicant_id})
        
        if not applicant:
            raise HTTPException(status_code=404, detail="ì§€ì›ìë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        # MongoDBì˜ _idë¥¼ idë¡œ ë³€í™˜
        applicant["id"] = str(applicant["_id"])
        del applicant["_id"]
        if "resume_id" in applicant and applicant["resume_id"]:
            applicant["resume_id"] = str(applicant["resume_id"])
        
        return Resume(**applicant)
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ì§€ì›ì ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")

# ì§€ì›ì í†µê³„ API
@app.get("/api/applicants/stats/overview")
async def get_applicant_stats():
    try:
        # DBê°€ ë¹„ì–´ìˆìœ¼ë©´ CSVì—ì„œ ìë™ ì„í¬íŠ¸
        await seed_applicants_from_csv_if_empty()
        # ì´ ì§€ì›ì ìˆ˜ (resumes ì»¬ë ‰ì…˜ ê¸°ì¤€)
        total_applicants = await db.resumes.count_documents({})

        if total_applicants == 0:
            # CSV ê¸°ë°˜ ê°€ìƒ í†µê³„
            csv_applicants = load_applicants_from_csv()
            total = len(csv_applicants)
            status_counts = {"pending": 0, "approved": 0, "rejected": 0}
            for a in csv_applicants:
                s = (a.get("status") or "").lower()
                if s in status_counts:
                    status_counts[s] += 1
            return {
                "total_applicants": total,
                "status_breakdown": status_counts,
                "recent_applicants_30_days": total,
                "success_rate": round((status_counts.get("approved", 0) / total * 100) if total > 0 else 0, 2)
            }

        # ìƒíƒœë³„ ì§€ì›ì ìˆ˜
        pending_count = await db.resumes.count_documents({"status": "pending"})
        approved_count = await db.resumes.count_documents({"status": "approved"})
        rejected_count = await db.resumes.count_documents({"status": "rejected"})
        
        # ìµœê·¼ 30ì¼ê°„ ì§€ì›ì ìˆ˜ (ì›” ê²½ê³„/ìœ¤ë‹¬ ì´ìŠˆ ì—†ì´ ì•ˆì „í•˜ê²Œ ê³„ì‚°)
        thirty_days_ago = datetime.now() - timedelta(days=30)
        recent_applicants = await db.resumes.count_documents({"created_at": {"$gte": thirty_days_ago}})
        
        return {
            "total_applicants": total_applicants,
            "status_breakdown": {
                "pending": pending_count,
                "approved": approved_count,
                "rejected": rejected_count
            },
            "recent_applicants_30_days": recent_applicants,
            "success_rate": round((approved_count / total_applicants * 100) if total_applicants > 0 else 0, 2)
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ì§€ì›ì í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")

# Vector Service API
@app.post("/api/vector/create")
async def create_vector(data: Dict[str, Any]):
    """í…ìŠ¤íŠ¸ë¥¼ ë²¡í„°ë¡œ ë³€í™˜í•˜ì—¬ ì €ì¥"""
    try:
        text = data.get("text", "")
        document_id = data.get("document_id")
        metadata = data.get("metadata", {})
        
        # ì—¬ê¸°ì„œ ì‹¤ì œ ë²¡í„°í™” ë¡œì§ êµ¬í˜„
        # ì˜ˆ: embedding_modelì„ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ë¥¼ ë²¡í„°ë¡œ ë³€í™˜
        
        # ì„ì‹œë¡œ ì„±ê³µ ì‘ë‹µ ë°˜í™˜
        return {
            "message": "Vector created successfully",
            "document_id": document_id,
            "vector_dimension": 384,  # ì˜ˆì‹œ ì°¨ì›
            "status": "success"
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ë²¡í„° ìƒì„± ì‹¤íŒ¨: {str(e)}")

@app.post("/api/vector/search")
async def search_vectors(data: Dict[str, Any]):
    """ë²¡í„° ìœ ì‚¬ë„ ê²€ìƒ‰"""
    try:
        query_text = data.get("query", "")
        top_k = data.get("top_k", 5)
        threshold = data.get("threshold", 0.7)
        
        # ì—¬ê¸°ì„œ ì‹¤ì œ ë²¡í„° ê²€ìƒ‰ ë¡œì§ êµ¬í˜„
        
        # ì„ì‹œë¡œ ê²€ìƒ‰ ê²°ê³¼ ë°˜í™˜
        return {
            "results": [
                {
                    "document_id": "doc_001",
                    "score": 0.95,
                    "text": "ê²€ìƒ‰ëœ í…ìŠ¤íŠ¸ ìƒ˜í”Œ 1",
                    "metadata": {"type": "resume", "applicant_id": "app_001"}
                },
                {
                    "document_id": "doc_002", 
                    "score": 0.87,
                    "text": "ê²€ìƒ‰ëœ í…ìŠ¤íŠ¸ ìƒ˜í”Œ 2",
                    "metadata": {"type": "cover_letter", "applicant_id": "app_002"}
                }
            ],
            "total_found": 2
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ë²¡í„° ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}")

# Chunking Service API
@app.post("/api/chunking/split")
async def split_text(data: Dict[str, Any]):
    """í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë¶„í• í•˜ê³  DBì— ì €ì¥"""
    try:
        text = data.get("text", "")
        resume_id = data.get("resume_id")
        field_name = data.get("field_name", "")  # growthBackground, motivation, careerHistory
        chunk_size = data.get("chunk_size", 1000)
        chunk_overlap = data.get("chunk_overlap", 200)
        split_type = data.get("split_type", "recursive")
        
        if not resume_id:
            raise HTTPException(status_code=400, detail="resume_idê°€ í•„ìš”í•©ë‹ˆë‹¤.")
        
        # í…ìŠ¤íŠ¸ ë¶„í•  ë¡œì§
        chunks = []
        text_length = len(text)
        start = 0
        chunk_index = 0
        
        while start < text_length:
            end = min(start + chunk_size, text_length)
            chunk_text = text[start:end]
            
            if chunk_text.strip():  # ë¹ˆ ì²­í¬ëŠ” ì œì™¸
                chunk_id = f"chunk_{chunk_index:03d}"
                vector_id = f"resume_{resume_id}_{chunk_id}"
                
                chunk_doc = {
                    "resume_id": resume_id,
                    "chunk_id": chunk_id,
                    "text": chunk_text,
                    "start_pos": start,
                    "end_pos": end,
                    "chunk_index": chunk_index,
                    "field_name": field_name,
                    "vector_id": vector_id,
                    "metadata": {
                        "length": len(chunk_text),
                        "split_type": split_type,
                        "chunk_size": chunk_size,
                        "chunk_overlap": chunk_overlap
                    },
                    "created_at": datetime.now()
                }
                
                # MongoDBì— ì²­í¬ ì €ì¥
                result = await db.resume_chunks.insert_one(chunk_doc)
                chunk_doc["id"] = str(result.inserted_id)
                
                chunks.append(chunk_doc)
                chunk_index += 1
            
            start = end - chunk_overlap if chunk_overlap > 0 else end
            
            if start >= text_length:
                break
        
        return {
            "chunks": chunks,
            "total_chunks": len(chunks),
            "original_length": text_length,
            "resume_id": resume_id,
            "field_name": field_name,
            "split_config": {
                "chunk_size": chunk_size,
                "chunk_overlap": chunk_overlap,
                "split_type": split_type
            }
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"í…ìŠ¤íŠ¸ ë¶„í•  ì‹¤íŒ¨: {str(e)}")

@app.get("/api/chunking/resume/{resume_id}")
async def get_resume_chunks(resume_id: str):
    """íŠ¹ì • ì´ë ¥ì„œì˜ ëª¨ë“  ì²­í¬ ì¡°íšŒ"""
    try:
        chunks = await db.resume_chunks.find({"resume_id": resume_id}).to_list(1000)
        
        # MongoDBì˜ _idë¥¼ idë¡œ ë³€í™˜
        for chunk in chunks:
            chunk["id"] = str(chunk["_id"])
            del chunk["_id"]
        
        return {
            "resume_id": resume_id,
            "chunks": [ResumeChunk(**chunk) for chunk in chunks],
            "total_chunks": len(chunks)
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ì²­í¬ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")

@app.post("/api/chunking/process-resume")
async def process_resume_with_chunking(data: Dict[str, Any]):
    """ì´ë ¥ì„œ ì „ì²´ë¥¼ í•„ë“œë³„ë¡œ ì²­í‚¹ ì²˜ë¦¬"""
    try:
        resume_id = data.get("resume_id")
        if not resume_id:
            raise HTTPException(status_code=400, detail="resume_idê°€ í•„ìš”í•©ë‹ˆë‹¤.")
        
        # ì´ë ¥ì„œ ì •ë³´ ì¡°íšŒ
        resume = await db.resumes.find_one({"_id": ObjectId(resume_id)})
        if not resume:
            raise HTTPException(status_code=404, detail="ì´ë ¥ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        chunk_size = data.get("chunk_size", 800)
        chunk_overlap = data.get("chunk_overlap", 150)
        
        # ì²­í‚¹í•  í•„ë“œë“¤
        fields_to_chunk = ["growthBackground", "motivation", "careerHistory"]
        all_chunks = []
        
        for field_name in fields_to_chunk:
            field_text = resume.get(field_name, "")
            if field_text and field_text.strip():
                # í•„ë“œë³„ ì²­í‚¹ ì²˜ë¦¬
                field_chunks = []
                text_length = len(field_text)
                start = 0
                chunk_index = 0
                
                while start < text_length:
                    end = min(start + chunk_size, text_length)
                    chunk_text = field_text[start:end]
                    
                    if chunk_text.strip():
                        chunk_id = f"{field_name}_chunk_{chunk_index:03d}"
                        vector_id = f"resume_{resume_id}_{chunk_id}"
                        
                        chunk_doc = {
                            "resume_id": resume_id,
                            "chunk_id": chunk_id,
                            "text": chunk_text,
                            "start_pos": start,
                            "end_pos": end,
                            "chunk_index": chunk_index,
                            "field_name": field_name,
                            "vector_id": vector_id,
                            "metadata": {
                                "applicant_name": resume.get("name", ""),
                                "position": resume.get("position", ""),
                                "department": resume.get("department", ""),
                                "length": len(chunk_text)
                            },
                            "created_at": datetime.now()
                        }
                        
                        result = await db.resume_chunks.insert_one(chunk_doc)
                        chunk_doc["id"] = str(result.inserted_id)
                        field_chunks.append(chunk_doc)
                        chunk_index += 1
                    
                    start = end - chunk_overlap if chunk_overlap > 0 else end
                    if start >= text_length:
                        break
                
                all_chunks.extend(field_chunks)
        
        return {
            "resume_id": resume_id,
            "applicant_name": resume.get("name", ""),
            "processed_fields": fields_to_chunk,
            "total_chunks": len(all_chunks),
            "chunks_by_field": {
                field: len([c for c in all_chunks if c["field_name"] == field])
                for field in fields_to_chunk
            },
            "chunks": all_chunks
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ì´ë ¥ì„œ ì²­í‚¹ ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")

@app.post("/api/chunking/merge")
async def merge_chunks(data: Dict[str, Any]):
    """ì²­í¬ë“¤ì„ ë³‘í•©"""
    try:
        chunks = data.get("chunks", [])
        separator = data.get("separator", "\n\n")
        
        # ì²­í¬ ë³‘í•©
        merged_text = separator.join([chunk.get("text", "") for chunk in chunks])
        
        return {
            "merged_text": merged_text,
            "total_length": len(merged_text),
            "chunks_merged": len(chunks),
            "separator_used": separator
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ì²­í¬ ë³‘í•© ì‹¤íŒ¨: {str(e)}")

# Similarity Service API
@app.post("/api/similarity/compare")
async def compare_similarity(data: Dict[str, Any]):
    """ë‘ í…ìŠ¤íŠ¸ ê°„ì˜ ìœ ì‚¬ë„ ê³„ì‚°"""
    try:
        text1 = data.get("text1", "")
        text2 = data.get("text2", "")
        method = data.get("method", "cosine")  # cosine, jaccard, levenshtein
        
        # ì—¬ê¸°ì„œ ì‹¤ì œ ìœ ì‚¬ë„ ê³„ì‚° ë¡œì§ êµ¬í˜„
        # ì˜ˆ: sentence-transformersì˜ cosine similarity
        
        # ì„ì‹œë¡œ ìœ ì‚¬ë„ ì ìˆ˜ ë°˜í™˜
        import random
        similarity_score = random.uniform(0.3, 0.95)  # ì„ì‹œ ì ìˆ˜
        
        return {
            "similarity_score": round(similarity_score, 4),
            "method": method,
            "text1_length": len(text1),
            "text2_length": len(text2),
            "comparison_result": {
                "highly_similar": similarity_score > 0.8,
                "moderately_similar": 0.5 < similarity_score <= 0.8,
                "low_similar": similarity_score <= 0.5
            }
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ìœ ì‚¬ë„ ê³„ì‚° ì‹¤íŒ¨: {str(e)}")

@app.post("/api/similarity/batch")
async def batch_similarity(data: Dict[str, Any]):
    """ì—¬ëŸ¬ í…ìŠ¤íŠ¸ë“¤ ê°„ì˜ ì¼ê´„ ìœ ì‚¬ë„ ê³„ì‚°"""
    try:
        texts = data.get("texts", [])
        reference_text = data.get("reference_text", "")
        method = data.get("method", "cosine")
        threshold = data.get("threshold", 0.7)
        
        # ë°°ì¹˜ ìœ ì‚¬ë„ ê³„ì‚°
        results = []
        import random
        
        for i, text in enumerate(texts):
            similarity_score = random.uniform(0.2, 0.95)  # ì„ì‹œ ì ìˆ˜
            results.append({
                "index": i,
                "text_preview": text[:100] + "..." if len(text) > 100 else text,
                "similarity_score": round(similarity_score, 4),
                "above_threshold": similarity_score >= threshold
            })
        
        # ì„ê³„ê°’ ì´ìƒì¸ ê²°ê³¼ë“¤ í•„í„°ë§
        filtered_results = [r for r in results if r["above_threshold"]]
        
        return {
            "results": results,
            "filtered_results": filtered_results,
            "total_compared": len(texts),
            "above_threshold_count": len(filtered_results),
            "method": method,
            "threshold": threshold,
            "reference_text_length": len(reference_text)
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ë°°ì¹˜ ìœ ì‚¬ë„ ê³„ì‚° ì‹¤íŒ¨: {str(e)}")

@app.get("/api/similarity/metrics")
async def get_similarity_metrics():
    """ìœ ì‚¬ë„ ì„œë¹„ìŠ¤ ë©”íŠ¸ë¦­ ì¡°íšŒ"""
    try:
        # ì„ì‹œ ë©”íŠ¸ë¦­ ë°ì´í„°
        return {
            "total_comparisons": 1250,
            "average_similarity": 0.67,
            "supported_methods": ["cosine", "jaccard", "levenshtein", "semantic"],
            "performance_stats": {
                "average_processing_time_ms": 45,
                "comparisons_per_second": 220,
                "cache_hit_rate": 0.78
            },
            "usage_by_method": {
                "cosine": 850,
                "semantic": 300,
                "jaccard": 70,
                "levenshtein": 30
            }
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ë©”íŠ¸ë¦­ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")

# ë‹¤ì¤‘ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ API ğŸ†•
@app.post("/api/resume/search/multi-hybrid")
async def search_resumes_multi_hybrid(data: Dict[str, Any]):
    """ë‹¤ì¤‘ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰: ë²¡í„° + í…ìŠ¤íŠ¸ + í‚¤ì›Œë“œ ê²€ìƒ‰ì„ ê²°í•©"""
    try:
        query = data.get("query", "")
        search_type = data.get("type", "resume")
        limit = data.get("limit", 10)
        
        print(f"[API] ë‹¤ì¤‘ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ìš”ì²­ - ì¿¼ë¦¬: '{query}', ì œí•œ: {limit}")
        
        if not query or not query.strip():
            raise HTTPException(status_code=400, detail="ê²€ìƒ‰ì–´ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
        
        # SimilarityServiceì˜ ë‹¤ì¤‘ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì‹¤í–‰
        result = await similarity_service.search_resumes_multi_hybrid(
            query=query,
            collection=db.applicants,
            search_type=search_type,
            limit=limit
        )
        
        if not result["success"]:
            raise HTTPException(status_code=500, detail="ë‹¤ì¤‘ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
        
        return {
            "success": True,
            "message": f"ë‹¤ì¤‘ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì™„ë£Œ: '{query}'",
            "data": result["data"]
        }
        
    except HTTPException:
        raise
    except Exception as e:
        print(f"[API] ë‹¤ì¤‘ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}")
        raise HTTPException(status_code=500, detail=f"ë‹¤ì¤‘ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}")

# í‚¤ì›Œë“œ ê²€ìƒ‰ API
@app.post("/api/resume/search/keyword")
async def search_resumes_keyword(data: Dict[str, Any]):
    """í‚¤ì›Œë“œ ê¸°ë°˜ ì´ë ¥ì„œ ê²€ìƒ‰ (BM25)"""
    try:
        query = data.get("query", "")
        limit = data.get("limit", 10)
        
        print(f"[API] í‚¤ì›Œë“œ ê²€ìƒ‰ ìš”ì²­ - ì¿¼ë¦¬: '{query}', ì œí•œ: {limit}")
        
        if not query or not query.strip():
            raise HTTPException(status_code=400, detail="ê²€ìƒ‰ì–´ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
        
        # KeywordSearchServiceë¥¼ í†µí•œ BM25 ê²€ìƒ‰
        result = await similarity_service.keyword_search_service.search_by_keywords(
            query=query,
            collection=db.applicants,
            limit=limit
        )
        
        if not result["success"]:
            raise HTTPException(status_code=500, detail=result.get("message", "í‚¤ì›Œë“œ ê²€ìƒ‰ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤."))
        
        return {
            "success": True,
            "message": result["message"],
            "data": {
                "query": result["query"],
                "results": result["results"],
                "total": result["total"],
                "search_method": "keyword_bm25",
                "query_tokens": result.get("query_tokens", [])
            }
        }
        
    except HTTPException:
        raise
    except Exception as e:
        print(f"[API] í‚¤ì›Œë“œ ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}")
        raise HTTPException(status_code=500, detail=f"í‚¤ì›Œë“œ ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}")

# í‚¤ì›Œë“œ ê²€ìƒ‰ ì¸ë±ìŠ¤ ê´€ë¦¬ API
@app.post("/api/resume/search/keyword/rebuild-index")
async def rebuild_keyword_index():
    """í‚¤ì›Œë“œ ê²€ìƒ‰ ì¸ë±ìŠ¤ ì¬êµ¬ì¶•"""
    try:
        print(f"[API] í‚¤ì›Œë“œ ì¸ë±ìŠ¤ ì¬êµ¬ì¶• ìš”ì²­")
        
        # KeywordSearchServiceë¥¼ í†µí•œ ì¸ë±ìŠ¤ ì¬êµ¬ì¶•
        result = await similarity_service.keyword_search_service.build_index(db.applicants)
        
        if not result["success"]:
            raise HTTPException(status_code=500, detail=result.get("message", "ì¸ë±ìŠ¤ ì¬êµ¬ì¶•ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤."))
        
        return {
            "success": True,
            "message": result["message"],
            "data": {
                "total_documents": result["total_documents"],
                "index_created_at": result["index_created_at"]
            }
        }
        
    except HTTPException:
        raise
    except Exception as e:
        print(f"[API] í‚¤ì›Œë“œ ì¸ë±ìŠ¤ ì¬êµ¬ì¶• ì‹¤íŒ¨: {str(e)}")
        raise HTTPException(status_code=500, detail=f"í‚¤ì›Œë“œ ì¸ë±ìŠ¤ ì¬êµ¬ì¶• ì‹¤íŒ¨: {str(e)}")

@app.get("/api/resume/search/keyword/stats")
async def get_keyword_search_stats():
    """í‚¤ì›Œë“œ ê²€ìƒ‰ ì¸ë±ìŠ¤ í†µê³„ ì¡°íšŒ"""
    try:
        stats = await similarity_service.keyword_search_service.get_index_stats()
        
        return {
            "success": True,
            "data": stats
        }
        
    except Exception as e:
        print(f"[API] í‚¤ì›Œë“œ ê²€ìƒ‰ í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")
        raise HTTPException(status_code=500, detail=f"í‚¤ì›Œë“œ ê²€ìƒ‰ í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")

# ìœ ì‚¬ ì¸ì¬ ì¶”ì²œ API
@app.post("/api/applicants/similar-recommendation/{applicant_id}")
async def recommend_similar_applicants(applicant_id: str):
    """ì§€ì›ì ê¸°ë°˜ ìœ ì‚¬ ì¸ì¬ ì¶”ì²œ (í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰: ë²¡í„° + í‚¤ì›Œë“œ)"""
    try:
        print(f"\n=== [SIMILAR_APPLICANTS] ìœ ì‚¬ ì¸ì¬ ì¶”ì²œ ì‹œì‘ ===")
        print(f"[SIMILAR_APPLICANTS] ìš”ì²­ applicant_id: {applicant_id}")
        print(f"[SIMILAR_APPLICANTS] ìš”ì²­ ì‹œê°„: {datetime.now().isoformat()}")
        
        # MongoDB ì„œë¹„ìŠ¤ ì´ˆê¸°í™”
        mongo_service = MongoService()
        print(f"[SIMILAR_APPLICANTS] MongoDB ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ")
        
        # 1. ì§€ì›ì ì •ë³´ ì¡°íšŒ
        print(f"[SIMILAR_APPLICANTS] ì§€ì›ì ì •ë³´ ì¡°íšŒ ì¤‘...")
        applicant = await mongo_service.get_applicant_by_id(applicant_id)
        if not applicant:
            print(f"[SIMILAR_APPLICANTS] âŒ ì§€ì›ìë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ - ID: {applicant_id}")
            return JSONResponse(
                status_code=404,
                content={
                    "success": False,
                    "error": "APPLICANT_NOT_FOUND",
                    "message": "ì§€ì›ìë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                }
            )
        
        print(f"[SIMILAR_APPLICANTS] âœ… ì§€ì›ì ì°¾ìŒ:")
        print(f"[SIMILAR_APPLICANTS]   - ì´ë¦„: {applicant.get('name', 'N/A')}")
        print(f"[SIMILAR_APPLICANTS]   - ì´ë©”ì¼: {applicant.get('email', 'N/A')}")
        print(f"[SIMILAR_APPLICANTS]   - ì§€ì›ì§ë¬´: {applicant.get('position', 'N/A')}")
        print(f"[SIMILAR_APPLICANTS]   - resume_id: {applicant.get('resume_id', 'N/A')}")
        
        # 2. ì§€ì›ì ê¸°ë°˜ ìœ ì‚¬ ì¸ì¬ ì¶”ì²œ ìˆ˜í–‰
        print(f"[SIMILAR_APPLICANTS] ì§€ì›ì ê¸°ë°˜ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì‹œì‘...")
        applicants_collection = mongo_service.db.applicants
        
        # ì§€ì›ì ê¸°ë°˜ ìœ ì‚¬ ì¸ì¬ ì¶”ì²œ ìˆ˜í–‰
        result = await similarity_service.search_similar_applicants_hybrid(
            target_applicant=applicant,
            applicants_collection=applicants_collection,
            limit=10
        )
        
        print(f"[SIMILAR_APPLICANTS] âœ… í•˜ì´ë¸Œë¦¬ë“œ ìœ ì‚¬ë„ ë¶„ì„ ì™„ë£Œ")
        print(f"[SIMILAR_APPLICANTS] ê²°ê³¼ ìš”ì•½:")
        if result.get("success"):
            results = result.get("data", {}).get("results", [])
            similar_count = len(results)
            vector_count = result.get("data", {}).get("vector_count", 0)
            keyword_count = result.get("data", {}).get("keyword_count", 0)
            
            print(f"[SIMILAR_APPLICANTS]   - ìœ ì‚¬í•œ ì§€ì›ì ìˆ˜: {similar_count}")
            print(f"[SIMILAR_APPLICANTS]   - ë²¡í„° ê²€ìƒ‰ ê²°ê³¼: {vector_count}ê°œ")
            print(f"[SIMILAR_APPLICANTS]   - í‚¤ì›Œë“œ ê²€ìƒ‰ ê²°ê³¼: {keyword_count}ê°œ")
            
            if similar_count > 0:
                for i, sim_applicant in enumerate(results[:3]):
                    final_score = sim_applicant.get("final_score", 0)
                    vector_score = sim_applicant.get("vector_score", 0)
                    keyword_score = sim_applicant.get("keyword_score", 0)
                    name = sim_applicant.get("applicant", {}).get("name", "N/A")
                    position = sim_applicant.get("applicant", {}).get("position", "N/A")
                    methods = sim_applicant.get("search_methods", [])
                    print(f"[SIMILAR_APPLICANTS]   - #{i+1}: {name} ({position}) (ìµœì¢…:{final_score:.3f}, V:{vector_score:.3f}, K:{keyword_score:.3f}, ë°©ë²•:{methods})")
        else:
            print(f"[SIMILAR_APPLICANTS]   - ë¶„ì„ ì‹¤íŒ¨: {result}")
        
        print(f"=== [SIMILAR_APPLICANTS] ìœ ì‚¬ ì¸ì¬ ì¶”ì²œ ì™„ë£Œ ===\n")
        
        # ê²°ê³¼ ë°˜í™˜
        if result.get("success"):
            return JSONResponse(
                status_code=200,
                content={
                    "success": True,
                    "message": "ìœ ì‚¬ ì¸ì¬ ì¶”ì²œ ì™„ë£Œ",
                    "data": result.get("data", {})
                }
            )
        else:
            return JSONResponse(
                status_code=500,
                content={
                    "success": False,
                    "error": "SIMILARITY_SEARCH_FAILED",
                    "message": result.get("message", "ìœ ì‚¬ ì¸ì¬ ì¶”ì²œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")
                }
            )
        
    except ValueError as ve:
        print(f"[ERROR] ì´ë ¥ì„œ ìœ ì‚¬ë„ ì²´í¬ - ê°’ ì˜¤ë¥˜: {str(ve)}")
        return JSONResponse(
            status_code=404,
            content={
                "success": False,
                "error": "NOT_FOUND",
                "message": str(ve)
            }
        )
    except Exception as e:
        print(f"[SIMILAR_APPLICANTS] âŒ ì‹œìŠ¤í…œ ì˜¤ë¥˜: {str(e)}")
        import traceback
        error_traceback = traceback.format_exc()
        print(f"[SIMILAR_APPLICANTS] ìƒì„¸ ì˜¤ë¥˜ ì •ë³´:")
        print(error_traceback)
        
        # ì§€ì›ìê°€ ì‚­ì œë˜ì–´ ê´€ë ¨ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ëŠ” ê²½ìš° 404 ë°˜í™˜
        error_message = str(e).lower()
        if "not found" in error_message or "ì°¾ì„ ìˆ˜ ì—†" in error_message or "deleted" in error_message:
            return JSONResponse(
                status_code=404,
                content={
                    "success": False,
                    "error": "APPLICANT_OR_DATA_NOT_FOUND",
                    "message": "ì§€ì›ì ë˜ëŠ” ê´€ë ¨ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                }
            )
        
        return JSONResponse(
            status_code=500,
            content={
                "success": False,
                "error": "INTERNAL_SERVER_ERROR",
                "message": f"ìœ ì‚¬ ì¸ì¬ ì¶”ì²œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
            }
        )

# ìì†Œì„œ í‘œì ˆì²´í¬ ì—”ë“œí¬ì¸íŠ¸
@app.post("/api/coverletter/similarity-check/{applicant_id}")
async def check_coverletter_plagiarism(applicant_id: str):
    """ìì†Œì„œ í‘œì ˆì²´í¬ ì „ìš© ì—”ë“œí¬ì¸íŠ¸ (applicant_id ê¸°ë°˜)"""
    try:
        print(f"[INFO] ìì†Œì„œ í‘œì ˆì²´í¬ ìš”ì²­ - applicant_id: {applicant_id}")
        
        # MongoDB ì„œë¹„ìŠ¤ ì´ˆê¸°í™”
        mongo_service = MongoService()
        
        # 1. ì§€ì›ì ì •ë³´ ì¡°íšŒ
        applicant = await mongo_service.get_applicant_by_id(applicant_id)
        print(f"[DEBUG] ì§€ì›ì ì¡°íšŒ ê²°ê³¼: {applicant is not None}")
        if applicant:
            print(f"[DEBUG] ì§€ì›ì í•„ë“œ: {list(applicant.keys())}")
            print(f"[DEBUG] cover_letter_id: {applicant.get('cover_letter_id', 'NONE')}")
        
        if not applicant:
            return JSONResponse(
                status_code=404,
                content={
                    "success": False,
                    "error": "APPLICANT_NOT_FOUND",
                    "message": "ì§€ì›ìë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                }
            )
        
        # 2. ìì†Œì„œ ID ì¶”ì¶œ
        cover_letter_id = applicant.get("cover_letter_id")
        print(f"[DEBUG] cover_letter_id ê°’: {cover_letter_id}")
        print(f"[DEBUG] cover_letter_id íƒ€ì…: {type(cover_letter_id)}")
        
        if not cover_letter_id:
            return JSONResponse(
                status_code=404,
                content={
                    "success": False,
                    "error": "COVER_LETTER_NOT_FOUND",
                    "message": f"í•´ë‹¹ ì§€ì›ì({applicant_id})ì˜ ìì†Œì„œê°€ ì—†ìŠµë‹ˆë‹¤. cover_letter_id í•„ë“œê°€ ë¹„ì–´ìˆê±°ë‚˜ nullì…ë‹ˆë‹¤.",
                    "debug_info": {
                        "applicant_id": applicant_id,
                        "has_applicant": True,
                        "cover_letter_id": cover_letter_id,
                        "applicant_fields": list(applicant.keys())
                    }
                }
            )
        
        print(f"[INFO] ìì†Œì„œ ID ì¶”ì¶œ ì™„ë£Œ: {cover_letter_id}")
        
        # 3. ìì†Œì„œ ì»¬ë ‰ì…˜ì—ì„œ ê²€ìƒ‰
        cover_letters_collection = mongo_service.db.cover_letters
        
        # 4. ìì†Œì„œ í‘œì ˆì²´í¬ ìˆ˜í–‰
        result = await similarity_service.find_similar_documents(
            document_id=cover_letter_id,
            collection=cover_letters_collection,
            document_type="cover_letter",
            limit=5
        )
        
        # ê²°ê³¼ ë°˜í™˜
        return JSONResponse(
            status_code=200,
            content={
                "success": True,
                "message": "ìì†Œì„œ í‘œì ˆì²´í¬ ì™„ë£Œ",
                "data": result
            }
        )
        
    except ValueError as ve:
        print(f"[ERROR] ìì†Œì„œ í‘œì ˆì²´í¬ - ê°’ ì˜¤ë¥˜: {str(ve)}")
        return JSONResponse(
            status_code=404,
            content={
                "success": False,
                "error": "NOT_FOUND",
                "message": str(ve)
            }
        )
    except Exception as e:
        print(f"[ERROR] ìì†Œì„œ í‘œì ˆì²´í¬ ì‹¤íŒ¨: {str(e)}")
        import traceback
        traceback.print_exc()
        return JSONResponse(
            status_code=500,
            content={
                "success": False,
                "error": "INTERNAL_SERVER_ERROR",
                "message": f"ìì†Œì„œ í‘œì ˆì²´í¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
            }
        )

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8000)