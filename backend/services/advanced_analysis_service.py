import os
import json
import logging
from typing import Dict, Any, List, Optional, Tuple
from datetime import datetime
import numpy as np
from sentence_transformers import SentenceTransformer
from transformers import (
    pipeline, 
    AutoTokenizer, 
    AutoModelForSeq2SeqLM,
    AutoModelForSequenceClassification
)
import torch
from sklearn.metrics.pairwise import cosine_similarity

logger = logging.getLogger(__name__)

class AdvancedAnalysisService:
    """2ë‹¨ê³„ ë¶„ì„ ì‹œìŠ¤í…œ: Hugging Face + GPT-4o"""
    
    def __init__(self):
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        logger.info(f"ğŸ”§ ë¶„ì„ ì„œë¹„ìŠ¤ ì´ˆê¸°í™” - ë””ë°”ì´ìŠ¤: {self.device}")
        
        # 1ë‹¨ê³„: Hugging Face ëª¨ë¸ë“¤
        self._init_huggingface_models()
        
        # 2ë‹¨ê³„: OpenAI ì„œë¹„ìŠ¤ (GPT-4o)
        self._init_openai_service()
        
        # ì§ë¬´ë³„ ê°€ì¤‘ì¹˜ ì„¤ì •
        self.job_weights = {
            "ê°œë°œì": {
                "ì§ë¬´_ì í•©ì„±": 0.4,
                "ê¸°ìˆ _ì—­ëŸ‰": 0.3,
                "ê²½ë ¥_ì„±ê³¼": 0.1,
                "ë¬¸ì œ_í•´ê²°": 0.1,
                "í˜‘ì—…": 0.05,
                "ì„±ì¥ì„±": 0.03,
                "í‘œí˜„ë ¥": 0.02
            },
            "ê¸°íšì": {
                "ì§ë¬´_ì í•©ì„±": 0.3,
                "ê¸°ìˆ _ì—­ëŸ‰": 0.1,
                "ê²½ë ¥_ì„±ê³¼": 0.25,
                "ë¬¸ì œ_í•´ê²°": 0.2,
                "í˜‘ì—…": 0.1,
                "ì„±ì¥ì„±": 0.03,
                "í‘œí˜„ë ¥": 0.02
            },
            "ë””ìì´ë„ˆ": {
                "ì§ë¬´_ì í•©ì„±": 0.3,
                "ê¸°ìˆ _ì—­ëŸ‰": 0.2,
                "ê²½ë ¥_ì„±ê³¼": 0.2,
                "ë¬¸ì œ_í•´ê²°": 0.15,
                "í˜‘ì—…": 0.1,
                "ì„±ì¥ì„±": 0.03,
                "í‘œí˜„ë ¥": 0.02
            },
            "ë§ˆì¼€í„°": {
                "ì§ë¬´_ì í•©ì„±": 0.25,
                "ê¸°ìˆ _ì—­ëŸ‰": 0.1,
                "ê²½ë ¥_ì„±ê³¼": 0.25,
                "ë¬¸ì œ_í•´ê²°": 0.15,
                "í˜‘ì—…": 0.15,
                "ì„±ì¥ì„±": 0.05,
                "í‘œí˜„ë ¥": 0.05
            }
        }
        
        # ê¸°ë³¸ ê°€ì¤‘ì¹˜
        self.default_weights = {
            "ì§ë¬´_ì í•©ì„±": 0.25,
            "ê¸°ìˆ _ì—­ëŸ‰": 0.2,
            "ê²½ë ¥_ì„±ê³¼": 0.2,
            "ë¬¸ì œ_í•´ê²°": 0.15,
            "í˜‘ì—…": 0.1,
            "ì„±ì¥ì„±": 0.05,
            "í‘œí˜„ë ¥": 0.05
        }
    
    def _init_huggingface_models(self):
        """Hugging Face ëª¨ë¸ë“¤ ì´ˆê¸°í™”"""
        try:
            logger.info("ğŸš€ Hugging Face ëª¨ë¸ ì´ˆê¸°í™” ì‹œì‘...")
            
            # 1. ì„ë² ë”© ëª¨ë¸: multi-qa-MiniLM-L6-cos-v1
            logger.info("ğŸ“Š ì„ë² ë”© ëª¨ë¸ ë¡œë”© ì¤‘...")
            self.embedding_model = SentenceTransformer('multi-qa-MiniLM-L6-cos-v1', device=self.device)
            
            # 2. ìš”ì•½ ëª¨ë¸: facebook/bart-large-cnn
            logger.info("ğŸ“ ìš”ì•½ ëª¨ë¸ ë¡œë”© ì¤‘...")
            self.summary_tokenizer = AutoTokenizer.from_pretrained("facebook/bart-large-cnn")
            self.summary_model = AutoModelForSeq2SeqLM.from_pretrained("facebook/bart-large-cnn")
            if self.device == "cuda":
                self.summary_model = self.summary_model.to(self.device)
            
            # 3. ë¶„ë¥˜ ëª¨ë¸: facebook/bart-large-mnli
            logger.info("ğŸ·ï¸ ë¶„ë¥˜ ëª¨ë¸ ë¡œë”© ì¤‘...")
            self.classifier = pipeline(
                "zero-shot-classification",
                model="facebook/bart-large-mnli",
                device=0 if self.device == "cuda" else -1
            )
            
            # 4. ë¬¸ë²• ê²€ì‚¬ ëª¨ë¸: prithivida/grammar_error_correcter_v1
            logger.info("âœï¸ ë¬¸ë²• ê²€ì‚¬ ëª¨ë¸ ë¡œë”© ì¤‘...")
            self.grammar_tokenizer = AutoTokenizer.from_pretrained("prithivida/grammar_error_correcter_v1")
            self.grammar_model = AutoModelForSeq2SeqLM.from_pretrained("prithivida/grammar_error_correcter_v1")
            if self.device == "cuda":
                self.grammar_model = self.grammar_model.to(self.device)
            
            logger.info("âœ… Hugging Face ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ!")
            
        except Exception as e:
            logger.error(f"âŒ Hugging Face ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            raise
    
    def _init_openai_service(self):
        """OpenAI ì„œë¹„ìŠ¤ ì´ˆê¸°í™”"""
        try:
            # ê¸°ì¡´ openai_service ì‚¬ìš©
            from openai_service import OpenAIService
            self.openai_service = OpenAIService()
            logger.info("âœ… OpenAI ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ!")
        except Exception as e:
            logger.error(f"âŒ OpenAI ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self.openai_service = None
    
    async def analyze_resume_two_stage(self, 
                                     resume_text: str, 
                                     cover_letter_text: str = "",
                                     job_description: str = "",
                                     job_position: str = "ê°œë°œì") -> Dict[str, Any]:
        """2ë‹¨ê³„ ì´ë ¥ì„œ ë¶„ì„"""
        try:
            logger.info("ğŸ” 2ë‹¨ê³„ ì´ë ¥ì„œ ë¶„ì„ ì‹œì‘...")
            
            # 1ë‹¨ê³„: Hugging Face ëª¨ë¸ë¡œ ë¹ ë¥¸ ê¸°ê³„ì  í‰ê°€
            logger.info("ğŸ“Š 1ë‹¨ê³„: Hugging Face ê¸°ê³„ì  í‰ê°€ ì‹œì‘...")
            stage1_results = await self._stage1_huggingface_analysis(
                resume_text, cover_letter_text, job_description
            )
            
            # 2ë‹¨ê³„: GPT-4oë¡œ ì‚¬ëŒë‹¤ìš´ ì¢…í•© í‰ê°€
            logger.info("ğŸ§  2ë‹¨ê³„: GPT-4o ì¢…í•© í‰ê°€ ì‹œì‘...")
            stage2_results = await self._stage2_gpt_analysis(
                resume_text, cover_letter_text, stage1_results
            )
            
            # ìµœì¢… ì ìˆ˜ ê³„ì‚° ë° ë­í‚¹
            final_score = self._calculate_final_score(
                stage1_results, stage2_results, job_position
            )
            
            # ê²°ê³¼ í†µí•©
            analysis_result = {
                "analysis_timestamp": datetime.now().isoformat(),
                "job_position": job_position,
                "stage1_huggingface": stage1_results,
                "stage2_gpt": stage2_results,
                "final_score": final_score,
                "ranking_ready": True
            }
            
            logger.info(f"âœ… 2ë‹¨ê³„ ë¶„ì„ ì™„ë£Œ! ìµœì¢… ì ìˆ˜: {final_score:.2f}")
            return analysis_result
            
        except Exception as e:
            logger.error(f"âŒ 2ë‹¨ê³„ ë¶„ì„ ì‹¤íŒ¨: {e}")
            raise
    
    async def _stage1_huggingface_analysis(self, 
                                          resume_text: str, 
                                          cover_letter_text: str,
                                          job_description: str) -> Dict[str, Any]:
        """1ë‹¨ê³„: Hugging Face ëª¨ë¸ ê¸°ë°˜ ë¶„ì„"""
        try:
            results = {}
            
            # 1. ì„ë² ë”© ê¸°ë°˜ ì§ë¬´ ì í•©ì„± ì ìˆ˜
            if job_description:
                job_fit_score = self._calculate_job_fit_score(resume_text, job_description)
                results["job_fit_score"] = job_fit_score
            
            # 2. ìš”ì•½ ìƒì„±
            summary = self._generate_summary(resume_text)
            results["summary"] = summary
            
            # 3. ìŠ¤í‚¬ ë¶„ë¥˜ ë° ì ìˆ˜
            skill_scores = self._classify_skills(resume_text)
            results["skill_scores"] = skill_scores
            
            # 4. ë¬¸ë²• ì ìˆ˜
            grammar_score = self._check_grammar(resume_text + " " + cover_letter_text)
            results["grammar_score"] = grammar_score
            
            # 5. í…ìŠ¤íŠ¸ í’ˆì§ˆ ì ìˆ˜
            text_quality_score = self._analyze_text_quality(resume_text + " " + cover_letter_text)
            results["text_quality_score"] = text_quality_score
            
            return results
            
        except Exception as e:
            logger.error(f"âŒ 1ë‹¨ê³„ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {"error": str(e)}
    
    def _calculate_job_fit_score(self, resume_text: str, job_description: str) -> float:
        """ì„ë² ë”© ê¸°ë°˜ ì§ë¬´ ì í•©ì„± ì ìˆ˜ ê³„ì‚°"""
        try:
            # í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë¶„í• 
            resume_chunks = self._split_text_to_chunks(resume_text, max_length=512)
            job_chunks = self._split_text_to_chunks(job_description, max_length=512)
            
            # ì„ë² ë”© ìƒì„±
            resume_embeddings = self.embedding_model.encode(resume_chunks)
            job_embeddings = self.embedding_model.encode(job_chunks)
            
            # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
            similarities = cosine_similarity(resume_embeddings, job_embeddings)
            
            # ìµœê³  ìœ ì‚¬ë„ ì ìˆ˜ ë°˜í™˜ (0-100 ìŠ¤ì¼€ì¼)
            max_similarity = np.max(similarities)
            return float(max_similarity * 100)
            
        except Exception as e:
            logger.error(f"ì§ë¬´ ì í•©ì„± ì ìˆ˜ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return 50.0  # ê¸°ë³¸ê°’
    
    def _generate_summary(self, text: str, max_length: int = 150) -> str:
        """í…ìŠ¤íŠ¸ ìš”ì•½ ìƒì„±"""
        try:
            if len(text) < 100:
                return text
            
            # í…ìŠ¤íŠ¸ë¥¼ ì ì ˆí•œ ê¸¸ì´ë¡œ ìë¥´ê¸°
            if len(text) > 1024:
                text = text[:1024]
            
            inputs = self.summary_tokenizer(text, return_tensors="pt", max_length=1024, truncation=True)
            if self.device == "cuda":
                inputs = {k: v.to(self.device) for k, v in inputs.items()}
            
            summary_ids = self.summary_model.generate(
                inputs["input_ids"], 
                max_length=max_length, 
                min_length=30, 
                length_penalty=2.0,
                num_beams=4,
                early_stopping=True
            )
            
            summary = self.summary_tokenizer.decode(summary_ids[0], skip_special_tokens=True)
            return summary
            
        except Exception as e:
            logger.error(f"ìš”ì•½ ìƒì„± ì‹¤íŒ¨: {e}")
            return text[:200] + "..." if len(text) > 200 else text
    
    def _classify_skills(self, text: str) -> Dict[str, float]:
        """ìŠ¤í‚¬ ë¶„ë¥˜ ë° ì ìˆ˜"""
        try:
            # ì¼ë°˜ì ì¸ IT ìŠ¤í‚¬ ì¹´í…Œê³ ë¦¬
            skill_categories = [
                "í”„ë¡œê·¸ë˜ë° ì–¸ì–´",
                "ì›¹ ê°œë°œ",
                "ë°ì´í„°ë² ì´ìŠ¤",
                "í´ë¼ìš°ë“œ/DevOps",
                "AI/ML",
                "ëª¨ë°”ì¼ ê°œë°œ",
                "í”„ë¡œì íŠ¸ ê´€ë¦¬",
                "ë””ìì¸/UX"
            ]
            
            # Zero-shot ë¶„ë¥˜ë¡œ ê° ì¹´í…Œê³ ë¦¬ë³„ ì ìˆ˜ ê³„ì‚°
            results = {}
            for category in skill_categories:
                try:
                    classification = self.classifier(
                        text, 
                        candidate_labels=[category, "í•´ë‹¹ì—†ìŒ"],
                        hypothesis_template="ì´ í…ìŠ¤íŠ¸ëŠ” {}ì— ê´€í•œ ë‚´ìš©ì…ë‹ˆë‹¤."
                    )
                    score = classification['scores'][0] * 100
                    results[category] = round(score, 2)
                except Exception as e:
                    logger.warning(f"ìŠ¤í‚¬ ë¶„ë¥˜ ì‹¤íŒ¨ ({category}): {e}")
                    results[category] = 0.0
            
            return results
            
        except Exception as e:
            logger.error(f"ìŠ¤í‚¬ ë¶„ë¥˜ ì‹¤íŒ¨: {e}")
            return {"ê¸°ë³¸": 50.0}
    
    def _check_grammar(self, text: str) -> float:
        """ë¬¸ë²• ê²€ì‚¬ ë° ì ìˆ˜"""
        try:
            if len(text) < 50:
                return 90.0  # ì§§ì€ í…ìŠ¤íŠ¸ëŠ” ê¸°ë³¸ ë†’ì€ ì ìˆ˜
            
            # ë¬¸ë²• ê²€ì‚¬ ëª¨ë¸ë¡œ í…ìŠ¤íŠ¸ ì²˜ë¦¬
            inputs = self.grammar_tokenizer(text, return_tensors="pt", max_length=512, truncation=True)
            if self.device == "cuda":
                inputs = {k: v.to(self.device) for k, v in inputs.items()}
            
            corrected_ids = self.grammar_model.generate(
                inputs["input_ids"],
                max_length=512,
                num_beams=5,
                early_stopping=True
            )
            
            corrected_text = self.grammar_tokenizer.decode(corrected_ids[0], skip_special_tokens=True)
            
            # ì›ë³¸ê³¼ ìˆ˜ì •ëœ í…ìŠ¤íŠ¸ ë¹„êµë¡œ ë¬¸ë²• ì ìˆ˜ ê³„ì‚°
            original_words = text.split()
            corrected_words = corrected_text.split()
            
            if len(original_words) == 0:
                return 90.0
            
            # ë‹¨ì–´ ìˆ˜ ë³€í™”ìœ¨ë¡œ ë¬¸ë²• ì ìˆ˜ ì¶”ì •
            change_ratio = abs(len(corrected_words) - len(original_words)) / len(original_words)
            grammar_score = max(0, 100 - (change_ratio * 100))
            
            return round(grammar_score, 2)
            
        except Exception as e:
            logger.error(f"ë¬¸ë²• ê²€ì‚¬ ì‹¤íŒ¨: {e}")
            return 75.0  # ê¸°ë³¸ê°’
    
    def _analyze_text_quality(self, text: str) -> float:
        """í…ìŠ¤íŠ¸ í’ˆì§ˆ ë¶„ì„"""
        try:
            if not text:
                return 0.0
            
            score = 0.0
            total_factors = 0
            
            # 1. ê¸¸ì´ ì ìˆ˜ (ì ì ˆí•œ ê¸¸ì´)
            length_score = min(100, len(text) / 10)  # 1000ì = 100ì 
            score += length_score
            total_factors += 1
            
            # 2. ë¬¸ì¥ êµ¬ì¡° ì ìˆ˜
            sentences = text.split('.')
            avg_sentence_length = sum(len(s.split()) for s in sentences if s.strip()) / len(sentences) if sentences else 0
            if 5 <= avg_sentence_length <= 25:
                structure_score = 100
            else:
                structure_score = max(0, 100 - abs(avg_sentence_length - 15) * 2)
            score += structure_score
            total_factors += 1
            
            # 3. íŠ¹ìˆ˜ë¬¸ì/ìˆ«ì ë¹„ìœ¨ ì ìˆ˜
            special_chars = sum(1 for c in text if not c.isalnum() and not c.isspace())
            special_ratio = special_chars / len(text) if text else 0
            if 0.05 <= special_ratio <= 0.15:
                special_score = 100
            else:
                special_score = max(0, 100 - abs(special_ratio - 0.1) * 1000)
            score += special_score
            total_factors += 1
            
            return round(score / total_factors, 2)
            
        except Exception as e:
            logger.error(f"í…ìŠ¤íŠ¸ í’ˆì§ˆ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return 70.0
    
    async def _stage2_gpt_analysis(self, 
                                  resume_text: str, 
                                  cover_letter_text: str,
                                  stage1_results: Dict[str, Any]) -> Dict[str, Any]:
        """2ë‹¨ê³„: GPT-4o ê¸°ë°˜ ì¢…í•© í‰ê°€"""
        try:
            if not self.openai_service:
                logger.warning("OpenAI ì„œë¹„ìŠ¤ ì—†ìŒ, 1ë‹¨ê³„ ê²°ê³¼ë§Œ ë°˜í™˜")
                return {"error": "OpenAI ì„œë¹„ìŠ¤ ì‚¬ìš© ë¶ˆê°€"}
            
            # GPT-4o ë¶„ì„ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
            analysis_prompt = self._build_gpt_analysis_prompt(
                resume_text, cover_letter_text, stage1_results
            )
            
            # GPT-4oë¡œ ë¶„ì„ ì‹¤í–‰
            gpt_response = await self.openai_service.generate_detailed_analysis_with_gpt4o(
                analysis_prompt, "resume"
            )
            
            # ì‘ë‹µ íŒŒì‹± ë° êµ¬ì¡°í™”
            structured_analysis = self._parse_gpt_analysis_response(gpt_response)
            
            return structured_analysis
            
        except Exception as e:
            logger.error(f"âŒ 2ë‹¨ê³„ GPT ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {"error": str(e)}
    
    def _build_gpt_analysis_prompt(self, 
                                  resume_text: str, 
                                  cover_letter_text: str,
                                  stage1_results: Dict[str, Any]) -> str:
        """GPT ë¶„ì„ìš© í”„ë¡¬í”„íŠ¸ êµ¬ì„±"""
        
        # 1ë‹¨ê³„ ê²°ê³¼ ìš”ì•½
        stage1_summary = f"""
1ë‹¨ê³„ ê¸°ê³„ì  ë¶„ì„ ê²°ê³¼:
- ì§ë¬´ ì í•©ì„±: {stage1_results.get('job_fit_score', 'N/A')}ì 
- ë¬¸ë²• ì ìˆ˜: {stage1_results.get('grammar_score', 'N/A')}ì 
- í…ìŠ¤íŠ¸ í’ˆì§ˆ: {stage1_results.get('text_quality_score', 'N/A')}ì 
- ì£¼ìš” ìŠ¤í‚¬: {', '.join([f'{k}({v}ì )' for k, v in stage1_results.get('skill_scores', {}).items() if v > 50])}
"""
        
        prompt = f"""
ë‹¹ì‹ ì€ 15ë…„ ê²½ë ¥ì˜ HR ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë‹¤ìŒ ì´ë ¥ì„œì™€ ìê¸°ì†Œê°œì„œë¥¼ ìƒì„¸íˆ ë¶„ì„í•˜ì—¬ í‰ê°€í•´ì£¼ì„¸ìš”.

[ë¶„ì„í•  ë¬¸ì„œ]
ì´ë ¥ì„œ: {resume_text[:2000]}{'...' if len(resume_text) > 2000 else ''}
ìê¸°ì†Œê°œì„œ: {cover_letter_text[:1000]}{'...' if len(cover_letter_text) > 1000 else ''}

{stage1_summary}

[ë¶„ì„ í•­ëª© ë° í‰ê°€ ê¸°ì¤€]
1. ì§ë¬´ ì í•©ì„± (0-100ì ): ì§€ì› ì§ë¬´ì™€ì˜ ì—°ê´€ì„± ë° ì í•©ì„±
2. í•µì‹¬ ê¸°ìˆ  ì—­ëŸ‰ (0-100ì ): ê¸°ìˆ  ìŠ¤í‚¬ì˜ êµ¬ì²´ì„±ê³¼ ìˆ˜ì¤€
3. ê²½ë ¥ ë° ì„±ê³¼ (0-100ì ): ê²½ë ¥ ì‚¬í•­ì˜ êµ¬ì²´ì„±ê³¼ ì„±ê³¼
4. ë¬¸ì œ í•´ê²° ëŠ¥ë ¥ (0-100ì ): ë¬¸ì œ í•´ê²° ê²½í—˜ê³¼ ì ‘ê·¼ ë°©ì‹
5. í˜‘ì—…/ì»¤ë®¤ë‹ˆì¼€ì´ì…˜ (0-100ì ): íŒ€ì›Œí¬ì™€ ì†Œí†µ ëŠ¥ë ¥
6. ì„±ì¥ ê°€ëŠ¥ì„± (0-100ì ): í•™ìŠµ ì˜ì§€ì™€ ë°œì „ ê°€ëŠ¥ì„±
7. í‘œí˜„ë ¥/ë¬¸ë²• (0-100ì ): ë¬¸ì„œ ì‘ì„± ëŠ¥ë ¥ê³¼ ë¬¸ë²•

[ì‘ë‹µ í˜•ì‹]
ë°˜ë“œì‹œ ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”:
{{
    "ì§ë¬´_ì í•©ì„±": 85,
    "í•µì‹¬_ê¸°ìˆ _ì—­ëŸ‰": 90,
    "ê²½ë ¥_ë°_ì„±ê³¼": 70,
    "ë¬¸ì œ_í•´ê²°_ëŠ¥ë ¥": 80,
    "í˜‘ì—…": 75,
    "ì„±ì¥_ê°€ëŠ¥ì„±": 85,
    "í‘œí˜„ë ¥": 90,
    "ì¢…í•©_í‰ê°€": "ì „ë°˜ì ìœ¼ë¡œ ìš°ìˆ˜í•œ ì§€ì›ìì´ë‚˜, ê²½ë ¥ ì„±ê³¼ì˜ êµ¬ì²´í™”ê°€ í•„ìš”í•©ë‹ˆë‹¤."
}}
"""
        return prompt
    
    def _parse_gpt_analysis_response(self, gpt_response: Any) -> Dict[str, Any]:
        """GPT ì‘ë‹µ íŒŒì‹± ë° êµ¬ì¡°í™”"""
        try:
            if hasattr(gpt_response, 'analysis_result'):
                analysis_text = gpt_response.analysis_result
            elif isinstance(gpt_response, str):
                analysis_text = gpt_response
            else:
                analysis_text = str(gpt_response)
            
            # JSON ì¶”ì¶œ ì‹œë„
            try:
                # JSON ë¶€ë¶„ ì°¾ê¸°
                start_idx = analysis_text.find('{')
                end_idx = analysis_text.rfind('}') + 1
                
                if start_idx != -1 and end_idx != -1:
                    json_str = analysis_text[start_idx:end_idx]
                    parsed = json.loads(json_str)
                    
                    # ì ìˆ˜ ì •ê·œí™” (0-100 ë²”ìœ„)
                    normalized_scores = {}
                    for key, value in parsed.items():
                        if key != "ì¢…í•©_í‰ê°€" and isinstance(value, (int, float)):
                            normalized_scores[key] = min(100, max(0, int(value)))
                        elif key == "ì¢…í•©_í‰ê°€":
                            normalized_scores[key] = value
                    
                    return normalized_scores
                else:
                    raise ValueError("JSON í˜•ì‹ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                    
            except (json.JSONDecodeError, ValueError) as e:
                logger.warning(f"JSON íŒŒì‹± ì‹¤íŒ¨, ê¸°ë³¸ê°’ ì‚¬ìš©: {e}")
                # ê¸°ë³¸ ì ìˆ˜ ë°˜í™˜
                return {
                    "ì§ë¬´_ì í•©ì„±": 70,
                    "í•µì‹¬_ê¸°ìˆ _ì—­ëŸ‰": 70,
                    "ê²½ë ¥_ë°_ì„±ê³¼": 70,
                    "ë¬¸ì œ_í•´ê²°_ëŠ¥ë ¥": 70,
                    "í˜‘ì—…": 70,
                    "ì„±ì¥_ê°€ëŠ¥ì„±": 70,
                    "í‘œí˜„ë ¥": 70,
                    "ì¢…í•©_í‰ê°€": "GPT ë¶„ì„ ì‹¤íŒ¨ë¡œ ê¸°ë³¸ ì ìˆ˜ ì ìš©"
                }
                
        except Exception as e:
            logger.error(f"GPT ì‘ë‹µ íŒŒì‹± ì‹¤íŒ¨: {e}")
            return {"error": str(e)}
    
    def _calculate_final_score(self, 
                              stage1_results: Dict[str, Any], 
                              stage2_results: Dict[str, Any],
                              job_position: str) -> float:
        """ìµœì¢… ì ìˆ˜ ê³„ì‚° (ê°€ì¤‘ì¹˜ ì ìš©)"""
        try:
            # ì§ë¬´ë³„ ê°€ì¤‘ì¹˜ ì„ íƒ
            weights = self.job_weights.get(job_position, self.default_weights)
            
            # 1ë‹¨ê³„ ì ìˆ˜ë“¤
            stage1_scores = {
                "ì§ë¬´_ì í•©ì„±": stage1_results.get("job_fit_score", 50),
                "ê¸°ìˆ _ì—­ëŸ‰": np.mean(list(stage1_results.get("skill_scores", {}).values())) if stage1_results.get("skill_scores") else 50,
                "í‘œí˜„ë ¥": stage1_results.get("grammar_score", 75),
                "í…ìŠ¤íŠ¸_í’ˆì§ˆ": stage1_results.get("text_quality_score", 70)
            }
            
            # 2ë‹¨ê³„ ì ìˆ˜ë“¤
            stage2_scores = {
                "ì§ë¬´_ì í•©ì„±": stage2_results.get("ì§ë¬´_ì í•©ì„±", 70),
                "ê¸°ìˆ _ì—­ëŸ‰": stage2_results.get("í•µì‹¬_ê¸°ìˆ _ì—­ëŸ‰", 70),
                "ê²½ë ¥_ì„±ê³¼": stage2_results.get("ê²½ë ¥_ë°_ì„±ê³¼", 70),
                "ë¬¸ì œ_í•´ê²°": stage2_results.get("ë¬¸ì œ_í•´ê²°_ëŠ¥ë ¥", 70),
                "í˜‘ì—…": stage2_results.get("í˜‘ì—…", 70),
                "ì„±ì¥ì„±": stage2_results.get("ì„±ì¥_ê°€ëŠ¥ì„±", 70),
                "í‘œí˜„ë ¥": stage2_results.get("í‘œí˜„ë ¥", 70)
            }
            
            # ê°€ì¤‘ í‰ê·  ê³„ì‚°
            final_score = 0.0
            total_weight = 0.0
            
            for category, weight in weights.items():
                if category in stage1_scores:
                    final_score += stage1_scores[category] * weight
                    total_weight += weight
                elif category in stage2_scores:
                    final_score += stage2_scores[category] * weight
                    total_weight += weight
            
            if total_weight > 0:
                final_score = final_score / total_weight
            
            return round(final_score, 2)
            
        except Exception as e:
            logger.error(f"ìµœì¢… ì ìˆ˜ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return 70.0  # ê¸°ë³¸ê°’
    
    def _split_text_to_chunks(self, text: str, max_length: int = 512) -> List[str]:
        """í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë¶„í• """
        try:
            words = text.split()
            chunks = []
            current_chunk = []
            current_length = 0
            
            for word in words:
                if current_length + len(word) + 1 <= max_length:
                    current_chunk.append(word)
                    current_length += len(word) + 1
                else:
                    if current_chunk:
                        chunks.append(" ".join(current_chunk))
                    current_chunk = [word]
                    current_length = len(word)
            
            if current_chunk:
                chunks.append(" ".join(current_chunk))
            
            return chunks if chunks else [text[:max_length]]
            
        except Exception as e:
            logger.error(f"í…ìŠ¤íŠ¸ ì²­í‚¹ ì‹¤íŒ¨: {e}")
            return [text[:max_length]]
    
    async def get_ranking_data(self, applicants_data: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """ì§€ì›ì ë­í‚¹ ë°ì´í„° ìƒì„±"""
        try:
            ranked_applicants = []
            
            for applicant in applicants_data:
                # ìµœì¢… ì ìˆ˜ ì¶”ì¶œ
                final_score = applicant.get("final_score", 0)
                
                # ë­í‚¹ ì •ë³´ ì¶”ê°€
                ranked_applicant = applicant.copy()
                ranked_applicant["ranking_score"] = final_score
                ranked_applicants.append(ranked_applicant)
            
            # ì ìˆ˜ ê¸°ì¤€ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬
            ranked_applicants.sort(key=lambda x: x.get("ranking_score", 0), reverse=True)
            
            # ë­í‚¹ ìˆœìœ„ ì¶”ê°€
            for i, applicant in enumerate(ranked_applicants):
                applicant["rank"] = i + 1
                applicant["rank_percentage"] = round((i + 1) / len(ranked_applicants) * 100, 1)
            
            return ranked_applicants
            
        except Exception as e:
            logger.error(f"ë­í‚¹ ë°ì´í„° ìƒì„± ì‹¤íŒ¨: {e}")
            return applicants_data
