from fastapi import APIRouter, HTTPException, UploadFile, File, Form
from fastapi.responses import JSONResponse
from typing import Optional, Dict, List
import os
from dotenv import load_dotenv
import tempfile
import asyncio
import aiofiles
from datetime import datetime
import sys
sys.path.append('..')  # ìƒìœ„ ë””ë ‰í† ë¦¬ì˜ openai_service.py ì‚¬ìš©
from openai_service import OpenAIService
from pydantic import BaseModel
import re

# .env íŒŒì¼ ë¡œë“œ (í˜„ì¬ ë””ë ‰í† ë¦¬ì—ì„œ)
print(f"ğŸ” upload.py í˜„ì¬ ì‘ì—… ë””ë ‰í† ë¦¬: {os.getcwd()}")
print(f"ğŸ” upload.py .env íŒŒì¼ ì¡´ì¬ ì—¬ë¶€: {os.path.exists('.env')}")
load_dotenv('.env')
print(f"ğŸ” upload.py OPENAI_API_KEY ë¡œë“œ í›„: {os.getenv('OPENAI_API_KEY')}")

# OpenAI API ì„¤ì •
try:
    openai_service = OpenAIService(model_name="gpt-4o-mini")
    print("OpenAI ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì„±ê³µ")
except Exception as e:
    print(f"OpenAI ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
    openai_service = None

router = APIRouter(tags=["upload"])

class SummaryRequest(BaseModel):
    content: str
    summary_type: str = "general"  # general, technical, experience

class SummaryResponse(BaseModel):
    summary: str
    keywords: list[str]
    confidence_score: float
    processing_time: float

# ìƒˆë¡œìš´ ìƒì„¸ ë¶„ì„ ëª¨ë¸ë“¤
class AnalysisScore(BaseModel):
    score: int  # 0-10
    feedback: str

class DocumentValidationRequest(BaseModel):
    content: str
    expected_type: str  # "ì´ë ¥ì„œ", "ìê¸°ì†Œê°œì„œ", "í¬íŠ¸í´ë¦¬ì˜¤"

class DocumentValidationResponse(BaseModel):
    is_valid: bool
    confidence: float
    reason: str
    suggested_type: str

class ResumeAnalysis(BaseModel):
    basic_info_completeness: AnalysisScore
    job_relevance: AnalysisScore
    experience_clarity: AnalysisScore
    tech_stack_clarity: AnalysisScore
    project_recency: AnalysisScore
    achievement_metrics: AnalysisScore
    readability: AnalysisScore
    typos_and_errors: AnalysisScore
    update_freshness: AnalysisScore

class CoverLetterAnalysis(BaseModel):
    motivation_relevance: AnalysisScore
    problem_solving_STAR: AnalysisScore
    quantitative_impact: AnalysisScore
    job_understanding: AnalysisScore
    unique_experience: AnalysisScore
    logical_flow: AnalysisScore
    keyword_diversity: AnalysisScore
    sentence_readability: AnalysisScore
    typos_and_errors: AnalysisScore

class PortfolioAnalysis(BaseModel):
    project_overview: AnalysisScore
    tech_stack: AnalysisScore
    personal_contribution: AnalysisScore
    achievement_metrics: AnalysisScore
    visual_quality: AnalysisScore
    documentation_quality: AnalysisScore
    job_relevance: AnalysisScore
    unique_features: AnalysisScore
    maintainability: AnalysisScore

class OverallSummary(BaseModel):
    total_score: float
    recommendation: str

class DetailedAnalysisResponse(BaseModel):
    resume_analysis: Optional[ResumeAnalysis] = None
    cover_letter_analysis: Optional[CoverLetterAnalysis] = None
    portfolio_analysis: Optional[PortfolioAnalysis] = None
    overall_summary: OverallSummary

# ===== ë¶„ì„ ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ êµ¬ì¡° ìƒì„± ìœ í‹¸ =====
def _build_score(msg: str) -> Dict[str, object]:
    return {"score": 0, "feedback": msg}

def build_fallback_analysis(document_type: str) -> Dict[str, object]:
    reason = "ë¬¸ì„œì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ì–´ í‰ê°€í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. í¸ì§‘ ê°€ëŠ¥í•œ PDF/DOCXë¡œ ì¬ì—…ë¡œë“œí•´ì£¼ì„¸ìš”."
    resume = {
        "basic_info_completeness": _build_score(reason),
        "job_relevance": _build_score(reason),
        "experience_clarity": _build_score(reason),
        "tech_stack_clarity": _build_score(reason),
        "project_recency": _build_score(reason),
        "achievement_metrics": _build_score(reason),
        "readability": _build_score(reason),
        "typos_and_errors": _build_score(reason),
        "update_freshness": _build_score(reason),
    }
    cover = {
        "motivation_relevance": _build_score(reason),
        "problem_solving_STAR": _build_score(reason),
        "quantitative_impact": _build_score(reason),
        "job_understanding": _build_score(reason),
        "unique_experience": _build_score(reason),
        "logical_flow": _build_score(reason),
        "keyword_diversity": _build_score(reason),
        "sentence_readability": _build_score(reason),
        "typos_and_errors": _build_score(reason),
    }
    portfolio = {
        "project_overview": _build_score(reason),
        "tech_stack": _build_score(reason),
        "personal_contribution": _build_score(reason),
        "achievement_metrics": _build_score(reason),
        "visual_quality": _build_score(reason),
        "documentation_quality": _build_score(reason),
        "job_relevance": _build_score(reason),
        "unique_features": _build_score(reason),
        "maintainability": _build_score(reason),
    }
    return {
        "resume_analysis": resume,
        "cover_letter_analysis": cover,
        "portfolio_analysis": portfolio,
        "overall_summary": {"total_score": 0, "recommendation": reason},
    }

# ===== ë‚´ìš© ê¸°ë°˜ ë¬¸ì„œ ìœ í˜• ë¶„ë¥˜ê¸° =====
def classify_document_type_by_content(text: str) -> Dict[str, object]:
    """ê°„ë‹¨í•œ ê·œì¹™ ê¸°ë°˜ìœ¼ë¡œ ë¬¸ì„œ ìœ í˜•(resume/cover_letter/portfolio)ì„ ë¶„ë¥˜í•©ë‹ˆë‹¤."""
    text_lower = text.lower()

    # í•œêµ­ì–´/ì˜ì–´ í‚¤ì›Œë“œ ì„¸íŠ¸
    resume_keywords = [
        "ê²½ë ¥", "ì´ë ¥", "í”„ë¡œì íŠ¸", "í•™ë ¥", "ê¸°ìˆ ", "ìŠ¤í‚¬", "ìê²©ì¦", "ê·¼ë¬´", "ë‹´ë‹¹", "ì„±ê³¼",
        "ê²½í—˜", "ìš”ì•½", "í•µì‹¬ì—­ëŸ‰", "phone", "email", "github", "linkedin",
        "experience", "education", "skills", "projects", "certificate"
    ]
    cover_letter_keywords = [
        "ì§€ì›ë™ê¸°", "ì„±ì¥ë°°ê²½", "ì…ì‚¬", "í¬ë¶€", "ì €ëŠ”", "ë°°ìš°ë©°", "í•˜ê³ ì", "ê¸°ì—¬", "ê´€ì‹¬",
        "ë™ê¸°", "ì—´ì •", "ì™œ", "ì™œ ìš°ë¦¬", "motiv", "cover letter", "passion"
    ]
    portfolio_keywords = [
        "í¬íŠ¸í´ë¦¬ì˜¤", "ì‘í’ˆ", "ì‹œì—°", "ë°ëª¨", "ë§í¬", "ì´ë¯¸ì§€", "ìŠ¤ìƒ·", "ìº¡ì²˜", "ë ˆí¬ì§€í† ë¦¬",
        "repository", "demo", "screenshot", "figma", "behance", "dribbble"
    ]

    def score_keywords(keywords: List[str]) -> float:
        score = 0.0
        for kw in keywords:
            # ë‹¨ì–´ ê²½ê³„ ìš°ì„ , ì—†ìœ¼ë©´ í¬í•¨ ê²€ì‚¬
            if re.search(rf"\b{re.escape(kw)}\b", text_lower) or kw in text_lower:
                score += 1.0
        # ì„¹ì…˜ í—¤ë” ë³´ë„ˆìŠ¤
        section_headers = ["ê²½ë ¥", "í•™ë ¥", "í”„ë¡œì íŠ¸", "skills", "experience", "education"]
        if any(h in text for h in section_headers):
            score += 0.5
        # ì—°ë½ì²˜ íŒ¨í„´ ë³´ë„ˆìŠ¤ (ì´ë ¥ì„œ ì§€í‘œ)
        if re.search(r"[0-9]{2,3}-[0-9]{3,4}-[0-9]{4}", text) or re.search(r"\b[a-z0-9._%+-]+@[a-z0-9.-]+\.[a-z]{2,}\b", text_lower):
            score += 0.7
        return score

    resume_score = score_keywords(resume_keywords)
    cover_letter_score = sum(1.0 for kw in cover_letter_keywords if kw in text_lower)
    portfolio_score = sum(1.0 for kw in portfolio_keywords if kw in text_lower)

    scores = {
        "resume": resume_score,
        "cover_letter": cover_letter_score,
        "portfolio": portfolio_score,
    }

    detected_type = max(scores.items(), key=lambda x: x[1])[0]
    max_score = scores[detected_type]
    # ê°„ë‹¨í•œ ì‹ ë¢°ë„ ì •ê·œí™” (ìµœëŒ€ 10ì  ê°€ì •)
    confidence = min(round(max_score / 10.0, 2), 1.0)

    return {"detected_type": detected_type, "confidence": confidence, "scores": scores}

# í—ˆìš©ëœ íŒŒì¼ íƒ€ì…
ALLOWED_EXTENSIONS = {
    '.pdf': 'application/pdf',
    '.doc': 'application/msword',
    '.docx': 'application/vnd.openxmlformats-officedocument.wordprocessingml.document',
    '.txt': 'text/plain'
}

# íŒŒì¼ í¬ê¸° ì œí•œ (50MB)
MAX_FILE_SIZE = 50 * 1024 * 1024

def validate_file(file: UploadFile) -> bool:
    """íŒŒì¼ ìœ íš¨ì„± ê²€ì‚¬"""
    if not file.filename:
        return False
    
    # íŒŒì¼ í™•ì¥ì í™•ì¸
    file_ext = os.path.splitext(file.filename.lower())[1]
    if file_ext not in ALLOWED_EXTENSIONS:
        return False
    
    return True

async def extract_text_from_file(file_path: str, file_ext: str) -> str:
    """íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ (ë‹¤ì¤‘ ë°±ì—… ì „ëµ)"""
    try:
        if file_ext == '.txt':
            async with aiofiles.open(file_path, 'r', encoding='utf-8') as f:
                return await f.read()
        elif file_ext == '.pdf':
            # 1ì°¨: PyPDF2
            try:
                import PyPDF2
                text = ""
                with open(file_path, 'rb') as pdf_file:
                    pdf_reader = PyPDF2.PdfReader(pdf_file)
                    for page in pdf_reader.pages:
                        extracted = page.extract_text() or ""
                        text += extracted + ("\n" if extracted else "")
                if text.strip():
                    return text
            except Exception:
                pass
            # 2ì°¨: pdfplumber
            try:
                import pdfplumber  # type: ignore
                text = ""
                with pdfplumber.open(file_path) as pdf:
                    for p in pdf.pages:
                        extracted = p.extract_text() or ""
                        text += extracted + ("\n" if extracted else "")
                if text.strip():
                    return text
            except Exception:
                pass
            # ì‹¤íŒ¨ ì‹œ ë¹ˆ ë¬¸ìì—´ ë°˜í™˜
            return ""
        elif file_ext in ['.doc', '.docx']:
            # 1ì°¨: python-docx
            try:
                from docx import Document  # type: ignore
                doc = Document(file_path)
                text = "\n".join([p.text for p in doc.paragraphs if p.text])
                if text.strip():
                    return text
            except Exception:
                pass
            # 2ì°¨: docx2txt
            try:
                import docx2txt  # type: ignore
                text = docx2txt.process(file_path) or ""
                return text
            except Exception:
                pass
            return ""
        else:
            return ""
    except Exception as e:
        return ""

async def generate_summary_with_openai(content: str, summary_type: str = "general") -> SummaryResponse:
    """OpenAI APIë¥¼ ì‚¬ìš©í•˜ì—¬ ìš”ì•½ ìƒì„±"""
    if not openai_service:
        raise HTTPException(status_code=500, detail="OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    
    start_time = datetime.now()
    
    try:
        # ìš”ì•½ íƒ€ì…ì— ë”°ë¥¸ í”„ë¡¬í”„íŠ¸ ì„¤ì •
        prompts = {
            "general": f"""
            ë‹¤ìŒ ì´ë ¥ì„œ/ìê¸°ì†Œê°œì„œ ë‚´ìš©ì„ ê°„ê²°í•˜ê²Œ ìš”ì•½í•´ì£¼ì„¸ìš”:
            
            {content}
            
            ìš”ì•½ ì‹œ ë‹¤ìŒ ì‚¬í•­ì„ í¬í•¨í•´ì£¼ì„¸ìš”:
            1. ì£¼ìš” ê²½ë ¥ ë° ê²½í—˜
            2. í•µì‹¬ ê¸°ìˆ  ìŠ¤íƒ
            3. ì£¼ìš” ì„±ê³¼ë‚˜ í”„ë¡œì íŠ¸
            4. ì§€ì› ì§ë¬´ì™€ì˜ ì—°ê´€ì„±
            
            ìš”ì•½ì€ 200ì ì´ë‚´ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”.
            """,
            "technical": f"""
            ë‹¤ìŒ ë‚´ìš©ì—ì„œ ê¸°ìˆ ì  ì—­ëŸ‰ì„ ì¤‘ì‹¬ìœ¼ë¡œ ìš”ì•½í•´ì£¼ì„¸ìš”:
            
            {content}
            
            ë‹¤ìŒ í•­ëª©ë“¤ì„ í¬í•¨í•´ì£¼ì„¸ìš”:
            1. í”„ë¡œê·¸ë˜ë° ì–¸ì–´ ë° í”„ë ˆì„ì›Œí¬
            2. ê°œë°œ ë„êµ¬ ë° í”Œë«í¼
            3. í”„ë¡œì íŠ¸ ê²½í—˜
            4. ê¸°ìˆ ì  ì„±ê³¼
            
            ìš”ì•½ì€ 150ì ì´ë‚´ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”.
            """,
            "experience": f"""
            ë‹¤ìŒ ë‚´ìš©ì—ì„œ ê²½ë ¥ê³¼ ê²½í—˜ì„ ì¤‘ì‹¬ìœ¼ë¡œ ìš”ì•½í•´ì£¼ì„¸ìš”:
            
            {content}
            
            ë‹¤ìŒ í•­ëª©ë“¤ì„ í¬í•¨í•´ì£¼ì„¸ìš”:
            1. ì´ ê²½ë ¥ ê¸°ê°„
            2. ì£¼ìš” íšŒì‚¬ ë° ì§ë¬´
            3. í•µì‹¬ í”„ë¡œì íŠ¸ ê²½í—˜
            4. ì£¼ìš” ì„±ê³¼ ë° ì—…ì 
            
            ìš”ì•½ì€ 150ì ì´ë‚´ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”.
            """
        }
        
        prompt = prompts.get(summary_type, prompts["general"])
        
        # OpenAI API í˜¸ì¶œ
        summary = await openai_service.generate_response(prompt)
        
        # í‚¤ì›Œë“œ ì¶”ì¶œì„ ìœ„í•œ ì¶”ê°€ ìš”ì²­
        keyword_prompt = f"""
        ë‹¤ìŒ ìš”ì•½ì—ì„œ í•µì‹¬ í‚¤ì›Œë“œ 5ê°œë¥¼ ì¶”ì¶œí•´ì£¼ì„¸ìš”:
        
        {summary}
        
        í‚¤ì›Œë“œëŠ” ì‰¼í‘œë¡œ êµ¬ë¶„í•˜ì—¬ ë‚˜ì—´í•´ì£¼ì„¸ìš”.
        """
        
        keyword_response = await openai_service.generate_response(keyword_prompt)
        
        keywords = [kw.strip() for kw in keyword_response.split(',')]
        
        processing_time = (datetime.now() - start_time).total_seconds()
        
        return SummaryResponse(
            summary=summary,
            keywords=keywords[:5],  # ìµœëŒ€ 5ê°œ í‚¤ì›Œë“œ
            confidence_score=0.85,  # ê¸°ë³¸ ì‹ ë¢°ë„ ì ìˆ˜
            processing_time=processing_time
        )
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ìš”ì•½ ìƒì„± ì‹¤íŒ¨: {str(e)}")

@router.post("/file")
async def upload_and_summarize_file(
    file: UploadFile = File(...),
    summary_type: str = Form("general")
):
    """íŒŒì¼ ì—…ë¡œë“œ ë° ìš”ì•½"""
    try:
        # íŒŒì¼ ìœ íš¨ì„± ê²€ì‚¬
        if not validate_file(file):
            raise HTTPException(
                status_code=400, 
                detail="ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹ì…ë‹ˆë‹¤. PDF, DOC, DOCX, TXT íŒŒì¼ë§Œ ì—…ë¡œë“œ ê°€ëŠ¥í•©ë‹ˆë‹¤."
            )
        
        # íŒŒì¼ í¬ê¸° í™•ì¸
        file_size = 0
        content = await file.read()
        file_size = len(content)
        
        if file_size > MAX_FILE_SIZE:
            raise HTTPException(
                status_code=400,
                detail="íŒŒì¼ í¬ê¸°ê°€ ë„ˆë¬´ í½ë‹ˆë‹¤. ìµœëŒ€ 50MBê¹Œì§€ ì—…ë¡œë“œ ê°€ëŠ¥í•©ë‹ˆë‹¤."
            )
        
        # ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥
        file_ext = os.path.splitext(file.filename.lower())[1]
        with tempfile.NamedTemporaryFile(delete=False, suffix=file_ext) as temp_file:
            temp_file.write(content)
            temp_file_path = temp_file.name
        
        try:
            # íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
            extracted_text = await extract_text_from_file(temp_file_path, file_ext)
            
            # í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨ ì‹œì—ë„ ë”ë¯¸ ë¶„ì„ìœ¼ë¡œ ê³„ì† ì§„í–‰ (ì‚¬ìš©ì ê²½í—˜ ê°œì„ )
            if not extracted_text or str(extracted_text).strip() == "":
                print("âš ï¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨: ë¹ˆ ë‚´ìš© ê°ì§€ â†’ ë”ë¯¸ ë¶„ì„ìœ¼ë¡œ ê³„ì† ì§„í–‰í•©ë‹ˆë‹¤.")
                extracted_text = "[EMPTY_CONTENT] í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨ (ìŠ¤ìº” PDF/ì´ë¯¸ì§€ ê¸°ë°˜ ë¬¸ì„œì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.)"
            
            # OpenAI APIë¡œ ìš”ì•½ ìƒì„±
            summary_result = await generate_summary_with_openai(extracted_text, summary_type)
            
            return {
                "filename": file.filename,
                "file_size": file_size,
                "extracted_text_length": len(extracted_text),
                "summary": summary_result.summary,
                "keywords": summary_result.keywords,
                "confidence_score": summary_result.confidence_score,
                "processing_time": summary_result.processing_time,
                "summary_type": summary_type
            }
            
        finally:
            # ì„ì‹œ íŒŒì¼ ì‚­ì œ
            if os.path.exists(temp_file_path):
                os.unlink(temp_file_path)
                
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"íŒŒì¼ ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")

@router.post("/summarize")
async def summarize_text(request: SummaryRequest):
    """í…ìŠ¤íŠ¸ ì§ì ‘ ìš”ì•½"""
    try:
        if not request.content or len(request.content.strip()) == 0:
            raise HTTPException(status_code=400, detail="ìš”ì•½í•  í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.")
        
        summary_result = await generate_summary_with_openai(
            request.content, 
            request.summary_type
        )
        
        return summary_result
        
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ìš”ì•½ ìƒì„± ì‹¤íŒ¨: {str(e)}")

@router.get("/health")
async def upload_health_check():
    """ì—…ë¡œë“œ ì„œë¹„ìŠ¤ í—¬ìŠ¤ ì²´í¬"""
    return {
        "status": "healthy",
        "openai_api_configured": bool(openai_service),
        "supported_formats": list(ALLOWED_EXTENSIONS.keys()),
        "max_file_size_mb": MAX_FILE_SIZE // (1024 * 1024)
    }