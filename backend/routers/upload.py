from fastapi import APIRouter, HTTPException, UploadFile, File, Form
from fastapi.responses import JSONResponse
from typing import Optional, Dict, List
import os
from dotenv import load_dotenv
import tempfile
import asyncio
import aiofiles
from datetime import datetime
import sys
sys.path.append('..')  # ìƒìœ„ ë””ë ‰í† ë¦¬ì˜ openai_service.py ì‚¬ìš©
from openai_service import OpenAIService
from pydantic import BaseModel
import re
import json # Added for JSON parsing

# .env íŒŒì¼ ë¡œë“œ (í˜„ì¬ ë””ë ‰í† ë¦¬ì—ì„œ)
print(f"ğŸ” upload.py í˜„ì¬ ì‘ì—… ë””ë ‰í† ë¦¬: {os.getcwd()}")
print(f"ğŸ” upload.py .env íŒŒì¼ ì¡´ì¬ ì—¬ë¶€: {os.path.exists('.env')}")
load_dotenv('.env')
print(f"ğŸ” upload.py OPENAI_API_KEY ë¡œë“œ í›„: {os.getenv('OPENAI_API_KEY')}")

# OpenAI API ì„¤ì •
try:
    openai_service = OpenAIService(model_name="gpt-4o")  # gpt-4oë¡œ ë³€ê²½
    print("OpenAI ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì„±ê³µ (GPT-4o)")
except Exception as e:
    print(f"OpenAI ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
    openai_service = None

router = APIRouter(tags=["upload"])

class SummaryRequest(BaseModel):
    content: str
    summary_type: str = "general"  # general, technical, experience

class SummaryResponse(BaseModel):
    summary: str
    keywords: list[str]
    confidence_score: float
    processing_time: float

# ìƒˆë¡œìš´ ìƒì„¸ ë¶„ì„ ëª¨ë¸ë“¤
class AnalysisScore(BaseModel):
    score: int  # 0-10
    feedback: str

class DocumentValidationRequest(BaseModel):
    content: str
    expected_type: str  # "ì´ë ¥ì„œ", "ìê¸°ì†Œê°œì„œ", "í¬íŠ¸í´ë¦¬ì˜¤"

class DocumentValidationResponse(BaseModel):
    is_valid: bool
    confidence: float
    reason: str
    suggested_type: str

class ResumeAnalysis(BaseModel):
    basic_info_completeness: AnalysisScore
    job_relevance: AnalysisScore
    experience_clarity: AnalysisScore
    tech_stack_clarity: AnalysisScore
    project_recency: AnalysisScore
    achievement_metrics: AnalysisScore
    readability: AnalysisScore
    typos_and_errors: AnalysisScore
    update_freshness: AnalysisScore

class CoverLetterAnalysis(BaseModel):
    motivation_relevance: AnalysisScore
    problem_solving_STAR: AnalysisScore
    quantitative_impact: AnalysisScore
    job_understanding: AnalysisScore
    unique_experience: AnalysisScore
    logical_flow: AnalysisScore
    keyword_diversity: AnalysisScore
    sentence_readability: AnalysisScore
    typos_and_errors: AnalysisScore

class PortfolioAnalysis(BaseModel):
    project_overview: AnalysisScore
    tech_stack: AnalysisScore
    personal_contribution: AnalysisScore
    achievement_metrics: AnalysisScore
    visual_quality: AnalysisScore
    documentation_quality: AnalysisScore
    job_relevance: AnalysisScore
    unique_features: AnalysisScore
    maintainability: AnalysisScore

class OverallSummary(BaseModel):
    total_score: float
    recommendation: str

class DetailedAnalysisResponse(BaseModel):
    resume_analysis: Optional[ResumeAnalysis] = None
    cover_letter_analysis: Optional[CoverLetterAnalysis] = None
    portfolio_analysis: Optional[PortfolioAnalysis] = None
    overall_summary: OverallSummary

# ===== ë¶„ì„ ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ êµ¬ì¡° ìƒì„± ìœ í‹¸ =====
def _build_score(msg: str) -> Dict[str, object]:
    return {"score": 0, "feedback": msg}

def build_fallback_analysis(document_type: str) -> Dict[str, object]:
    reason = "ë¬¸ì„œì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ì–´ í‰ê°€í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. í¸ì§‘ ê°€ëŠ¥í•œ PDF/DOCXë¡œ ì¬ì—…ë¡œë“œí•´ì£¼ì„¸ìš”."
    resume = {
        "basic_info_completeness": _build_score(reason),
        "job_relevance": _build_score(reason),
        "experience_clarity": _build_score(reason),
        "tech_stack_clarity": _build_score(reason),
        "project_recency": _build_score(reason),
        "achievement_metrics": _build_score(reason),
        "readability": _build_score(reason),
        "typos_and_errors": _build_score(reason),
        "update_freshness": _build_score(reason),
    }
    cover = {
        "motivation_relevance": _build_score(reason),
        "problem_solving_STAR": _build_score(reason),
        "quantitative_impact": _build_score(reason),
        "job_understanding": _build_score(reason),
        "unique_experience": _build_score(reason),
        "logical_flow": _build_score(reason),
        "keyword_diversity": _build_score(reason),
        "sentence_readability": _build_score(reason),
        "typos_and_errors": _build_score(reason),
    }
    portfolio = {
        "project_overview": _build_score(reason),
        "tech_stack": _build_score(reason),
        "personal_contribution": _build_score(reason),
        "achievement_metrics": _build_score(reason),
        "visual_quality": _build_score(reason),
        "documentation_quality": _build_score(reason),
        "job_relevance": _build_score(reason),
        "unique_features": _build_score(reason),
        "maintainability": _build_score(reason),
    }
    return {
        "resume_analysis": resume,
        "cover_letter_analysis": cover,
        "portfolio_analysis": portfolio,
        "overall_summary": {"total_score": 0, "recommendation": reason},
    }

# ===== ë‚´ìš© ê¸°ë°˜ ë¬¸ì„œ ìœ í˜• ë¶„ë¥˜ê¸° =====
def classify_document_type_by_content(text: str) -> Dict[str, object]:
    """ê°„ë‹¨í•œ ê·œì¹™ ê¸°ë°˜ìœ¼ë¡œ ë¬¸ì„œ ìœ í˜•(resume/cover_letter/portfolio)ì„ ë¶„ë¥˜í•©ë‹ˆë‹¤."""
    text_lower = text.lower()

    # í•œêµ­ì–´/ì˜ì–´ í‚¤ì›Œë“œ ì„¸íŠ¸
    resume_keywords = [
        "ê²½ë ¥", "ì´ë ¥", "í”„ë¡œì íŠ¸", "í•™ë ¥", "ê¸°ìˆ ", "ìŠ¤í‚¬", "ìê²©ì¦", "ê·¼ë¬´", "ë‹´ë‹¹", "ì„±ê³¼",
        "ê²½í—˜", "ìš”ì•½", "í•µì‹¬ì—­ëŸ‰", "phone", "email", "github", "linkedin",
        "experience", "education", "skills", "projects", "certificate"
    ]
    cover_letter_keywords = [
        "ì§€ì›ë™ê¸°", "ì„±ì¥ë°°ê²½", "ì…ì‚¬", "í¬ë¶€", "ì €ëŠ”", "ë°°ìš°ë©°", "í•˜ê³ ì", "ê¸°ì—¬", "ê´€ì‹¬",
        "ë™ê¸°", "ì—´ì •", "ì™œ", "ì™œ ìš°ë¦¬", "motiv", "cover letter", "passion"
    ]
    portfolio_keywords = [
        "í¬íŠ¸í´ë¦¬ì˜¤", "ì‘í’ˆ", "ì‹œì—°", "ë°ëª¨", "ë§í¬", "ì´ë¯¸ì§€", "ìŠ¤ìƒ·", "ìº¡ì²˜", "ë ˆí¬ì§€í† ë¦¬",
        "repository", "demo", "screenshot", "figma", "behance", "dribbble"
    ]

    def score_keywords(keywords: List[str]) -> float:
        score = 0.0
        for kw in keywords:
            # ë‹¨ì–´ ê²½ê³„ ìš°ì„ , ì—†ìœ¼ë©´ í¬í•¨ ê²€ì‚¬
            if re.search(rf"\b{re.escape(kw)}\b", text_lower) or kw in text_lower:
                score += 1.0
        # ì„¹ì…˜ í—¤ë” ë³´ë„ˆìŠ¤
        section_headers = ["ê²½ë ¥", "í•™ë ¥", "í”„ë¡œì íŠ¸", "skills", "experience", "education"]
        if any(h in text for h in section_headers):
            score += 0.5
        # ì—°ë½ì²˜ íŒ¨í„´ ë³´ë„ˆìŠ¤ (ì´ë ¥ì„œ ì§€í‘œ)
        if re.search(r"[0-9]{2,3}-[0-9]{3,4}-[0-9]{4}", text) or re.search(r"\b[a-z0-9._%+-]+@[a-z0-9.-]+\.[a-z]{2,}\b", text_lower):
            score += 0.7
        return score

    resume_score = score_keywords(resume_keywords)
    cover_letter_score = sum(1.0 for kw in cover_letter_keywords if kw in text_lower)
    portfolio_score = sum(1.0 for kw in portfolio_keywords if kw in text_lower)

    scores = {
        "resume": resume_score,
        "cover_letter": cover_letter_score,
        "portfolio": portfolio_score,
    }

    detected_type = max(scores.items(), key=lambda x: x[1])[0]
    max_score = scores[detected_type]
    # ê°„ë‹¨í•œ ì‹ ë¢°ë„ ì •ê·œí™” (ìµœëŒ€ 10ì  ê°€ì •)
    confidence = min(round(max_score / 10.0, 2), 1.0)

    return {"detected_type": detected_type, "confidence": confidence, "scores": scores}

# í—ˆìš©ëœ íŒŒì¼ íƒ€ì…
ALLOWED_EXTENSIONS = {
    '.pdf': 'application/pdf',
    '.doc': 'application/msword',
    '.docx': 'application/vnd.openxmlformats-officedocument.wordprocessingml.document',
    '.txt': 'text/plain'
}

# íŒŒì¼ í¬ê¸° ì œí•œ (50MB)
MAX_FILE_SIZE = 50 * 1024 * 1024

def validate_file(file: UploadFile) -> bool:
    """íŒŒì¼ ìœ íš¨ì„± ê²€ì‚¬"""
    if not file.filename:
        return False
    
    # íŒŒì¼ í™•ì¥ì í™•ì¸
    file_ext = os.path.splitext(file.filename.lower())[1]
    if file_ext not in ALLOWED_EXTENSIONS:
        return False
    
    return True

async def extract_text_from_file(file_path: str, file_ext: str) -> str:
    """íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ (ë‹¤ì¤‘ ë°±ì—… ì „ëµ)"""
    try:
        if file_ext == '.txt':
            async with aiofiles.open(file_path, 'r', encoding='utf-8') as f:
                return await f.read()
        elif file_ext == '.pdf':
            # 1ì°¨: PyPDF2
            try:
                import PyPDF2
                text = ""
                with open(file_path, 'rb') as pdf_file:
                    pdf_reader = PyPDF2.PdfReader(pdf_file)
                    for page in pdf_reader.pages:
                        extracted = page.extract_text() or ""
                        text += extracted + ("\n" if extracted else "")
                    if text.strip():
                        return text
            except Exception:
                pass
            # 2ì°¨: pdfplumber
            try:
                import pdfplumber  # type: ignore
                text = ""
                with pdfplumber.open(file_path) as pdf:
                    for p in pdf.pages:
                        extracted = p.extract_text() or ""
                        text += extracted + ("\n" if extracted else "")
                if text.strip():
                    return text
            except Exception:
                pass
            # ì‹¤íŒ¨ ì‹œ ë¹ˆ ë¬¸ìì—´ ë°˜í™˜
            return ""
        elif file_ext in ['.doc', '.docx']:
            # 1ì°¨: python-docx
            try:
                from docx import Document  # type: ignore
                doc = Document(file_path)
                text = "\n".join([p.text for p in doc.paragraphs if p.text])
                if text.strip():
                    return text
            except Exception:
                pass
            # 2ì°¨: docx2txt
            try:
                import docx2txt  # type: ignore
                text = docx2txt.process(file_path) or ""
                return text
            except Exception:
                pass
            return ""
        else:
            return ""
    except Exception as e:
        return ""

async def generate_summary_with_openai(content: str, summary_type: str = "general") -> SummaryResponse:
    """OpenAI APIë¥¼ ì‚¬ìš©í•˜ì—¬ ìš”ì•½ ìƒì„±"""
    if not openai_service:
        raise HTTPException(status_code=500, detail="OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    
    start_time = datetime.now()
    
    try:
        # ìš”ì•½ íƒ€ì…ì— ë”°ë¥¸ í”„ë¡¬í”„íŠ¸ ì„¤ì •
        prompts = {
            "general": f"""
            ë‹¤ìŒ ì´ë ¥ì„œ/ìê¸°ì†Œê°œì„œ ë‚´ìš©ì„ ê°„ê²°í•˜ê²Œ ìš”ì•½í•´ì£¼ì„¸ìš”:
            
            {content}
            
            ìš”ì•½ ì‹œ ë‹¤ìŒ ì‚¬í•­ì„ í¬í•¨í•´ì£¼ì„¸ìš”:
            1. ì£¼ìš” ê²½ë ¥ ë° ê²½í—˜
            2. í•µì‹¬ ê¸°ìˆ  ìŠ¤íƒ
            3. ì£¼ìš” ì„±ê³¼ë‚˜ í”„ë¡œì íŠ¸
            4. ì§€ì› ì§ë¬´ì™€ì˜ ì—°ê´€ì„±
            
            ìš”ì•½ì€ 200ì ì´ë‚´ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”.
            """,
            "technical": f"""
            ë‹¤ìŒ ë‚´ìš©ì—ì„œ ê¸°ìˆ ì  ì—­ëŸ‰ì„ ì¤‘ì‹¬ìœ¼ë¡œ ìš”ì•½í•´ì£¼ì„¸ìš”:
            
            {content}
            
            ë‹¤ìŒ í•­ëª©ë“¤ì„ í¬í•¨í•´ì£¼ì„¸ìš”:
            1. í”„ë¡œê·¸ë˜ë° ì–¸ì–´ ë° í”„ë ˆì„ì›Œí¬
            2. ê°œë°œ ë„êµ¬ ë° í”Œë«í¼
            3. í”„ë¡œì íŠ¸ ê²½í—˜
            4. ê¸°ìˆ ì  ì„±ê³¼
            
            ìš”ì•½ì€ 150ì ì´ë‚´ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”.
            """,
            "experience": f"""
            ë‹¤ìŒ ë‚´ìš©ì—ì„œ ê²½ë ¥ê³¼ ê²½í—˜ì„ ì¤‘ì‹¬ìœ¼ë¡œ ìš”ì•½í•´ì£¼ì„¸ìš”:
            
            {content}
            
            ë‹¤ìŒ í•­ëª©ë“¤ì„ í¬í•¨í•´ì£¼ì„¸ìš”:
            1. ì´ ê²½ë ¥ ê¸°ê°„
            2. ì£¼ìš” íšŒì‚¬ ë° ì§ë¬´
            3. í•µì‹¬ í”„ë¡œì íŠ¸ ê²½í—˜
            4. ì£¼ìš” ì„±ê³¼ ë° ì—…ì 
            
            ìš”ì•½ì€ 150ì ì´ë‚´ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”.
            """
        }
        
        prompt = prompts.get(summary_type, prompts["general"])
        
        # OpenAI API í˜¸ì¶œ
        summary = await openai_service.generate_response(prompt)
        
        # í‚¤ì›Œë“œ ì¶”ì¶œì„ ìœ„í•œ ì¶”ê°€ ìš”ì²­
        keyword_prompt = f"""
        ë‹¤ìŒ ìš”ì•½ì—ì„œ í•µì‹¬ í‚¤ì›Œë“œ 5ê°œë¥¼ ì¶”ì¶œí•´ì£¼ì„¸ìš”:
        
        {summary}
        
        í‚¤ì›Œë“œëŠ” ì‰¼í‘œë¡œ êµ¬ë¶„í•˜ì—¬ ë‚˜ì—´í•´ì£¼ì„¸ìš”.
        """
        
        keyword_response = await openai_service.generate_response(keyword_prompt)
        
        keywords = [kw.strip() for kw in keyword_response.split(',')]
        
        processing_time = (datetime.now() - start_time).total_seconds()
        
        return SummaryResponse(
            summary=summary,
            keywords=keywords[:5],  # ìµœëŒ€ 5ê°œ í‚¤ì›Œë“œ
            confidence_score=0.85,  # ê¸°ë³¸ ì‹ ë¢°ë„ ì ìˆ˜
            processing_time=processing_time
        )
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ìš”ì•½ ìƒì„± ì‹¤íŒ¨: {str(e)}")

async def generate_detailed_analysis_with_gpt4o(content: str, document_type: str = "resume") -> DetailedAnalysisResponse:
    """GPT-4oë¥¼ ì‚¬ìš©í•˜ì—¬ ì´ë ¥ì„œ/ìì†Œì„œ ìƒì„¸ ë¶„ì„ ìƒì„±"""
    if not openai_service:
        raise HTTPException(status_code=500, detail="OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    
    start_time = datetime.now()
    
    try:
        # ë¬¸ì„œ íƒ€ì…ì— ë”°ë¥¸ ìƒì„¸ ë¶„ì„ í”„ë¡¬í”„íŠ¸ ìƒì„±
        if document_type == "resume":
            analysis_prompt = f"""
ë‹¹ì‹ ì€ 15ë…„ ê²½ë ¥ì˜ HR ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë‹¤ìŒ ì´ë ¥ì„œë¥¼ ìƒì„¸íˆ ë¶„ì„í•˜ì—¬ í‰ê°€í•´ì£¼ì„¸ìš”.

[ë¶„ì„í•  ì´ë ¥ì„œ ë‚´ìš©]
{content}

[ë¶„ì„ í•­ëª© ë° í‰ê°€ ê¸°ì¤€]
1. basic_info_completeness (ê¸°ë³¸ì •ë³´ ì™„ì„±ë„): ì—°ë½ì²˜, í•™ë ¥, ê²½ë ¥ ë“± ê¸°ë³¸ ì •ë³´ì˜ ì™„ì„±ë„ (0-10ì )
2. job_relevance (ì§ë¬´ ì í•©ì„±): ì§€ì› ì§ë¬´ì™€ì˜ ì—°ê´€ì„± ë° ì í•©ì„± (0-10ì )
3. experience_clarity (ê²½ë ¥ ëª…í™•ì„±): ê²½ë ¥ ì‚¬í•­ì˜ êµ¬ì²´ì„±ê³¼ ëª…í™•ì„± (0-10ì )
4. tech_stack_clarity (ê¸°ìˆ  ìŠ¤íƒ ëª…í™•ì„±): ê¸°ìˆ  ìŠ¤í‚¬ì˜ êµ¬ì²´ì„±ê³¼ ìˆ˜ì¤€ (0-10ì )
5. project_recency (í”„ë¡œì íŠ¸ ìµœì‹ ì„±): ìµœê·¼ í”„ë¡œì íŠ¸ ê²½í—˜ì˜ ì ì ˆì„± (0-10ì )
6. achievement_metrics (ì„±ê³¼ ì§€í‘œ): êµ¬ì²´ì  ì„±ê³¼ì™€ ìˆ˜ì¹˜í™” ì •ë„ (0-10ì )
7. readability (ê°€ë…ì„±): ë¬¸ì„œ êµ¬ì¡°ì™€ ì½ê¸° ì‰¬ìš´ ì •ë„ (0-10ì )
8. typos_and_errors (ì˜¤ë¥˜ ì •ë„): ë§ì¶¤ë²•, ë¬¸ë²• ì˜¤ë¥˜ì˜ ì •ë„ (0-10ì )
9. update_freshness (ìµœì‹ ì„±): ì •ë³´ì˜ ìµœì‹ ì„±ê³¼ ì—…ë°ì´íŠ¸ ì •ë„ (0-10ì )

[ì‘ë‹µ í˜•ì‹]
ë°˜ë“œì‹œ ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”:
{{
  "resume_analysis": {{
        "basic_info_completeness": {{"score": 8, "feedback": "ì—°ë½ì²˜ì™€ í•™ë ¥ ì •ë³´ê°€ ì™„ë²½í•˜ê²Œ ê¸°ì¬ë¨"}},
        "job_relevance": {{"score": 7, "feedback": "ì§€ì› ì§ë¬´ì™€ ê´€ë ¨ëœ ê²½í—˜ì´ ì ì ˆí•¨"}},
        "experience_clarity": {{"score": 8, "feedback": "ê²½ë ¥ ì‚¬í•­ì´ êµ¬ì²´ì ìœ¼ë¡œ ê¸°ìˆ ë¨"}},
        "tech_stack_clarity": {{"score": 9, "feedback": "ê¸°ìˆ  ìŠ¤í‚¬ì´ ëª…í™•í•˜ê²Œ ì •ë¦¬ë¨"}},
        "project_recency": {{"score": 7, "feedback": "ìµœê·¼ 2ë…„ ë‚´ í”„ë¡œì íŠ¸ ê²½í—˜ ìˆìŒ"}},
        "achievement_metrics": {{"score": 6, "feedback": "ì¼ë¶€ ì„±ê³¼ê°€ ìˆ˜ì¹˜í™”ë˜ì–´ ìˆìŒ"}},
        "readability": {{"score": 8, "feedback": "ë¬¸ì„œ êµ¬ì¡°ê°€ ì²´ê³„ì ì´ê³  ì½ê¸° ì‰¬ì›€"}},
        "typos_and_errors": {{"score": 9, "feedback": "ì˜¤ë¥˜ê°€ ê±°ì˜ ì—†ìŒ"}},
        "update_freshness": {{"score": 8, "feedback": "ìµœì‹  ì •ë³´ë¡œ ì—…ë°ì´íŠ¸ë¨"}}
    }},
    "overall_summary": {{
        "total_score": 7.8,
        "recommendation": "ì „ë°˜ì ìœ¼ë¡œ ìš°ìˆ˜í•œ ì´ë ¥ì„œì´ë‚˜, ì„±ê³¼ ì§€í‘œì˜ êµ¬ì²´í™”ê°€ í•„ìš”í•©ë‹ˆë‹¤."
    }}
}}
"""
        elif document_type == "cover_letter":
            analysis_prompt = f"""
ë‹¹ì‹ ì€ 15ë…„ ê²½ë ¥ì˜ HR ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë‹¤ìŒ ìê¸°ì†Œê°œì„œë¥¼ ìƒì„¸íˆ ë¶„ì„í•˜ì—¬ í‰ê°€í•´ì£¼ì„¸ìš”.

[ë¶„ì„í•  ìê¸°ì†Œê°œì„œ ë‚´ìš©]
{content}

[ë¶„ì„ í•­ëª© ë° í‰ê°€ ê¸°ì¤€]
1. motivation_relevance (ì§€ì›ë™ê¸° ì—°ê´€ì„±): ì§€ì› íšŒì‚¬/ì§ë¬´ì™€ì˜ ì—°ê´€ì„± (0-10ì )
2. problem_solving_STAR (ë¬¸ì œí•´ê²° STAR): êµ¬ì²´ì  ì‚¬ë¡€ì™€ STAR êµ¬ì¡°ì˜ ì™„ì„±ë„ (0-10ì )
3. quantitative_impact (ì •ëŸ‰ì  ì„±ê³¼): ìˆ˜ì¹˜í™”ëœ ì„±ê³¼ì™€ ì„íŒ©íŠ¸ (0-10ì )
4. job_understanding (ì§ë¬´ ì´í•´ë„): ì§€ì› ì§ë¬´ì— ëŒ€í•œ ì´í•´ë„ (0-10ì )
5. unique_experience (ë…íŠ¹í•œ ê²½í—˜): ì°¨ë³„í™”ëœ ê²½í—˜ê³¼ ìŠ¤í† ë¦¬ (0-10ì )
6. logical_flow (ë…¼ë¦¬ì  íë¦„): ë¬¸ë‹¨ ê°„ ë…¼ë¦¬ì  ì—°ê²°ì„± (0-10ì )
7. keyword_diversity (í‚¤ì›Œë“œ ë‹¤ì–‘ì„±): ì§ë¬´ ê´€ë ¨ í‚¤ì›Œë“œì˜ ì ì ˆì„± (0-10ì )
8. sentence_readability (ë¬¸ì¥ ê°€ë…ì„±): ë¬¸ì¥ì˜ ëª…í™•ì„±ê³¼ ì´í•´ë„ (0-10ì )
9. typos_and_errors (ì˜¤ë¥˜ ì •ë„): ë§ì¶¤ë²•, ë¬¸ë²• ì˜¤ë¥˜ì˜ ì •ë„ (0-10ì )

[ì‘ë‹µ í˜•ì‹]
ë°˜ë“œì‹œ ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”:
{{
  "cover_letter_analysis": {{
        "motivation_relevance": {{"score": 8, "feedback": "ì§€ì› ë™ê¸°ê°€ ëª…í™•í•˜ê³  ì„¤ë“ë ¥ ìˆìŒ"}},
        "problem_solving_STAR": {{"score": 7, "feedback": "STAR êµ¬ì¡°ê°€ ì ì ˆí•˜ê²Œ êµ¬ì„±ë¨"}},
        "quantitative_impact": {{"score": 6, "feedback": "ì¼ë¶€ ì„±ê³¼ê°€ ìˆ˜ì¹˜í™”ë˜ì–´ ìˆìŒ"}},
        "job_understanding": {{"score": 8, "feedback": "ì§ë¬´ì— ëŒ€í•œ ì´í•´ë„ê°€ ë†’ìŒ"}},
        "unique_experience": {{"score": 7, "feedback": "ì°¨ë³„í™”ëœ ê²½í—˜ì´ ì˜ ë“œëŸ¬ë‚¨"}},
        "logical_flow": {{"score": 8, "feedback": "ë…¼ë¦¬ì  íë¦„ì´ ìì—°ìŠ¤ëŸ¬ì›€"}},
        "keyword_diversity": {{"score": 7, "feedback": "ì§ë¬´ ê´€ë ¨ í‚¤ì›Œë“œê°€ ì ì ˆí•¨"}},
        "sentence_readability": {{"score": 8, "feedback": "ë¬¸ì¥ì´ ëª…í™•í•˜ê³  ì´í•´í•˜ê¸° ì‰¬ì›€"}},
        "typos_and_errors": {{"score": 9, "feedback": "ì˜¤ë¥˜ê°€ ê±°ì˜ ì—†ìŒ"}}
  }},
  "overall_summary": {{
        "total_score": 7.5,
        "recommendation": "ì „ë°˜ì ìœ¼ë¡œ ìš°ìˆ˜í•œ ìê¸°ì†Œê°œì„œì´ë‚˜, ì •ëŸ‰ì  ì„±ê³¼ í‘œí˜„ì„ ê°•í™”í•˜ë©´ ë”ìš± ì¢‹ê² ìŠµë‹ˆë‹¤."
    }}
}}
"""
        else:
            analysis_prompt = f"""
ë‹¹ì‹ ì€ 15ë…„ ê²½ë ¥ì˜ HR ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë‹¤ìŒ ë¬¸ì„œë¥¼ ìƒì„¸íˆ ë¶„ì„í•˜ì—¬ í‰ê°€í•´ì£¼ì„¸ìš”.

[ë¶„ì„í•  ë¬¸ì„œ ë‚´ìš©]
{content}

[ë¶„ì„ í•­ëª© ë° í‰ê°€ ê¸°ì¤€]
1. content_completeness (ë‚´ìš© ì™„ì„±ë„): ë¬¸ì„œ ë‚´ìš©ì˜ ì™„ì„±ë„ì™€ ì¶©ì‹¤ì„± (0-10ì )
2. clarity (ëª…í™•ì„±): ë‚´ìš©ì˜ ëª…í™•ì„±ê³¼ ì´í•´ë„ (0-10ì )
3. relevance (ê´€ë ¨ì„±): ì£¼ì œì™€ì˜ ê´€ë ¨ì„± (0-10ì )
4. quality (í’ˆì§ˆ): ì „ë°˜ì ì¸ ë¬¸ì„œ í’ˆì§ˆ (0-10ì )

[ì‘ë‹µ í˜•ì‹]
ë°˜ë“œì‹œ ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”:
{{
  "overall_summary": {{
        "total_score": 7.5,
        "recommendation": "ì „ë°˜ì ìœ¼ë¡œ ìš°ìˆ˜í•œ ë¬¸ì„œì…ë‹ˆë‹¤."
    }}
}}
"""

        print(f"ğŸš€ GPT-4o ìƒì„¸ ë¶„ì„ ì‹œì‘...")
        
        # GPT-4o API í˜¸ì¶œ
        response = await openai_service.generate_response(analysis_prompt)
        
        print(f"âœ… GPT-4o ì‘ë‹µ ìˆ˜ì‹  ì™„ë£Œ")
        
        # JSON íŒŒì‹± ì‹œë„
        try:
            # JSON ì‘ë‹µì—ì„œ ì¶”ì¶œ
            json_match = re.search(r'\{.*\}', response, re.DOTALL)
            if json_match:
                json_str = json_match.group()
                result = json.loads(json_str)
                
                # ì „ì²´ ì ìˆ˜ ê³„ì‚°
                if document_type == "resume" and "resume_analysis" in result:
                    scores = []
                    for field, data in result["resume_analysis"].items():
                        if isinstance(data, dict) and "score" in data:
                            scores.append(data["score"])
                    if scores:
                        result["overall_summary"]["total_score"] = sum(scores) / len(scores)
                
                elif document_type == "cover_letter" and "cover_letter_analysis" in result:
                    scores = []
                    for field, data in result["cover_letter_analysis"].items():
                        if isinstance(data, dict) and "score" in data:
                            scores.append(data["score"])
                    if scores:
                        result["overall_summary"]["total_score"] = sum(scores) / len(scores)
                
                end_time = datetime.now()
                processing_time = (end_time - start_time).total_seconds()
                print(f"âš¡ ìƒì„¸ ë¶„ì„ ì™„ë£Œ: {processing_time:.2f}ì´ˆ")
                
                return DetailedAnalysisResponse(**result)
                
        except Exception as parse_error:
            print(f"âš ï¸ JSON íŒŒì‹± ì‹¤íŒ¨, ê¸°ë³¸ ì‘ë‹µ ìƒì„±: {parse_error}")
        
        # JSON íŒŒì‹± ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ ì‘ë‹µ ìƒì„±
        end_time = datetime.now()
        processing_time = (end_time - start_time).total_seconds()
        print(f"âš¡ ê¸°ë³¸ ë¶„ì„ ì™„ë£Œ: {processing_time:.2f}ì´ˆ")
        
        if document_type == "resume":
            return DetailedAnalysisResponse(
                resume_analysis=ResumeAnalysis(
                    basic_info_completeness=AnalysisScore(score=7, feedback="ê¸°ë³¸ ë¶„ì„ ì™„ë£Œ"),
                    job_relevance=AnalysisScore(score=7, feedback="ê¸°ë³¸ ë¶„ì„ ì™„ë£Œ"),
                    experience_clarity=AnalysisScore(score=7, feedback="ê¸°ë³¸ ë¶„ì„ ì™„ë£Œ"),
                    tech_stack_clarity=AnalysisScore(score=7, feedback="ê¸°ë³¸ ë¶„ì„ ì™„ë£Œ"),
                    project_recency=AnalysisScore(score=7, feedback="ê¸°ë³¸ ë¶„ì„ ì™„ë£Œ"),
                    achievement_metrics=AnalysisScore(score=7, feedback="ê¸°ë³¸ ë¶„ì„ ì™„ë£Œ"),
                    readability=AnalysisScore(score=7, feedback="ê¸°ë³¸ ë¶„ì„ ì™„ë£Œ"),
                    typos_and_errors=AnalysisScore(score=7, feedback="ê¸°ë³¸ ë¶„ì„ ì™„ë£Œ"),
                    update_freshness=AnalysisScore(score=7, feedback="ê¸°ë³¸ ë¶„ì„ ì™„ë£Œ")
                ),
                overall_summary=OverallSummary(total_score=7.0, recommendation="GPT-4o ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
            )
        elif document_type == "cover_letter":
            return DetailedAnalysisResponse(
                cover_letter_analysis=CoverLetterAnalysis(
                    motivation_relevance=AnalysisScore(score=7, feedback="ê¸°ë³¸ ë¶„ì„ ì™„ë£Œ"),
                    problem_solving_STAR=AnalysisScore(score=7, feedback="ê¸°ë³¸ ë¶„ì„ ì™„ë£Œ"),
                    quantitative_impact=AnalysisScore(score=7, feedback="ê¸°ë³¸ ë¶„ì„ ì™„ë£Œ"),
                    job_understanding=AnalysisScore(score=7, feedback="ê¸°ë³¸ ë¶„ì„ ì™„ë£Œ"),
                    unique_experience=AnalysisScore(score=7, feedback="ê¸°ë³¸ ë¶„ì„ ì™„ë£Œ"),
                    logical_flow=AnalysisScore(score=7, feedback="ê¸°ë³¸ ë¶„ì„ ì™„ë£Œ"),
                    keyword_diversity=AnalysisScore(score=7, feedback="ê¸°ë³¸ ë¶„ì„ ì™„ë£Œ"),
                    sentence_readability=AnalysisScore(score=7, feedback="ê¸°ë³¸ ë¶„ì„ ì™„ë£Œ"),
                    typos_and_errors=AnalysisScore(score=7, feedback="ê¸°ë³¸ ë¶„ì„ ì™„ë£Œ")
                ),
                overall_summary=OverallSummary(total_score=7.0, recommendation="GPT-4o ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
            )
        else:
            return DetailedAnalysisResponse(
                overall_summary=OverallSummary(total_score=7.0, recommendation="GPT-4o ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
            )
        
    except Exception as e:
        end_time = datetime.now()
        processing_time = (end_time - start_time).total_seconds()
        print(f"âŒ ìƒì„¸ ë¶„ì„ ì‹¤íŒ¨ ({processing_time:.2f}ì´ˆ): {e}")
        raise HTTPException(status_code=500, detail=f"ìƒì„¸ ë¶„ì„ ì‹¤íŒ¨: {str(e)}")

@router.post("/file")
async def upload_and_summarize_file(
    file: UploadFile = File(...),
    summary_type: str = Form("general")
):
    """íŒŒì¼ ì—…ë¡œë“œ ë° ìš”ì•½"""
    try:
        # íŒŒì¼ ìœ íš¨ì„± ê²€ì‚¬
        if not validate_file(file):
            raise HTTPException(
                status_code=400, 
                detail="ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹ì…ë‹ˆë‹¤. PDF, DOC, DOCX, TXT íŒŒì¼ë§Œ ì—…ë¡œë“œ ê°€ëŠ¥í•©ë‹ˆë‹¤."
            )
        
        # íŒŒì¼ í¬ê¸° í™•ì¸
        file_size = 0
        content = await file.read()
        file_size = len(content)
        
        if file_size > MAX_FILE_SIZE:
            raise HTTPException(
                status_code=400,
                detail="íŒŒì¼ í¬ê¸°ê°€ ë„ˆë¬´ í½ë‹ˆë‹¤. ìµœëŒ€ 50MBê¹Œì§€ ì—…ë¡œë“œ ê°€ëŠ¥í•©ë‹ˆë‹¤."
            )
        
        # ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥
        file_ext = os.path.splitext(file.filename.lower())[1]
        with tempfile.NamedTemporaryFile(delete=False, suffix=file_ext) as temp_file:
            temp_file.write(content)
            temp_file_path = temp_file.name
        
        try:
            # íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
            extracted_text = await extract_text_from_file(temp_file_path, file_ext)
            
            # í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨ ì‹œì—ë„ ë”ë¯¸ ë¶„ì„ìœ¼ë¡œ ê³„ì† ì§„í–‰ (ì‚¬ìš©ì ê²½í—˜ ê°œì„ )
            if not extracted_text or str(extracted_text).strip() == "":
                print("âš ï¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨: ë¹ˆ ë‚´ìš© ê°ì§€ â†’ ë”ë¯¸ ë¶„ì„ìœ¼ë¡œ ê³„ì† ì§„í–‰í•©ë‹ˆë‹¤.")
                extracted_text = "[EMPTY_CONTENT] í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨ (ìŠ¤ìº” PDF/ì´ë¯¸ì§€ ê¸°ë°˜ ë¬¸ì„œì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.)"
            
            # OpenAI APIë¡œ ìš”ì•½ ìƒì„±
            summary_result = await generate_summary_with_openai(extracted_text, summary_type)
            
            return {
                "filename": file.filename,
                "file_size": file_size,
                "extracted_text_length": len(extracted_text),
                "summary": summary_result.summary,
                "keywords": summary_result.keywords,
                "confidence_score": summary_result.confidence_score,
                "processing_time": summary_result.processing_time,
                "summary_type": summary_type
            }
            
        finally:
            # ì„ì‹œ íŒŒì¼ ì‚­ì œ
            if os.path.exists(temp_file_path):
                os.unlink(temp_file_path)
                
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"íŒŒì¼ ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")

@router.post("/summarize")
async def summarize_text(request: SummaryRequest):
    """í…ìŠ¤íŠ¸ ì§ì ‘ ìš”ì•½"""
    try:
        if not request.content or len(request.content.strip()) == 0:
            raise HTTPException(status_code=400, detail="ìš”ì•½í•  í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.")
        
        summary_result = await generate_summary_with_openai(
            request.content, 
            request.summary_type
        )
        
        return summary_result
        
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ìš”ì•½ ìƒì„± ì‹¤íŒ¨: {str(e)}")

@router.get("/health")
async def upload_health_check():
    """ì—…ë¡œë“œ ì„œë¹„ìŠ¤ í—¬ìŠ¤ ì²´í¬"""
    return {
        "status": "healthy",
        "openai_api_configured": bool(openai_service),
        "supported_formats": list(ALLOWED_EXTENSIONS.keys()),
        "max_file_size_mb": MAX_FILE_SIZE // (1024 * 1024)
    }

@router.post("/analyze")
async def analyze_document(
    file: UploadFile = File(...),
    analysis_type: str = Form("resume")  # resume, cover_letter, portfolio
):
    """ë¬¸ì„œ ìƒì„¸ ë¶„ì„ (GPT-4o ì‚¬ìš©)"""
    try:
        # íŒŒì¼ ìœ íš¨ì„± ê²€ì‚¬
        if not validate_file(file):
            raise HTTPException(
                status_code=400, 
                detail="ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹ì…ë‹ˆë‹¤. PDF, DOC, DOCX, TXT íŒŒì¼ë§Œ ì—…ë¡œë“œ ê°€ëŠ¥í•©ë‹ˆë‹¤."
            )
        
        # íŒŒì¼ í¬ê¸° í™•ì¸
        file_size = 0
        content = await file.read()
        file_size = len(content)
        
        if file_size > MAX_FILE_SIZE:
            raise HTTPException(
                status_code=400,
                detail="íŒŒì¼ í¬ê¸°ê°€ ë„ˆë¬´ í½ë‹ˆë‹¤. ìµœëŒ€ 50MBê¹Œì§€ ì—…ë¡œë“œ ê°€ëŠ¥í•©ë‹ˆë‹¤."
            )
        
        # ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥
        file_ext = os.path.splitext(file.filename.lower())[1]
        with tempfile.NamedTemporaryFile(delete=False, suffix=file_ext) as temp_file:
            temp_file.write(content)
            temp_file_path = temp_file.name
        
        try:
            # íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
            extracted_text = await extract_text_from_file(temp_file_path, file_ext)
            
            # í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨ ì‹œì—ë„ ë”ë¯¸ ë¶„ì„ìœ¼ë¡œ ê³„ì† ì§„í–‰
            if not extracted_text or str(extracted_text).strip() == "":
                print("âš ï¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨: ë¹ˆ ë‚´ìš© ê°ì§€ â†’ ë”ë¯¸ ë¶„ì„ìœ¼ë¡œ ê³„ì† ì§„í–‰í•©ë‹ˆë‹¤.")
                extracted_text = "[EMPTY_CONTENT] í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨ (ìŠ¤ìº” PDF/ì´ë¯¸ì§€ ê¸°ë°˜ ë¬¸ì„œì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.)"
            
            # GPT-4oë¡œ ìƒì„¸ ë¶„ì„ ìƒì„±
            analysis_result = await generate_detailed_analysis_with_gpt4o(extracted_text, analysis_type)
            
            return {
                "filename": file.filename,
                "file_size": file_size,
                "extracted_text_length": len(extracted_text),
                "analysis_type": analysis_type,
                "analysis_result": analysis_result.dict()
            }
            
        finally:
            # ì„ì‹œ íŒŒì¼ ì‚­ì œ
            if os.path.exists(temp_file_path):
                os.unlink(temp_file_path)
                
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ë¬¸ì„œ ë¶„ì„ ì‹¤íŒ¨: {str(e)}")

@router.post("/analyze-text")
async def analyze_text_content(
    request: SummaryRequest,
    analysis_type: str = Form("resume")
):
    """í…ìŠ¤íŠ¸ ë‚´ìš© ìƒì„¸ ë¶„ì„ (GPT-4o ì‚¬ìš©)"""
    try:
        if not request.content or len(request.content.strip()) == 0:
            raise HTTPException(status_code=400, detail="ë¶„ì„í•  í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.")
        
        # GPT-4oë¡œ ìƒì„¸ ë¶„ì„ ìƒì„±
        analysis_result = await generate_detailed_analysis_with_gpt4o(
            request.content, 
            analysis_type
        )
        
        return {
            "content_length": len(request.content),
            "analysis_type": analysis_type,
            "analysis_result": analysis_result.dict()
        }
        
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"í…ìŠ¤íŠ¸ ë¶„ì„ ì‹¤íŒ¨: {str(e)}")