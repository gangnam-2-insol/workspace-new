from fastapi import APIRouter, HTTPException, UploadFile, File, Form
from fastapi.responses import JSONResponse
from typing import Optional, Dict, List
import os
from dotenv import load_dotenv
import tempfile
import asyncio
import aiofiles
from datetime import datetime
import google.generativeai as genai
from pydantic import BaseModel
import re

# .env ÌååÏùº Î°úÎìú (ÌòÑÏû¨ ÎîîÎ†âÌÜ†Î¶¨ÏóêÏÑú)
print(f"üîç upload.py ÌòÑÏû¨ ÏûëÏóÖ ÎîîÎ†âÌÜ†Î¶¨: {os.getcwd()}")
print(f"üîç upload.py .env ÌååÏùº Ï°¥Ïû¨ Ïó¨Î∂Ä: {os.path.exists('.env')}")
load_dotenv('.env')
print(f"üîç upload.py GOOGLE_API_KEY Î°úÎìú ÌõÑ: {os.getenv('GOOGLE_API_KEY')}")

# Gemini API ÏÑ§Ï†ï
GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY")
if GOOGLE_API_KEY:
    genai.configure(api_key=GOOGLE_API_KEY)
    model = genai.GenerativeModel('gemini-1.5-flash')

router = APIRouter(tags=["upload"])

class SummaryRequest(BaseModel):
    content: str
    summary_type: str = "general"  # general, technical, experience

class SummaryResponse(BaseModel):
    summary: str
    keywords: list[str]
    confidence_score: float
    processing_time: float

# ÏÉàÎ°úÏö¥ ÏÉÅÏÑ∏ Î∂ÑÏÑù Î™®Îç∏Îì§
class AnalysisScore(BaseModel):
    score: int  # 0-10
    feedback: str

class DocumentValidationRequest(BaseModel):
    content: str
    expected_type: str  # "Ïù¥Î†•ÏÑú", "ÏûêÍ∏∞ÏÜåÍ∞úÏÑú", "Ìè¨Ìä∏Ìè¥Î¶¨Ïò§"

class DocumentValidationResponse(BaseModel):
    is_valid: bool
    confidence: float
    reason: str
    suggested_type: str

class ResumeAnalysis(BaseModel):
    basic_info_completeness: AnalysisScore
    job_relevance: AnalysisScore
    experience_clarity: AnalysisScore
    tech_stack_clarity: AnalysisScore
    project_recency: AnalysisScore
    achievement_metrics: AnalysisScore
    readability: AnalysisScore
    typos_and_errors: AnalysisScore
    update_freshness: AnalysisScore

class CoverLetterAnalysis(BaseModel):
    motivation_relevance: AnalysisScore
    problem_solving_STAR: AnalysisScore
    quantitative_impact: AnalysisScore
    job_understanding: AnalysisScore
    unique_experience: AnalysisScore
    logical_flow: AnalysisScore
    keyword_diversity: AnalysisScore
    sentence_readability: AnalysisScore
    typos_and_errors: AnalysisScore

class PortfolioAnalysis(BaseModel):
    project_overview: AnalysisScore
    tech_stack: AnalysisScore
    personal_contribution: AnalysisScore
    achievement_metrics: AnalysisScore
    visual_quality: AnalysisScore
    documentation_quality: AnalysisScore
    job_relevance: AnalysisScore
    unique_features: AnalysisScore
    maintainability: AnalysisScore

class OverallSummary(BaseModel):
    total_score: float
    recommendation: str

class DetailedAnalysisResponse(BaseModel):
    resume_analysis: Optional[ResumeAnalysis] = None
    cover_letter_analysis: Optional[CoverLetterAnalysis] = None
    portfolio_analysis: Optional[PortfolioAnalysis] = None
    overall_summary: OverallSummary

# ===== Î∂ÑÏÑù Ïã§Ìå® Ïãú Í∏∞Î≥∏ Íµ¨Ï°∞ ÏÉùÏÑ± Ïú†Ìã∏ =====
def _build_score(msg: str) -> Dict[str, object]:
    return {"score": 0, "feedback": msg}

def build_fallback_analysis(document_type: str) -> Dict[str, object]:
    reason = "Î¨∏ÏÑúÏóêÏÑú ÌÖçÏä§Ìä∏Î•º Ï∂îÏ∂úÌï† Ïàò ÏóÜÏñ¥ ÌèâÍ∞ÄÌïòÏßÄ Î™ªÌñàÏäµÎãàÎã§. Ìé∏Ïßë Í∞ÄÎä•Ìïú PDF/DOCXÎ°ú Ïû¨ÏóÖÎ°úÎìúÌï¥Ï£ºÏÑ∏Ïöî."
    resume = {
        "basic_info_completeness": _build_score(reason),
        "job_relevance": _build_score(reason),
        "experience_clarity": _build_score(reason),
        "tech_stack_clarity": _build_score(reason),
        "project_recency": _build_score(reason),
        "achievement_metrics": _build_score(reason),
        "readability": _build_score(reason),
        "typos_and_errors": _build_score(reason),
        "update_freshness": _build_score(reason),
    }
    cover = {
        "motivation_relevance": _build_score(reason),
        "problem_solving_STAR": _build_score(reason),
        "quantitative_impact": _build_score(reason),
        "job_understanding": _build_score(reason),
        "unique_experience": _build_score(reason),
        "logical_flow": _build_score(reason),
        "keyword_diversity": _build_score(reason),
        "sentence_readability": _build_score(reason),
        "typos_and_errors": _build_score(reason),
    }
    portfolio = {
        "project_overview": _build_score(reason),
        "tech_stack": _build_score(reason),
        "personal_contribution": _build_score(reason),
        "achievement_metrics": _build_score(reason),
        "visual_quality": _build_score(reason),
        "documentation_quality": _build_score(reason),
        "job_relevance": _build_score(reason),
        "unique_features": _build_score(reason),
        "maintainability": _build_score(reason),
    }
    return {
        "resume_analysis": resume,
        "cover_letter_analysis": cover,
        "portfolio_analysis": portfolio,
        "overall_summary": {"total_score": 0, "recommendation": reason},
    }

# ===== ÎÇ¥Ïö© Í∏∞Î∞ò Î¨∏ÏÑú Ïú†Ìòï Î∂ÑÎ•òÍ∏∞ =====
def classify_document_type_by_content(text: str) -> Dict[str, object]:
    """Í∞ÑÎã®Ìïú Í∑úÏπô Í∏∞Î∞òÏúºÎ°ú Î¨∏ÏÑú Ïú†Ìòï(resume/cover_letter/portfolio)ÏùÑ Î∂ÑÎ•òÌï©ÎãàÎã§."""
    text_lower = text.lower()

    # ÌïúÍµ≠Ïñ¥/ÏòÅÏñ¥ ÌÇ§ÏõåÎìú ÏÑ∏Ìä∏
    resume_keywords = [
        "Í≤ΩÎ†•", "Ïù¥Î†•", "ÌîÑÎ°úÏ†ùÌä∏", "ÌïôÎ†•", "Í∏∞Ïà†", "Ïä§ÌÇ¨", "ÏûêÍ≤©Ï¶ù", "Í∑ºÎ¨¥", "Îã¥Îãπ", "ÏÑ±Í≥º",
        "Í≤ΩÌóò", "ÏöîÏïΩ", "ÌïµÏã¨Ïó≠Îüâ", "phone", "email", "github", "linkedin",
        "experience", "education", "skills", "projects", "certificate"
    ]
    cover_letter_keywords = [
        "ÏßÄÏõêÎèôÍ∏∞", "ÏÑ±Ïû•Î∞∞Í≤Ω", "ÏûÖÏÇ¨", "Ìè¨Î∂Ä", "Ï†ÄÎäî", "Î∞∞Ïö∞Î©∞", "ÌïòÍ≥†Ïûê", "Í∏∞Ïó¨", "Í¥ÄÏã¨",
        "ÎèôÍ∏∞", "Ïó¥Ï†ï", "Ïôú", "Ïôú Ïö∞Î¶¨", "motiv", "cover letter", "passion"
    ]
    portfolio_keywords = [
        "Ìè¨Ìä∏Ìè¥Î¶¨Ïò§", "ÏûëÌíà", "ÏãúÏó∞", "Îç∞Î™®", "ÎßÅÌÅ¨", "Ïù¥ÎØ∏ÏßÄ", "Ïä§ÏÉ∑", "Ï∫°Ï≤ò", "Î†àÌè¨ÏßÄÌÜ†Î¶¨",
        "repository", "demo", "screenshot", "figma", "behance", "dribbble"
    ]

    def score_keywords(keywords: List[str]) -> float:
        score = 0.0
        for kw in keywords:
            # Îã®Ïñ¥ Í≤ΩÍ≥Ñ Ïö∞ÏÑ†, ÏóÜÏúºÎ©¥ Ìè¨Ìï® Í≤ÄÏÇ¨
            if re.search(rf"\b{re.escape(kw)}\b", text_lower) or kw in text_lower:
                score += 1.0
        # ÏÑπÏÖò Ìó§Îçî Î≥¥ÎÑàÏä§
        section_headers = ["Í≤ΩÎ†•", "ÌïôÎ†•", "ÌîÑÎ°úÏ†ùÌä∏", "skills", "experience", "education"]
        if any(h in text for h in section_headers):
            score += 0.5
        # Ïó∞ÎùΩÏ≤ò Ìå®ÌÑ¥ Î≥¥ÎÑàÏä§ (Ïù¥Î†•ÏÑú ÏßÄÌëú)
        if re.search(r"[0-9]{2,3}-[0-9]{3,4}-[0-9]{4}", text) or re.search(r"\b[a-z0-9._%+-]+@[a-z0-9.-]+\.[a-z]{2,}\b", text_lower):
            score += 0.7
        return score

    resume_score = score_keywords(resume_keywords)
    cover_letter_score = sum(1.0 for kw in cover_letter_keywords if kw in text_lower)
    portfolio_score = sum(1.0 for kw in portfolio_keywords if kw in text_lower)

    scores = {
        "resume": resume_score,
        "cover_letter": cover_letter_score,
        "portfolio": portfolio_score,
    }

    detected_type = max(scores.items(), key=lambda x: x[1])[0]
    max_score = scores[detected_type]
    # Í∞ÑÎã®Ìïú Ïã†Î¢∞ÎèÑ Ï†ïÍ∑úÌôî (ÏµúÎåÄ 10Ï†ê Í∞ÄÏ†ï)
    confidence = min(round(max_score / 10.0, 2), 1.0)

    return {"detected_type": detected_type, "confidence": confidence, "scores": scores}

# ÌóàÏö©Îêú ÌååÏùº ÌÉÄÏûÖ
ALLOWED_EXTENSIONS = {
    '.pdf': 'application/pdf',
    '.doc': 'application/msword',
    '.docx': 'application/vnd.openxmlformats-officedocument.wordprocessingml.document',
    '.txt': 'text/plain'
}

# ÌååÏùº ÌÅ¨Í∏∞ Ï†úÌïú (10MB)
MAX_FILE_SIZE = 10 * 1024 * 1024

def validate_file(file: UploadFile) -> bool:
    """ÌååÏùº Ïú†Ìö®ÏÑ± Í≤ÄÏÇ¨"""
    if not file.filename:
        return False
    
    # ÌååÏùº ÌôïÏû•Ïûê ÌôïÏù∏
    file_ext = os.path.splitext(file.filename.lower())[1]
    if file_ext not in ALLOWED_EXTENSIONS:
        return False
    
    return True

async def extract_text_from_file(file_path: str, file_ext: str) -> str:
    """ÌååÏùºÏóêÏÑú ÌÖçÏä§Ìä∏ Ï∂îÏ∂ú (Îã§Ï§ë Î∞±ÏóÖ Ï†ÑÎûµ)"""
    try:
        if file_ext == '.txt':
            async with aiofiles.open(file_path, 'r', encoding='utf-8') as f:
                return await f.read()
        elif file_ext == '.pdf':
            # 1Ï∞®: PyPDF2
            try:
                import PyPDF2
                text = ""
                with open(file_path, 'rb') as pdf_file:
                    pdf_reader = PyPDF2.PdfReader(pdf_file)
                    for page in pdf_reader.pages:
                        extracted = page.extract_text() or ""
                        text += extracted + ("\n" if extracted else "")
                if text.strip():
                    return text
            except Exception:
                pass
            # 2Ï∞®: pdfplumber
            try:
                import pdfplumber  # type: ignore
                text = ""
                with pdfplumber.open(file_path) as pdf:
                    for p in pdf.pages:
                        extracted = p.extract_text() or ""
                        text += extracted + ("\n" if extracted else "")
                if text.strip():
                    return text
            except Exception:
                pass
            # Ïã§Ìå® Ïãú Îπà Î¨∏ÏûêÏó¥ Î∞òÌôò
            return ""
        elif file_ext in ['.doc', '.docx']:
            # 1Ï∞®: python-docx
            try:
                from docx import Document  # type: ignore
                doc = Document(file_path)
                text = "\n".join([p.text for p in doc.paragraphs if p.text])
                if text.strip():
                    return text
            except Exception:
                pass
            # 2Ï∞®: docx2txt
            try:
                import docx2txt  # type: ignore
                text = docx2txt.process(file_path) or ""
                return text
            except Exception:
                pass
            return ""
        else:
            return ""
    except Exception as e:
        return ""

async def generate_summary_with_gemini(content: str, summary_type: str = "general") -> SummaryResponse:
    """Gemini APIÎ•º ÏÇ¨Ïö©ÌïòÏó¨ ÏöîÏïΩ ÏÉùÏÑ±"""
    if not GOOGLE_API_KEY:
        raise HTTPException(status_code=500, detail="Gemini API ÌÇ§Í∞Ä ÏÑ§Ï†ïÎêòÏßÄ ÏïäÏïòÏäµÎãàÎã§.")
    
    start_time = datetime.now()
    
    try:
        # ÏöîÏïΩ ÌÉÄÏûÖÏóê Îî∞Î•∏ ÌîÑÎ°¨ÌîÑÌä∏ ÏÑ§Ï†ï
        prompts = {
            "general": f"""
            Îã§Ïùå Ïù¥Î†•ÏÑú/ÏûêÍ∏∞ÏÜåÍ∞úÏÑú ÎÇ¥Ïö©ÏùÑ Í∞ÑÍ≤∞ÌïòÍ≤å ÏöîÏïΩÌï¥Ï£ºÏÑ∏Ïöî:
            
            {content}
            
            ÏöîÏïΩ Ïãú Îã§Ïùå ÏÇ¨Ìï≠ÏùÑ Ìè¨Ìï®Ìï¥Ï£ºÏÑ∏Ïöî:
            1. Ï£ºÏöî Í≤ΩÎ†• Î∞è Í≤ΩÌóò
            2. ÌïµÏã¨ Í∏∞Ïà† Ïä§ÌÉù
            3. Ï£ºÏöî ÏÑ±Í≥ºÎÇò ÌîÑÎ°úÏ†ùÌä∏
            4. ÏßÄÏõê ÏßÅÎ¨¥ÏôÄÏùò Ïó∞Í¥ÄÏÑ±
            
            ÏöîÏïΩÏùÄ 200Ïûê Ïù¥ÎÇ¥Î°ú ÏûëÏÑ±Ìï¥Ï£ºÏÑ∏Ïöî.
            """,
            "technical": f"""
            Îã§Ïùå ÎÇ¥Ïö©ÏóêÏÑú Í∏∞Ïà†Ï†Å Ïó≠ÎüâÏùÑ Ï§ëÏã¨ÏúºÎ°ú ÏöîÏïΩÌï¥Ï£ºÏÑ∏Ïöî:
            
            {content}
            
            Îã§Ïùå Ìï≠Î™©Îì§ÏùÑ Ìè¨Ìï®Ìï¥Ï£ºÏÑ∏Ïöî:
            1. ÌîÑÎ°úÍ∑∏ÎûòÎ∞ç Ïñ∏Ïñ¥ Î∞è ÌîÑÎ†àÏûÑÏõåÌÅ¨
            2. Í∞úÎ∞ú ÎèÑÍµ¨ Î∞è ÌîåÎû´Ìèº
            3. ÌîÑÎ°úÏ†ùÌä∏ Í≤ΩÌóò
            4. Í∏∞Ïà†Ï†Å ÏÑ±Í≥º
            
            ÏöîÏïΩÏùÄ 150Ïûê Ïù¥ÎÇ¥Î°ú ÏûëÏÑ±Ìï¥Ï£ºÏÑ∏Ïöî.
            """,
            "experience": f"""
            Îã§Ïùå ÎÇ¥Ïö©ÏóêÏÑú Í≤ΩÎ†•Í≥º Í≤ΩÌóòÏùÑ Ï§ëÏã¨ÏúºÎ°ú ÏöîÏïΩÌï¥Ï£ºÏÑ∏Ïöî:
            
            {content}
            
            Îã§Ïùå Ìï≠Î™©Îì§ÏùÑ Ìè¨Ìï®Ìï¥Ï£ºÏÑ∏Ïöî:
            1. Ï¥ù Í≤ΩÎ†• Í∏∞Í∞Ñ
            2. Ï£ºÏöî ÌöåÏÇ¨ Î∞è ÏßÅÎ¨¥
            3. ÌïµÏã¨ ÌîÑÎ°úÏ†ùÌä∏ Í≤ΩÌóò
            4. Ï£ºÏöî ÏÑ±Í≥º Î∞è ÏóÖÏ†Å
            
            ÏöîÏïΩÏùÄ 150Ïûê Ïù¥ÎÇ¥Î°ú ÏûëÏÑ±Ìï¥Ï£ºÏÑ∏Ïöî.
            """
        }
        
        prompt = prompts.get(summary_type, prompts["general"])
        
        # Gemini API Ìò∏Ï∂ú
        response = await asyncio.to_thread(
            model.generate_content,
            prompt
        )
        
        summary = response.text.strip()
        
        # ÌÇ§ÏõåÎìú Ï∂îÏ∂úÏùÑ ÏúÑÌïú Ï∂îÍ∞Ä ÏöîÏ≤≠
        keyword_prompt = f"""
        Îã§Ïùå ÏöîÏïΩÏóêÏÑú ÌïµÏã¨ ÌÇ§ÏõåÎìú 5Í∞úÎ•º Ï∂îÏ∂úÌï¥Ï£ºÏÑ∏Ïöî:
        
        {summary}
        
        ÌÇ§ÏõåÎìúÎäî ÏâºÌëúÎ°ú Íµ¨Î∂ÑÌïòÏó¨ ÎÇòÏó¥Ìï¥Ï£ºÏÑ∏Ïöî.
        """
        
        keyword_response = await asyncio.to_thread(
            model.generate_content,
            keyword_prompt
        )
        
        keywords = [kw.strip() for kw in keyword_response.text.split(',')]
        
        processing_time = (datetime.now() - start_time).total_seconds()
        
        return SummaryResponse(
            summary=summary,
            keywords=keywords[:5],  # ÏµúÎåÄ 5Í∞ú ÌÇ§ÏõåÎìú
            confidence_score=0.85,  # Í∏∞Î≥∏ Ïã†Î¢∞ÎèÑ Ï†êÏàò
            processing_time=processing_time
        )
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ÏöîÏïΩ ÏÉùÏÑ± Ïã§Ìå®: {str(e)}")

async def generate_detailed_analysis_with_gemini(content: str, document_type: str = "resume") -> DetailedAnalysisResponse:
    """Gemini APIÎ•º ÏÇ¨Ïö©ÌïòÏó¨ ÏÉÅÏÑ∏ Î∂ÑÏÑù ÏÉùÏÑ±"""
    if not GOOGLE_API_KEY:
        raise HTTPException(status_code=500, detail="Gemini API ÌÇ§Í∞Ä ÏÑ§Ï†ïÎêòÏßÄ ÏïäÏïòÏäµÎãàÎã§.")
    
    start_time = datetime.now()
    
    try:
        # Î¨∏ÏÑú ÌÉÄÏûÖÏóê Îî∞Î•∏ ÎßûÏ∂§Ìòï ÌîÑÎ°¨ÌîÑÌä∏ ÏÉùÏÑ±
        # ÌÜµÌï© Î∂ÑÏÑù ÌîÑÎ°¨ÌîÑÌä∏ - ÏßÅÎ¨¥Î≥Ñ ÎßûÏ∂§Ìòï Î∂ÑÏÑù Î∞è Ï†ÅÌï©Î•† Í≥ÑÏÇ∞
        analysis_prompt = f"""
[ROLE] ÎãπÏã†ÏùÄ ÏßÅÎ¨¥Î≥Ñ Ï±ÑÏö© Ï†ÑÎ¨∏Í∞ÄÏûÖÎãàÎã§. ÏóÖÎ°úÎìúÎêú Ïù¥Î†•ÏÑúÏôÄ ÏûêÍ∏∞ÏÜåÍ∞úÏÑúÎ•º ÏßÅÎ¨¥Î≥Ñ Ï§ëÏöîÏÇ¨Ìï≠Í≥º ÌïÑÏàò Ïó≠ÎüâÏóê ÎßûÏ∂∞ ÌèâÍ∞ÄÌïòÍ≥†, Ï†ÅÌï©Î•†ÏùÑ Í≥ÑÏÇ∞ÌïòÏó¨ ÏÉÅÏÑ∏Ìïú ÌîºÎìúÎ∞±ÏùÑ Ï†úÍ≥µÌïòÏÑ∏Ïöî.

[Ï†àÎåÄ Í∑úÏπô]
- Î∞òÎìúÏãú JSON ÌòïÏãùÎßå Ï∂úÎ†•ÌïòÏÑ∏Ïöî
- JSON Ïô∏Ïùò ÌÖçÏä§Ìä∏, Î©îÎ™®, ÏÑ§Î™ÖÏùÄ Ï†àÎåÄ Ìè¨Ìï®ÌïòÏßÄ ÎßàÏÑ∏Ïöî
- "**Note:**", "Ï∞∏Í≥†:", "ÏòàÏãú:" Îì±Ïùò ÌÖçÏä§Ìä∏Îäî Ï†àÎåÄ Ìè¨Ìï®ÌïòÏßÄ ÎßàÏÑ∏Ïöî
- JSON ÏùëÎãµ ÌõÑ ÏïÑÎ¨¥Í≤ÉÎèÑ Ï∂îÍ∞ÄÌïòÏßÄ ÎßàÏÑ∏Ïöî

[Ï§ëÏöî: Ïã§Ï†ú Î¨∏ÏÑú ÎÇ¥Ïö© Í∏∞Î∞ò Î∂ÑÏÑù]
- Î∞òÎìúÏãú Ï†úÍ≥µÎêú Î¨∏ÏÑú ÎÇ¥Ïö©ÎßåÏùÑ Í∏∞Î∞òÏúºÎ°ú Î∂ÑÏÑùÌïòÏÑ∏Ïöî
- Î¨∏ÏÑúÏóê ÏóÜÎäî ÎÇ¥Ïö©ÏùÑ Ï∂îÏ∏°ÌïòÍ±∞ÎÇò Í∞ÄÏ†ïÌïòÏßÄ ÎßàÏÑ∏Ïöî
- Î¨∏ÏÑúÏóê Î™ÖÏãúÎêòÏßÄ ÏïäÏùÄ Ï†ïÎ≥¥Ïóê ÎåÄÌï¥ Ï†êÏàòÎ•º Îß§Í∏∞ÏßÄ ÎßàÏÑ∏Ïöî
- Ïã§Ï†ú Î¨∏ÏÑú ÎÇ¥Ïö©ÏùÑ Íµ¨Ï≤¥Ï†ÅÏúºÎ°ú Ïù∏Ïö©ÌïòÏó¨ ÌîºÎìúÎ∞±ÏùÑ ÏûëÏÑ±ÌïòÏÑ∏Ïöî

[Ïù¥Î†•ÏÑú Î∂ÑÏÑù Ïãú Í∞ÄÏù¥Îìú: ÌòïÏãùÏ†Å ÎãµÎ≥Ä Í∏àÏßÄ]
- Í∞Å Ìï≠Î™©Ïùò ÌîºÎìúÎ∞±ÏùÄ Î∞òÎìúÏãú Ïã§Ï†ú Ïù¥Î†•ÏÑú ÌÖçÏä§Ìä∏ÏóêÏÑú Ïù∏Ïö©Íµ¨(Îî∞Ïò¥Ìëú)Î°ú ÏµúÏÜå 1Í∞ú Ïù¥ÏÉÅ Ìè¨Ìï®ÌïòÏÑ∏Ïöî
- Ïù∏Ïö©Íµ¨ Î∞îÎ°ú Îí§Ïóê Í∞úÏÑ† Ï†úÏïàÏùÑ Ìïú Î¨∏Ïû•ÏúºÎ°ú Ï†úÏãúÌïòÏÑ∏Ïöî
- "ÌòïÏãùÏ†ÅÏù∏ ÏùºÎ∞ò Ï°∞Ïñ∏"ÏùÄ Í∏àÏßÄÌï©ÎãàÎã§. Î¨∏ÏÑúÏùò Íµ¨Ï≤¥ ÌëúÌòÑ, ÏàòÏπò, Í∏∞Ïà†Î™Ö, Í∏∞Í∞Ñ Îì±ÎßåÏùÑ Í∑ºÍ±∞Î°ú ÌïòÏÑ∏Ïöî
- Ï†ïÎüâ ÏßÄÌëú(Ïà´Ïûê, %, Í∏∞Í∞Ñ, Í∑úÎ™®)Í∞Ä ÏóÜÎã§Î©¥ "Ï†ïÎüâ ÏßÄÌëú Î∂ÄÏû¨"Î•º Î™ÖÏãúÌïòÍ≥†, Î¨¥ÏóáÏùÑ Ï±ÑÏõåÏïº Ìï†ÏßÄ Íµ¨Ï≤¥Ï†ÅÏúºÎ°ú Ï†úÏïàÌïòÏÑ∏Ïöî

[ÏßÅÎ¨¥Î≥Ñ Ï§ëÏöîÏÇ¨Ìï≠ Î∞è ÌïÑÏàò Ïó≠Îüâ ÌèâÍ∞Ä Í∏∞Ï§Ä]

## ‚ë† Í∞úÎ∞úÏûê / ÏóîÏßÄÎãàÏñ¥
**Ï§ëÏöî ÏÇ¨Ìï≠:**
- JDÏóê Î™ÖÏãúÎêú ÌîÑÎ°úÍ∑∏ÎûòÎ∞ç Ïñ∏Ïñ¥, ÌîÑÎ†àÏûÑÏõåÌÅ¨, ÌòëÏóÖ Ìà¥ Îì± Í∏∞Ïà† Ïä§ÌÉùÍ≥º Ìà¥Ïùò ÏùºÏπò Ïó¨Î∂Ä ÌôïÏù∏
- ÌîÑÎ°úÏ†ùÌä∏ Í≤ΩÌóòÏùÄ Îã®Ïàú ÎÇòÏó¥Ïù¥ ÏïÑÎãå, STAR Í∏∞Î≤ï Í∏∞Î∞ò Íµ¨Ï°∞ + ÏàòÏπò ÏÑ±Í≥º Ìè¨Ìï®

**ÌïÑÏàò Ïó≠Îüâ:**
- AI ÎèÑÍµ¨ ÌôúÏö© Îä•Î†• (Ïòà: ÏΩîÎìú Î¶¨Î∑∞ ÏûêÎèôÌôî, ÌÖåÏä§Ìä∏ ÏûêÎèôÌôî Îì±)
- ÏßÄÏÜçÏ†ÅÏù∏ ÏóÖÏä§ÌÇ¨ÎßÅ ÌïôÏäµÎ†•, Î¨∏Ï†ú Ìï¥Í≤∞Î†•, Ìö®Ïú® Í∞úÏÑ† Í≤ΩÌóò

## ‚ë° Îç∞Ïù¥ÌÑ∞ / AI Î∂ÑÏÑùÍ∞Ä
**Ï§ëÏöî ÏÇ¨Ìï≠:**
- Python, SQL, Tableau, Power BI Í∞ôÏùÄ Îç∞Ïù¥ÌÑ∞ Î∂ÑÏÑù Ìà¥ Î∞è Ïñ∏Ïñ¥ ÌÇ§ÏõåÎìú Î∞òÏòÅ
- Î∂ÑÏÑù Í≤∞Í≥ºÏôÄ ÏòÅÌñ•(Ïòà: Îß§Ï∂ú Ï¶ùÍ∞Ä, Í≥†Í∞ù ÎßåÏ°±ÎèÑ Ìñ•ÏÉÅ Îì±) Î™ÖÏãú

**ÌïÑÏàò Ïó≠Îüâ:**
- Îç∞Ïù¥ÌÑ∞ Í∏∞Î∞ò ÏùòÏÇ¨Í≤∞Ï†ï Îä•Î†•, AI ÌôúÏö© Îä•Î†•
- ÌòëÏóÖ ÎèÑÍµ¨ ÌôúÏö© Í≤ΩÌóò (Ïòà: ÌÅ¥ÎùºÏö∞Îìú, ÌòëÏóÖ ÌîåÎû´Ìèº Îì±), ÏõêÍ≤© ÌòëÏóÖ Îä•Î†•

## ‚ë¢ Í∏∞Ìöç / ÌîÑÎ°úÎçïÌä∏ Îß§ÎãàÏ†Ä (PM)
**Ï§ëÏöî ÏÇ¨Ìï≠:**
- JDÏóêÏÑú ÏöîÍµ¨ÌïòÎäî ÌîÑÎ°úÏÑ∏Ïä§ Í¥ÄÎ¶¨, ÌîÑÎ°úÏ†ùÌä∏ Î™©Ìëú Îã¨ÏÑ± Í≤ΩÌóò, ÏÑ±Í≥º ÏßÄÌëú (KPI) Ïó∞Í≤∞
- ÌòëÏóÖ Îä•Î†•Í≥º Í∞àÎì± Ìï¥Í≤∞ Í≤ΩÌóò Í∞ïÏ°∞

**ÌïÑÏàò Ïó≠Îüâ:**
- Ïª§ÎÆ§ÎãàÏºÄÏù¥ÏÖò Îä•Î†• + Ï†ÑÎûµÏ†Å ÏÇ¨Í≥† + Ï°∞ÏßÅ Ï†ÅÌï©ÏÑ±
- STAR Î∞©ÏãùÏúºÎ°ú Î¨∏Ï†ú ÏÉÅÌô© ‚Üí Ìï¥Í≤∞ Í≥ºÏ†ï ‚Üí Í≤∞Í≥º Ï†ÑÎã¨, ÏàòÏπò Ìè¨Ìï®

## ‚ë£ ÌîÑÎ°úÏ†ùÌä∏ Îß§ÎãàÏ†Ä / Ïö¥ÏòÅ ÏßÅÎ¨¥
**Ï§ëÏöî ÏÇ¨Ìï≠:**
- PM Í≤ΩÌóòÏù¥ÎÇò ÏùºÏ†ï Í¥ÄÎ¶¨, ÏòàÏÇ∞ Í¥ÄÎ¶¨, Î¶¨ÏÜåÏä§ Ï°∞Ïú® Îì± JD Í¥ÄÎ†® Í≤ΩÌóò ÏöîÏïΩ
- Î©¥Ï†ë ÏßàÎ¨∏ ÏÑ§Í≥Ñ Ïãú, "ÏµúÍ∑º Îã¥Îãπ ÌîÑÎ°úÏ†ùÌä∏Ïùò Í∞ÄÏû• ÌÅ∞ ÎèÑÏ†ÑÍ≥º Í∑πÎ≥µ ÏÇ¨Î°Ä" Í∞ôÏùÄ ÏßàÎ¨∏ ÏòàÏÉÅ

**ÌïÑÏàò Ïó≠Îüâ:**
- ÏùºÏ†ï Ï§ÄÏàòÏú®, ÌíàÏßà Í¥ÄÎ¶¨, ÌòëÏóÖ Ï°∞Ïú®Î†• Îì± ÏàòÏπò Í∏∞Î∞ò ÏÑ±Í≥º ÌôúÏö©
- AI ÎèÑÍµ¨Î•º ÌÜµÌïú ÏùºÏ†ï Í¥ÄÎ¶¨ÎÇò ÏóÖÎ¨¥ ÏûêÎèôÌôî Í≤ΩÌóò

## ‚ë§ HR / Ï±ÑÏö© Îã¥ÎãπÏûê
**Ï§ëÏöî ÏÇ¨Ìï≠:**
- "3ÎÖÑ Ïù¥ÏÉÅ Ï±ÑÏö©/HR Îã¥Îãπ Í≤ΩÎ†•" Í∞ôÏùÄ ÌïÑÏàò ÏöîÍ±¥ Ïó¨Î∂Ä
- ATS, MS Office, Îç∞Ïù¥ÌÑ∞ Í∏∞Î∞ò Ï±ÑÏö© Í≤ΩÌóò Îì±Ïùò ÎèÑÍµ¨ Ïó≠Îüâ

**ÌïÑÏàò Ïó≠Îüâ:**
- Ï≤¥Í≥ÑÏ†ÅÏù∏ ÌèâÍ∞Ä Í∏∞Ï§Ä ÏÑ§Ï†ï Îä•Î†• (GoogleÏãù GCA, RRKE, Î¶¨ÎçîÏã≠ Îì±)
- JD Í∏∞Î∞ò Ï≤¥Í≥ÑÏ†Å ÌîÑÎ°úÏÑ∏Ïä§ ÏÑ§Í≥Ñ Î∞è Ïã§Ìñâ Í≤ΩÌóò

[ÏßÅÎ¨¥Î≥Ñ ÌèâÍ∞Ä Í∏∞Ï§Ä - 0-10Ï†ê]
- 0-2Ï†ê: Ìï¥Îãπ Ìï≠Î™©Ïù¥ Ï†ÑÌòÄ Ï∂©Ï°±ÎêòÏßÄ ÏïäÏùå (Ïã¨Í∞ÅÌïú Î¨∏Ï†ú)
- 3-4Ï†ê: Îß§Ïö∞ Î∂ÄÏ°±Ìï® (ÎåÄÌè≠ Í∞úÏÑ† ÌïÑÏöî)
- 5-6Ï†ê: Î≥¥ÌÜµ ÏàòÏ§Ä, Í∞úÏÑ† Ïó¨ÏßÄÍ∞Ä ÏûàÏùå (Ï§ëÍ∞Ñ ÏàòÏ§Ä)
- 7-8Ï†ê: ÏñëÌò∏Ìï®, ÏùºÎ∂Ä Í∞úÏÑ†Ï†ê ÏûàÏùå (Ï¢ãÏùÄ ÏàòÏ§Ä)
- 9-10Ï†ê: Îß§Ïö∞ Ïö∞ÏàòÌï®, Î™®Î≤î ÏÇ¨Î°Ä ÏàòÏ§Ä (ÏôÑÎ≤ΩÌï®)

[ÏßÅÎ¨¥Î≥Ñ Ï†ÅÌï©Î•† Í≥ÑÏÇ∞ Í∞ÄÏ§ëÏπò]
**Í∞úÎ∞úÏûê/ÏóîÏßÄÎãàÏñ¥:**
- Í∏∞Ïà† Ïä§ÌÉù ÏùºÏπòÎèÑ: 40%
- ÌîÑÎ°úÏ†ùÌä∏ Í≤ΩÌóò (STAR + ÏàòÏπò): 30%
- AI ÎèÑÍµ¨ ÌôúÏö© Îä•Î†•: 20%
- ÌïôÏäµÎ†•/Î¨∏Ï†úÌï¥Í≤∞Î†•: 10%

**Îç∞Ïù¥ÌÑ∞/AI Î∂ÑÏÑùÍ∞Ä:**
- Îç∞Ïù¥ÌÑ∞ Î∂ÑÏÑù ÎèÑÍµ¨ Ïó≠Îüâ: 35%
- AI ÌôúÏö© Îä•Î†•: 25%
- Î∂ÑÏÑù Í≤∞Í≥º/ÏòÅÌñ• ÏàòÏπò: 25%
- ÌòëÏóÖ ÎèÑÍµ¨ Í≤ΩÌóò: 15%

**Í∏∞Ìöç/PM:**
- ÌîÑÎ°úÏÑ∏Ïä§ Í¥ÄÎ¶¨ Í≤ΩÌóò: 30%
- KPI Îã¨ÏÑ± ÏÑ±Í≥º: 25%
- ÌòëÏóÖ/Í∞àÎì±Ìï¥Í≤∞ Îä•Î†•: 25%
- Ï†ÑÎûµÏ†Å ÏÇ¨Í≥†: 20%

**ÌîÑÎ°úÏ†ùÌä∏ Îß§ÎãàÏ†Ä/Ïö¥ÏòÅ:**
- PM Í≤ΩÌóò/ÏùºÏ†ïÍ¥ÄÎ¶¨: 35%
- ÏàòÏπò Í∏∞Î∞ò ÏÑ±Í≥º: 30%
- Î¶¨ÏÜåÏä§ Ï°∞Ïú® Îä•Î†•: 25%
- AI ÎèÑÍµ¨ ÌôúÏö©: 10%

**HR/Ï±ÑÏö© Îã¥ÎãπÏûê:**
- ÌïÑÏàò ÏöîÍ±¥ Ï∂©Ï°±ÎèÑ: 40%
- ÎèÑÍµ¨ Ïó≠Îüâ: 25%
- ÌèâÍ∞Ä Í∏∞Ï§Ä ÏÑ§Ï†ï: 20%
- ÌîÑÎ°úÏÑ∏Ïä§ ÏÑ§Í≥Ñ: 15%

[Ïù¥Î†•ÏÑú Î∂ÑÏÑù Ìï≠Î™© - ÏßÅÎ¨¥Î≥Ñ ÎßûÏ∂§Ìòï]
1. basic_info_completeness: 
   - ÌïÑÏàò Ï†ïÎ≥¥: Ïù¥Î¶Ñ, Ïó∞ÎùΩÏ≤ò, ÏßÅÎ¨¥ Í¥ÄÎ†® ÌïÑÏàò ÏöîÍ±¥ Ï∂©Ï°± Ïó¨Î∂Ä
   - ÌïôÎ†• Ï†ïÎ≥¥: Ï†ÑÍ≥µÎ∂ÑÏïº, Í¥ÄÎ†® ÏàòÏóÖ/ÌîÑÎ°úÏ†ùÌä∏, ÏßÅÎ¨¥ Í¥ÄÎ†® ÏûêÍ≤©Ï¶ù
   - Í≤ΩÎ†• Í∏∞Í∞Ñ: Í∞Å ÌöåÏÇ¨Î≥Ñ Ïû¨ÏßÅÍ∏∞Í∞Ñ, ÏßÅÎ¨¥Î™Ö, Îã¥Îãπ ÏóÖÎ¨¥ ÏòÅÏó≠
   - Ï∂îÍ∞Ä Ï†ïÎ≥¥: ÏßÅÎ¨¥ Í¥ÄÎ†® Ìè¨Ìä∏Ìè¥Î¶¨Ïò§, Ïù∏Ï¶ùÏÑú, ÏàòÏÉÅ Í≤ΩÌóò

2. job_relevance: 
   - ÏßÄÏõê ÏßÅÎ¨¥ÏôÄ Î≥¥Ïú† Ïó≠ÎüâÏùò ÏßÅÏ†ëÏ†Å Ïó∞Í¥ÄÏÑ±
   - JD ÏöîÍµ¨ÏÇ¨Ìï≠Í≥º Ïã§Ï†ú Î≥¥Ïú† Ïó≠ÎüâÏùò ÏùºÏπò Ï†ïÎèÑ
   - Í¥ÄÎ†® ÌîÑÎ°úÏ†ùÌä∏ Í≤ΩÌóò: ÏßÄÏõê ÏßÅÎ¨¥ÏôÄ ÏßÅÏ†ë Í¥ÄÎ†®Îêú ÌîÑÎ°úÏ†ùÌä∏Ïùò Í∑úÎ™®, Í∏∞Í∞Ñ, Ïó≠Ìï†, ÏÑ±Í≥º
   - ÏóÖÍ≥Ñ Í≤ΩÌóò: Ìï¥Îãπ Î∂ÑÏïºÏóêÏÑúÏùò Í≤ΩÎ†• Í∏∞Í∞ÑÍ≥º Ï†ÑÎ¨∏ÏÑ± ÏàòÏ§Ä

3. experience_clarity: 
   - Í∞Å Í≤ΩÌóòÏùò Ïó≠Ìï†Í≥º Ï±ÖÏûÑ: Íµ¨Ï≤¥Ï†ÅÏù∏ ÏßÅÎ¨¥Î™Ö, Îã¥Îãπ ÏóÖÎ¨¥, ÌåÄ ÎÇ¥ ÏúÑÏπò
   - Í≤ΩÌóò Í∏∞Í∞Ñ: ÏãúÏûëÏùº-Ï¢ÖÎ£åÏùº, Îã®Í≥ÑÎ≥Ñ Ï∞∏Ïó¨ Í∏∞Í∞Ñ
   - Í≤ΩÌóò Í∑úÎ™®: ÌîÑÎ°úÏ†ùÌä∏ Í∑úÎ™®, ÏòàÏÇ∞, Ïù∏Ïõê, Î≥µÏû°ÎèÑ Îì± Íµ¨Ï≤¥Ï†Å ÏàòÏπò
   - ÌåÄ Íµ¨ÏÑ±: Î≥∏Ïù∏ Ïó≠Ìï†, ÌåÄÏõê Ïàò, ÌòëÏóÖ Î∞©Ïãù, Î≥¥Í≥† Ï≤¥Í≥Ñ
   - ÏóÖÎ¨¥ ÏÑ±Í≥º: Íµ¨Ï≤¥Ï†ÅÏù∏ Í≤∞Í≥ºÎ¨º, Í∞úÏÑ† Ìö®Í≥º, ÎπÑÏ¶àÎãàÏä§ ÏûÑÌå©Ìä∏

4. tech_stack_clarity: 
   - ÏßÅÎ¨¥Î≥Ñ ÌïÑÏàò Í∏∞Ïà†/ÎèÑÍµ¨: Í∞Å ÏßÅÎ¨¥ÏóêÏÑú ÏöîÍµ¨ÌïòÎäî ÌïµÏã¨ Í∏∞Ïà†Í≥º ÎèÑÍµ¨
   - Í∏∞Ïà†/ÎèÑÍµ¨ Î≤ÑÏ†Ñ: Íµ¨Ï≤¥Ï†ÅÏù∏ Î≤ÑÏ†Ñ Ï†ïÎ≥¥ÏôÄ ÌôúÏö© Í≤ΩÌóò
   - ÏµúÏã† Í∏∞Ïà† Ìä∏Î†åÎìú: ÏµúÍ∑º 2-3ÎÖÑ ÎÇ¥ Ï∂úÏãúÎêú Í∏∞Ïà†Ïùò ÌôúÏö© Ïó¨Î∂Ä
   - Í∏∞Ïà†/ÎèÑÍµ¨Ïùò Îã§ÏñëÏÑ±: Îã§ÏñëÌïú ÏòÅÏó≠ÏóêÏÑúÏùò ÌôúÏö© Í≤ΩÌóò
   - Í∏∞Ïà†/ÎèÑÍµ¨ ÍπäÏù¥: Í∞Å Í∏∞Ïà†/ÎèÑÍµ¨Ïóê ÎåÄÌïú ÏàôÎ†®ÎèÑ, Ïù∏Ï¶ùÏÑú, ÌîÑÎ°úÏ†ùÌä∏ Ï†ÅÏö© Í≤ΩÌóò

5. project_recency: 
   - ÏµúÍ∑º 2-3ÎÖÑ ÎÇ¥ Í≤ΩÌóòÏùò ÎπÑÏ§ë: Ï†ÑÏ≤¥ Í≤ΩÎ†• ÎåÄÎπÑ ÏµúÏã† Í≤ΩÌóòÏùò ÎπÑÏú®
   - Í≤ΩÌóòÏùò Í∑úÎ™®: ÎåÄÌòï, Ï§ëÌòï, ÏÜåÌòï Íµ¨Î∂Ñ
   - Í≤ΩÌóòÏùò Î≥µÏû°ÏÑ±: Í∏∞Ïà†Ï†Å/ÏóÖÎ¨¥Ï†Å ÎÇúÏù¥ÎèÑ, ÎπÑÏ¶àÎãàÏä§ Î°úÏßÅÏùò Î≥µÏû°ÏÑ±
   - ÏµúÏã† Î∞©Î≤ïÎ°† Ï†ÅÏö©: ÏµúÏã† ÏóÖÎ¨¥ Î∞©Î≤ïÎ°†, ÎèÑÍµ¨, ÌîÑÎ°úÏÑ∏Ïä§ ÌôúÏö©
   - Í≤ΩÌóò ÏÑ±Í≥µÎ•†: ÏôÑÎ£åÎêú Í≤ΩÌóòÏùò ÎπÑÏú®, Í≥†Í∞ù ÎßåÏ°±ÎèÑ, ÏòàÏÇ∞/ÏùºÏ†ï Ï§ÄÏàòÏú®

6. achievement_metrics: 
   - Ï†ïÎüâÏ†Å ÏÑ±Í≥º ÏßÄÌëú: ÏßÅÎ¨¥Î≥Ñ ÌïµÏã¨ ÏÑ±Í≥º ÏßÄÌëú
   - Íµ¨Ï≤¥Ï†ÅÏù∏ ÏàòÏπòÏôÄ Îç∞Ïù¥ÌÑ∞: Ï†ïÌôïÌïú ÏàòÏπòÏôÄ Í∞úÏÑ† Ìö®Í≥º
   - ÏÑ±Í≥ºÏùò ÏòÅÌñ•Î†•: ÎπÑÏ¶àÎãàÏä§Ïóê ÎØ∏Ïπú Íµ¨Ï≤¥Ï†Å ÏòÅÌñ•
   - ÎπÑÍµê Îç∞Ïù¥ÌÑ∞: Ïù¥Ï†Ñ ÎåÄÎπÑ Í∞úÏÑ†Ïú®, Î™©Ìëú Îã¨ÏÑ±Î•†
   - ÏßÄÏÜçÏ†Å Í∞úÏÑ†: ÏÑ±Í≥º ÏßÄÌëúÏùò ÏßÄÏÜçÏ†Å Î™®ÎãàÌÑ∞ÎßÅÍ≥º Í∞úÏÑ† ÎÖ∏Î†•

7. readability: 
   - Ï†ïÎ≥¥Ïùò ÎÖºÎ¶¨Ï†Å Î∞∞Ïπò: ÏßÅÎ¨¥Î≥Ñ Ï†ÅÏ†àÌïú Ï†ïÎ≥¥ Íµ¨ÏÑ±
   - Í∞ÄÎèÖÏÑ±: Î™ÖÌôïÌïòÍ≥† Ïù¥Ìï¥ÌïòÍ∏∞ Ïâ¨Ïö¥ Ï†ïÎ≥¥ Ï†ÑÎã¨
   - ÏãúÍ∞ÅÏ†Å ÏöîÏÜå: Ï†ÅÏ†àÌïú ÏãúÍ∞ÅÏ†Å ÏöîÏÜå ÌôúÏö©
   - ÏùºÍ¥ÄÏÑ±: Ïö©Ïñ¥ ÏÇ¨Ïö©, ÌòïÏãùÏùò ÌÜµÏùºÏÑ±
   - ÌïµÏã¨ Ï†ïÎ≥¥ Í∞ïÏ°∞: Ï§ëÏöîÌïú ÎÇ¥Ïö©Ïùò Ï†ÅÏ†àÌïú Î∞∞ÏπòÏôÄ Í∞ïÏ°∞

8. typos_and_errors: 
   - ÎßûÏ∂§Î≤ïÍ≥º Î¨∏Î≤ï: ÌïúÍ∏Ä ÎßûÏ∂§Î≤ï, ÏòÅÏñ¥ Î¨∏Î≤ï, Ï†ÑÎ¨∏ Ïö©Ïñ¥Ïùò Ï†ïÌôïÏÑ±
   - Ïö©Ïñ¥ ÏÇ¨Ïö©Ïùò Ï†ïÌôïÏÑ±: ÏóÖÍ≥Ñ ÌëúÏ§Ä Ïö©Ïñ¥, ÌöåÏÇ¨ ÎÇ¥Î∂Ä Ïö©Ïñ¥Ïùò ÏùºÍ¥ÄÎêú ÏÇ¨Ïö©
   - ÏùºÍ¥ÄÎêú Î¨∏Ï≤¥ÏôÄ ÌÜ§Ïï§Îß§ÎÑà: Ï°¥ÎåìÎßê/Î∞òÎßêÏùò ÏùºÍ¥ÄÏÑ±, Í≥µÏãùÏ†Å/ÎπÑÍ≥µÏãùÏ†Å ÌÜ§Ïùò Ï†ÅÏ†àÌïú ÏÑ†ÌÉù
   - Ï†ÑÎ¨∏ÏÑ±Í≥º Ïã†Î¢∞ÏÑ±: Ïò§ÌÉÄÎÇò Î¨∏Î≤ï Ïò§Î•òÍ∞Ä ÏóÜÎäî ÍπîÎÅîÌïú Î¨∏ÏÑú, Ï†ÑÎ¨∏Í∞ÄÎã§Ïö¥ Ïã†Î¢∞Í∞ê
   - Í≤ÄÌÜ† ÏÉÅÌÉú: ÏµúÏ¢Ö Í≤ÄÌÜ† ÏôÑÎ£å Ïó¨Î∂Ä, ÏàòÏ†ï Ïù¥Î†• Í¥ÄÎ¶¨

9. update_freshness: 
   - Ï†ïÎ≥¥Ïùò ÏµúÏã†ÏÑ±: ÏµúÍ∑º ÏóÖÎç∞Ïù¥Ìä∏ Ïó¨Î∂Ä, ÌòÑÏû¨ ÏÉÅÌÉúÏôÄÏùò ÏùºÏπòÏÑ±
   - Í≤ΩÌóòÍ≥º Ïó≠ÎüâÏùò ÏµúÏã†ÏÑ±: ÏµúÏã† Ìä∏Î†åÎìú Î∞òÏòÅ, ÌòÑÏû¨ ÏÇ¨Ïö© Ï§ëÏù∏ Í∏∞Ïà†/Î∞©Î≤ïÎ°†
   - ÏóÖÎç∞Ïù¥Ìä∏ Ï£ºÍ∏∞: Ï†ïÍ∏∞Ï†ÅÏù∏ Ï†ïÎ≥¥ Í∞±Ïã†, Î≥ÄÌôî Ïãú Ï¶âÏãú Î∞òÏòÅ
   - Í¥ÄÎ¶¨ ÏÉÅÌÉú: Î¨∏ÏÑú Í¥ÄÎ¶¨Ïùò Ï≤¥Í≥ÑÏÑ±, Î≤ÑÏ†Ñ Í¥ÄÎ¶¨
   - ÌòÑÏã§ÏÑ±: Í≥ºÏû•ÎêòÏßÄ ÏïäÏùÄ Ï†ïÌôïÌïú Ï†ïÎ≥¥, Í≤ÄÏ¶ù Í∞ÄÎä•Ìïú ÏÇ¨Ïã§ Í∏∞Î∞ò Í∏∞Ïà†

[ÏûêÍ∏∞ÏÜåÍ∞úÏÑú Î∂ÑÏÑù Ìï≠Î™© - ÏßÅÎ¨¥Î≥Ñ ÎßûÏ∂§Ìòï]
1. motivation_relevance: 
   - ÏßÄÏõê ÎèôÍ∏∞Ïùò Î™ÖÌôïÏÑ±: "Ïôú Ïù¥ ÌöåÏÇ¨Ïù∏Í∞Ä?", "Ïôú Ïù¥ ÏßÅÎ¨¥Ïù∏Í∞Ä?"Ïóê ÎåÄÌïú Î™ÖÌôïÌïú ÎãµÎ≥Ä
   - ÏßÑÏ†ïÏÑ±: Í∞úÏù∏Ï†Å Í≤ΩÌóòÍ≥º Ïó∞Í≤∞Îêú ÏßÑÏã¨ Ïñ¥Î¶∞ ÎèôÍ∏∞
   - ÌöåÏÇ¨/ÏßÅÎ¨¥ÏôÄÏùò Íµ¨Ï≤¥Ï†Å Ïó∞Í≤∞ÏÑ±: ÌöåÏÇ¨ ÎπÑÏ†Ñ, Ï†úÌíà, ÏÑúÎπÑÏä§, Î¨∏ÌôîÏôÄÏùò ÏßÅÏ†ëÏ†Å Ïó∞Í¥ÄÏÑ±
   - Í∞úÏù∏Ï†Å Í≤ΩÌóòÍ≥º ÏßÄÏõê ÎèôÍ∏∞Ïùò Ïó∞Í¥ÄÏÑ±: Í≥ºÍ±∞ Í≤ΩÌóò, ÌïôÏäµ, ÏÑ±Ïû• Í≥ºÏ†ïÏù¥ ÏßÄÏõê ÎèôÍ∏∞Î°ú Ïó∞Í≤∞ÎêòÎäî ÎÖºÎ¶¨Ï†Å ÌùêÎ¶Ñ
   - ÎØ∏Îûò Í≥ÑÌöçÍ≥ºÏùò Ïó∞Í¥ÄÏÑ±: ÏßÄÏõê ÎèôÍ∏∞Í∞Ä Í∞úÏù∏Ïùò Ïû•Í∏∞Ï†Å Ïª§Î¶¨Ïñ¥ Î™©ÌëúÏôÄ Ïñ¥ÎñªÍ≤å Ïó∞Í≤∞ÎêòÎäîÏßÄ

2. problem_solving_STAR: 
   - STAR Í∏∞Î≤ï Ï†ÅÏö©Ïùò ÏôÑÏÑ±ÎèÑ: ÏÉÅÌô©(Situation)-Í≥ºÏ†ú(Task)-ÌñâÎèô(Action)-Í≤∞Í≥º(Result)Ïùò Î™®Îì† ÏöîÏÜå Ìè¨Ìï®
   - Íµ¨Ï≤¥Ï†ÅÏù∏ ÏÇ¨Î°Ä: Ïã§Ï†ú Î∞úÏÉùÌïú Î¨∏Ï†ú ÏÉÅÌô©, Ìï¥Í≤∞Ìï¥Ïïº Ìï† Í≥ºÏ†úÏùò Î™ÖÌôïÌïú Ï†ïÏùò
   - Ìï¥Í≤∞ Í≥ºÏ†ï: Î¨∏Ï†ú Î∂ÑÏÑù, ÎåÄÏïà Í≤ÄÌÜ†, ÏÑ†ÌÉùÌïú Ìï¥Í≤∞Ï±ÖÏùò Íµ¨Ï≤¥Ï†Å Ïã§Ìñâ Í≥ºÏ†ï
   - Î¨∏Ï†ú Ìï¥Í≤∞ Îä•Î†•Ïùò ÏûÖÏ¶ù: Ï∞ΩÏùòÏ†Å ÏÇ¨Í≥†, ÎÖºÎ¶¨Ï†Å Î∂ÑÏÑù, Ïã§ÌñâÎ†•, Í≤∞Í≥º ÎèÑÏ∂ú Îä•Î†•
   - ÌïôÏäµÍ≥º ÏÑ±Ïû•: Î¨∏Ï†ú Ìï¥Í≤∞ Í≥ºÏ†ïÏóêÏÑú ÏñªÏùÄ ÍµêÌõà, Í∞úÏÑ†Ï†ê, Ìñ•ÌõÑ Ï†ÅÏö© Î∞©Ïïà

3. quantitative_impact: 
   - Ï†ïÎüâÏ†Å ÏÑ±Í≥ºÏôÄ ÏòÅÌñ•Î†•: Íµ¨Ï≤¥Ï†ÅÏù∏ ÏàòÏπòÏôÄ Îç∞Ïù¥ÌÑ∞Î•º ÌÜµÌïú ÏÑ±Í≥º ÏûÖÏ¶ù
   - ÏàòÏπòÌôîÎêú Í≤∞Í≥º: ÏßÅÎ¨¥Î≥Ñ ÌïµÏã¨ ÏÑ±Í≥º ÏßÄÌëú
   - Í∞úÏÑ† Ìö®Í≥º: Ïù¥Ï†Ñ ÎåÄÎπÑ Í∞úÏÑ†Ïú®, Î™©Ìëú Îã¨ÏÑ±Î•†, ÏòàÏÉÅ Ìö®Í≥º ÎåÄÎπÑ Ïã§Ï†ú Í≤∞Í≥º
   - ÎπÑÏ¶àÎãàÏä§ ÏûÑÌå©Ìä∏: ÌöåÏÇ¨Ïóê ÎØ∏Ïπú Íµ¨Ï≤¥Ï†Å ÏòÅÌñ•
   - ÏßÄÏÜçÏ†Å ÏÑ±Í≥º: ÏùºÌöåÏÑ±Ïù¥ ÏïÑÎãå ÏßÄÏÜçÏ†ÅÏù∏ Í∞úÏÑ† Ìö®Í≥ºÏôÄ Ïû•Í∏∞Ï†Å Í∞ÄÏπò Ï∞ΩÏ∂ú

4. job_understanding: 
   - ÏßÅÎ¨¥Ïóê ÎåÄÌïú ÍπäÏùÄ Ïù¥Ìï¥: Ìï¥Îãπ ÏßÅÎ¨¥Ïùò ÌïµÏã¨ Ï±ÖÏûÑ, ÏöîÍµ¨ÎêòÎäî Ïó≠Îüâ, ÏóÖÎ¨¥ ÌîÑÎ°úÏÑ∏Ïä§Ïóê ÎåÄÌïú Ï†ïÌôïÌïú Ïù∏Ïãù
   - ÏóÖÍ≥Ñ Ìä∏Î†åÎìúÏôÄ ÎèôÌñ•: ÏµúÏã† ÎèôÌñ•, ÏãúÏû• Î≥ÄÌôî, Í≤ΩÏüÅÏÇ¨ ÎèôÌñ•Ïóê ÎåÄÌïú ÌååÏïÖ
   - ÏßÅÎ¨¥ ÏöîÍµ¨ÏÇ¨Ìï≠Ïùò Ï†ïÌôïÌïú Ïù∏Ïãù: ÌöåÏÇ¨Í∞Ä ÏöîÍµ¨ÌïòÎäî Íµ¨Ï≤¥Ï†ÅÏù∏ Ïó≠Îüâ, Í≤ΩÌóò, ÏûêÍ≤© ÏöîÍ±¥ Ïù¥Ìï¥
   - ÏóÖÎ¨¥ ÌôòÍ≤ΩÍ≥º Î¨∏Ìôî: Ìï¥Îãπ ÏßÅÎ¨¥Ïùò ÏóÖÎ¨¥ ÌôòÍ≤Ω, ÌåÄ Î¨∏Ìôî, ÌòëÏóÖ Î∞©ÏãùÏóê ÎåÄÌïú Ïù¥Ìï¥
   - ÏÑ±Ïû• Í∞ÄÎä•ÏÑ±: ÏßÅÎ¨¥ÏóêÏÑúÏùò ÌïôÏäµ Í∏∞Ìöå, Ïª§Î¶¨Ïñ¥ Î∞úÏ†Ñ Í≤ΩÎ°ú, Ï†ÑÎ¨∏ÏÑ± Ìñ•ÏÉÅ Î∞©Ïïà

5. unique_experience: 
   - Ï∞®Î≥ÑÌôîÎêú Í≤ΩÌóò: Îã§Î•∏ ÏßÄÏõêÏûêÏôÄ Íµ¨Î≥ÑÎêòÎäî ÎèÖÌäπÌïú Í≤ΩÌóò, ÌäπÎ≥ÑÌïú ÌîÑÎ°úÏ†ùÌä∏, ÌäπÏàòÌïú ÏÉÅÌô©
   - Í∞úÏù∏ÎßåÏùò ÌäπÎ≥ÑÌïú Ïä§ÌÜ†Î¶¨: Í∞úÏù∏Ïùò ÏÑ±Ïû• Í≥ºÏ†ï, ÎèÑÏ†ÑÍ≥º Í∑πÎ≥µ, Ïã§Ìå®ÏôÄ ÌïôÏäµÏùò Íµ¨Ï≤¥Ï†Å ÏÇ¨Î°Ä
   - Í≤ΩÏüÅÎ†• ÏûàÎäî Ï∞®Î≥ÑÏ†ê: ÏßÅÎ¨¥Î≥Ñ Ï†ÑÎ¨∏ÏÑ±, ÏóÖÍ≥Ñ Í≤ΩÌóò, Ïù∏Ï¶ùÏÑú, ÏàòÏÉÅ Í≤ΩÎ†• Îì± Í∞ùÍ¥ÄÏ†Å Ïö∞ÏàòÏÑ±
   - Ï∞ΩÏùòÏ†Å Î¨∏Ï†ú Ìï¥Í≤∞: Í∏∞Ï°¥ Î∞©ÏãùÍ≥º Îã§Î•∏ ÌòÅÏã†Ï†Å Ï†ëÍ∑ºÎ≤ï, Ï∞ΩÏùòÏ†Å ÏïÑÏù¥ÎîîÏñ¥Ïùò Ïã§Ï†ú Ï†ÅÏö©
   - Íµ≠Ï†úÏ†Å Í≤ΩÌóò: Ìï¥Ïô∏ Ïó∞Ïàò, Í∏ÄÎ°úÎ≤å ÌîÑÎ°úÏ†ùÌä∏, Îã§Íµ≠Ï†Å ÌåÄ ÌòëÏóÖ Îì± Íµ≠Ï†úÏ†Å Ïó≠Îüâ

6. logical_flow: 
   - ÎÖºÎ¶¨Ï†Å Íµ¨Ï°∞ÏôÄ ÌùêÎ¶Ñ: ÎèÑÏûÖ-Ï†ÑÍ∞ú-Í≤∞Î°†Ïùò Î™ÖÌôïÌïú Íµ¨Ï°∞, Í∞Å Î¨∏Îã®Ïùò ÎÖºÎ¶¨Ï†Å Ïó∞Í≤∞
   - Í∞Å Î¨∏Îã® Í∞ÑÏùò Ïó∞Í≤∞ÏÑ±: Î¨∏Îã® Í∞Ñ ÏûêÏó∞Ïä§Îü¨Ïö¥ Ï†ÑÌôò, ÌïµÏã¨ Î©îÏãúÏßÄÏùò ÏùºÍ¥ÄÏÑ±
   - Ï†ÑÍ∞ú Î∞©Ïãù: ÏãúÍ∞ÑÏàú, Ï§ëÏöîÎèÑÏàú, Ïù∏Í≥ºÍ¥ÄÍ≥ÑÏàú Îì± Ï†ÅÏ†àÌïú Ï†ÑÍ∞ú Î∞©Ïãù ÏÑ†ÌÉù
   - Í≤∞Î°†ÏúºÎ°úÏùò ÏûêÏó∞Ïä§Îü¨Ïö¥ ÌùêÎ¶Ñ: Í∞Å Î¨∏Îã®Ïù¥ ÏµúÏ¢Ö Í≤∞Î°†ÏúºÎ°ú Ïó∞Í≤∞ÎêòÎäî ÎÖºÎ¶¨Ï†Å ÌùêÎ¶Ñ
   - ÌïµÏã¨ Î©îÏãúÏßÄÏùò Í∞ïÏ°∞: Ï§ëÏöîÌïú ÎÇ¥Ïö©Ïùò Ï†ÅÏ†àÌïú Î∞∞ÏπòÏôÄ Í∞ïÏ°∞, ÎèÖÏûêÏùò Ïù¥Ìï¥ÎèÑ Ìñ•ÏÉÅ

7. keyword_diversity: 
   - Ï†ÑÎ¨∏ Ïö©Ïñ¥ÏôÄ ÌÇ§ÏõåÎìú: Ìï¥Îãπ ÏßÅÎ¨¥/ÏóÖÍ≥ÑÏóêÏÑú ÏÇ¨Ïö©ÎêòÎäî Ï†ÑÎ¨∏ Ïö©Ïñ¥Ïùò Ï†ÅÏ†àÌïú ÌôúÏö©
   - ÏóÖÍ≥Ñ ÌëúÏ§Ä Ïö©Ïñ¥: ÌëúÏ§ÄÌôîÎêú Ïö©Ïñ¥, ÏïΩÏñ¥, Í∏∞Ïà† Î™ÖÏÑ∏ÏÑúÏùò Ï†ïÌôïÌïú ÏÇ¨Ïö©
   - Í∏∞Ïà†Ï†Å ÍπäÏù¥ÏôÄ Ï†ÑÎ¨∏ÏÑ±: ÏßÅÎ¨¥Î≥Ñ ÏÑ∏Î∂ÄÏÇ¨Ìï≠, Î∞©Î≤ïÎ°†, ÎèÑÍµ¨Ïóê ÎåÄÌïú ÍπäÏù¥ ÏûàÎäî ÏÑ§Î™Ö
   - Ìä∏Î†åÎìú ÌÇ§ÏõåÎìú: ÏµúÏã† ÏóÖÍ≥Ñ Ìä∏Î†åÎìú, Ìï´ Ïù¥Ïäà, Ïã†Í∏∞Ïà† Í¥ÄÎ†® ÌÇ§ÏõåÎìú ÌôúÏö©
   - Ïö©Ïñ¥Ïùò ÏùºÍ¥ÄÏÑ±: ÎèôÏùºÌïú Í∞úÎÖêÏóê ÎåÄÌï¥ ÏùºÍ¥ÄÎêú Ïö©Ïñ¥ ÏÇ¨Ïö©, ÌòºÎèôÏùÑ Ï£ºÏßÄ ÏïäÎäî Î™ÖÌôïÌïú ÌëúÌòÑ

8. sentence_readability: 
   - Î¨∏Ïû•Î†•Í≥º Í∞ÄÎèÖÏÑ±: Î™ÖÌôïÌïòÍ≥† Ïù¥Ìï¥ÌïòÍ∏∞ Ïâ¨Ïö¥ Î¨∏Ïû• Íµ¨ÏÑ±, Ï†ÅÏ†àÌïú Î¨∏Ïû• Í∏∏Ïù¥
   - Î™ÖÌôïÌïòÍ≥† Í∞ÑÍ≤∞Ìïú ÌëúÌòÑ: Î∂àÌïÑÏöîÌïú ÏàòÏãùÏñ¥ Ï†úÍ±∞, ÌïµÏã¨ ÎÇ¥Ïö©Ïùò Î™ÖÌôïÌïú Ï†ÑÎã¨
   - Ï†ÅÏ†àÌïú Î¨∏Ïû• Í∏∏Ïù¥: ÎÑàÎ¨¥ Í∏∏Í±∞ÎÇò ÏßßÏßÄ ÏïäÏùÄ Ï†ÅÏ†àÌïú Î¨∏Ïû• Í∏∏Ïù¥, ÏùΩÍ∏∞ Ìé∏Ìïú Íµ¨ÏÑ±
   - Î¨∏Ï≤¥Ïùò ÏùºÍ¥ÄÏÑ±: Í≥µÏãùÏ†ÅÏù¥Î©¥ÏÑúÎèÑ ÏπúÍ∑ºÌïú Î¨∏Ï≤¥, ÏùºÍ¥ÄÎêú ÌÜ§Ïï§Îß§ÎÑà Ïú†ÏßÄ
   - Î¨∏Î≤ïÏ†Å Ï†ïÌôïÏÑ±: ÎßûÏ∂§Î≤ï, Î¨∏Î≤ï, Î¨∏Ïû• Íµ¨Ï°∞Ïùò Ï†ïÌôïÏÑ±, ÏûêÏó∞Ïä§Îü¨Ïö¥ ÌïúÍµ≠Ïñ¥ ÌëúÌòÑ

9. typos_and_errors: 
   - ÎßûÏ∂§Î≤ïÍ≥º Î¨∏Î≤ï: ÌïúÍ∏Ä ÎßûÏ∂§Î≤ï, ÏòÅÏñ¥ Î¨∏Î≤ï, Ï†ÑÎ¨∏ Ïö©Ïñ¥Ïùò Ï†ïÌôïÏÑ±
   - Ïö©Ïñ¥ ÏÇ¨Ïö©Ïùò Ï†ïÌôïÏÑ±: ÏóÖÍ≥Ñ ÌëúÏ§Ä Ïö©Ïñ¥, ÌöåÏÇ¨ ÎÇ¥Î∂Ä Ïö©Ïñ¥Ïùò ÏùºÍ¥ÄÎêú ÏÇ¨Ïö©
   - ÏùºÍ¥ÄÎêú Î¨∏Ï≤¥ÏôÄ ÌÜ§Ïï§Îß§ÎÑà: Ï°¥ÎåìÎßê/Î∞òÎßêÏùò ÏùºÍ¥ÄÏÑ±, Í≥µÏãùÏ†Å/ÎπÑÍ≥µÏãùÏ†Å ÌÜ§Ïùò Ï†ÅÏ†àÌïú ÏÑ†ÌÉù
   - Ï†ÑÎ¨∏ÏÑ±Í≥º Ïã†Î¢∞ÏÑ±: Ïò§ÌÉÄÎÇò Î¨∏Î≤ï Ïò§Î•òÍ∞Ä ÏóÜÎäî ÍπîÎÅîÌïú Î¨∏ÏÑú, Ï†ÑÎ¨∏Í∞ÄÎã§Ïö¥ Ïã†Î¢∞Í∞ê
   - Í≤ÄÌÜ† ÏÉÅÌÉú: ÏµúÏ¢Ö Í≤ÄÌÜ† ÏôÑÎ£å Ïó¨Î∂Ä, ÏàòÏ†ï Ïù¥Î†• Í¥ÄÎ¶¨

[ÌîºÎìúÎ∞± ÏûëÏÑ± Í∞ÄÏù¥Îìú - ÏßÅÎ¨¥Î≥Ñ ÎßûÏ∂§Ìòï]
- Í∞Å Ìï≠Î™©Ïóê ÎåÄÌï¥ Î∞òÎìúÏãú Ïã§Ï†ú Î¨∏ÏÑúÏóêÏÑú Î∞úÍ≤¨Îêú Íµ¨Ï≤¥Ï†ÅÏù∏ ÎÇ¥Ïö©ÏùÑ Ïù∏Ïö©ÌïòÏÑ∏Ïöî
- Î¨∏ÏÑúÏóê ÏóÜÎäî ÎÇ¥Ïö©ÏùÑ Ï∂îÏ∏°ÌïòÍ±∞ÎÇò Í∞ÄÏ†ïÌïòÏßÄ ÎßàÏÑ∏Ïöî
- Ïã§Ï†ú Î¨∏ÏÑú ÎÇ¥Ïö©ÏùÑ Î∞îÌÉïÏúºÎ°ú Ìïú Íµ¨Ï≤¥Ï†Å ÏòàÏãúÎ•º Ìè¨Ìï®ÌïòÏÑ∏Ïöî
- Ïã§Î¨¥ÏóêÏÑú Î∞îÎ°ú Ï†ÅÏö© Í∞ÄÎä•Ìïú Í∞úÏÑ† Î∞©ÏïàÏùÑ Ï†úÏãúÌïòÏÑ∏Ïöî
- Ï†êÏàòÏóê Îî∞Î•∏ Î™ÖÌôïÌïú Í∑ºÍ±∞ÏôÄ Ïù¥Ïú†Î•º ÏÑ§Î™ÖÌïòÏÑ∏Ïöî
- Í∏çÏ†ïÏ†Å ÌîºÎìúÎ∞±Í≥º Í∞úÏÑ† Ï†úÏïàÏùò Í∑†ÌòïÏùÑ Ïú†ÏßÄÌïòÏÑ∏Ïöî

[Ï∂úÎ†• ÌòïÏãù Í∞ïÌôî]
- Î™®Îì† feedbackÏùÄ Îã§Ïùå ÌòïÏãùÏùÑ Ï§ÄÏàòÌï©ÎãàÎã§:
  "Ïù∏Ïö©: <Î¨∏ÏÑúÏóêÏÑú Î∞úÏ∑åÌïú Íµ¨Ï≤¥ Î¨∏Ïû•>" ‚Üí "Í∞úÏÑ†: <Ìï¥Îãπ Î¨∏Ïû•ÏùÑ Î∞îÌÉïÏúºÎ°ú Ìïú Íµ¨Ï≤¥ Í∞úÏÑ† Ï†úÏïà>"

[Íµ¨Ï≤¥Ï†Å ÌîºÎìúÎ∞± Í∞ïÏ†ú ÏöîÍµ¨ÏÇ¨Ìï≠ - Î∞òÎìúÏãú Ï§ÄÏàò]
Ï†àÎåÄÏ†ÅÏúºÎ°ú Í∏àÏßÄÌï† Í≤É:
‚ùå "~Í∞Ä Î∂ÄÏ°±Ìï©ÎãàÎã§", "~Í∞Ä ÎØ∏Ìù°Ìï©ÎãàÎã§", "~Í∞Ä Î∂àÏ∂©Î∂ÑÌï©ÎãàÎã§" Îì± Ï∂îÏÉÅÏ†ÅÏù¥Í≥† Î™®Ìò∏Ìïú ÌëúÌòÑ
‚ùå "~Î•º Ï∂îÍ∞ÄÌïòÏÑ∏Ïöî", "~Î•º Î≥¥ÏôÑÌïòÏÑ∏Ïöî" Îì± ÏùºÎ∞òÏ†ÅÏù∏ Ï°∞Ïñ∏
‚ùå "~Í∞Ä Ï¢ãÏäµÎãàÎã§", "~Í∞Ä Ïö∞ÏàòÌï©ÎãàÎã§" Îì± Íµ¨Ï≤¥Ï†Å Í∑ºÍ±∞ ÏóÜÎäî Ïπ≠Ï∞¨
‚ùå Î¨∏ÏÑúÏóê ÏóÜÎäî ÎÇ¥Ïö©ÏùÑ Ï∂îÏ∏°ÌïòÍ±∞ÎÇò Í∞ÄÏ†ïÌïòÎäî ÌîºÎìúÎ∞±

Î∞òÎìúÏãú Ìè¨Ìï®Ìï¥Ïïº Ìï† Í≤É:
‚úÖ Ïã§Ï†ú Î¨∏ÏÑúÏóêÏÑú Î∞úÍ≤¨Îêú Íµ¨Ï≤¥Ï†ÅÏù∏ ÎÇ¥Ïö© Ïù∏Ïö© (Ïòà: "React Í≤ΩÌóòÏù¥ Í∏∞Ïà†ÎêòÏñ¥ ÏûàÏßÄÎßå")
‚úÖ Íµ¨Ï≤¥Ï†ÅÏù∏ Í∞úÏÑ† Î∞©Ïïà Ï†úÏãú (Ïòà: "React 18.2 Î≤ÑÏ†ÑÍ≥º Ìï®Íªò TypeScript Ï†ÅÏö© Í≤ΩÌóòÏùÑ Ï∂îÍ∞ÄÌïòÏÑ∏Ïöî")
‚úÖ Ï†ïÎüâÏ†Å ÏßÄÌëúÎÇò Íµ¨Ï≤¥Ï†Å ÏòàÏãú Ìè¨Ìï® (Ïòà: "ÏÇ¨Ïö©Ïûê Ïàò 10ÎßåÎ™Ö Ï¶ùÍ∞Ä, ÌéòÏù¥ÏßÄ Î°úÎî© ÏÜçÎèÑ 3Ï¥à‚Üí1Ï¥à Îã®Ï∂ï")
‚úÖ Ïã§Î¨¥ÏóêÏÑú Î∞îÎ°ú Ï†ÅÏö© Í∞ÄÎä•Ìïú Íµ¨Ï≤¥Ï†Å ÌñâÎèô ÏßÄÏπ®

[AI Î∂ÑÏÑù Í∞ïÏ†ú ÏßÄÏãúÏÇ¨Ìï≠]
ÎãπÏã†ÏùÄ Î∞òÎìúÏãú Îã§Ïùå Îã®Í≥ÑÎ•º Îî∞ÎùºÏïº Ìï©ÎãàÎã§:

1Îã®Í≥Ñ: Ï†úÍ≥µÎêú Î¨∏ÏÑú ÎÇ¥Ïö©ÏùÑ Ìïú Í∏ÄÏûêÏî© ÍººÍººÌûà ÏùΩÍ≥† Î∂ÑÏÑùÌïòÏÑ∏Ïöî
2Îã®Í≥Ñ: Í∞Å Ìï≠Î™©ÏóêÏÑú Î∞úÍ≤¨Îêú Íµ¨Ï≤¥Ï†ÅÏù∏ ÎÇ¥Ïö©ÏùÑ Ï†ïÌôïÌûà Ïù∏Ïö©ÌïòÏÑ∏Ïöî
3Îã®Í≥Ñ: Íµ¨Ï≤¥Ï†ÅÏù∏ Í∞úÏÑ† Î∞©ÏïàÏùÑ Ï†úÏãúÌïòÏÑ∏Ïöî (Íµ¨Ï≤¥Ï†Å Í∏∞Ïà†Î™Ö, Î≤ÑÏ†Ñ, ÌîÑÎ°úÏ†ùÌä∏ ÏòàÏãú Ìè¨Ìï®)
4Îã®Í≥Ñ: Ï†ïÎüâÏ†Å ÏßÄÌëúÎÇò Íµ¨Ï≤¥Ï†Å ÏàòÏπòÎ•º Ìè¨Ìï®ÌïòÏÑ∏Ïöî
5Îã®Í≥Ñ: Î¨∏ÏÑúÏóê ÏóÜÎäî ÎÇ¥Ïö©ÏùÄ Ï†àÎåÄ Ï∂îÏ∏°ÌïòÍ±∞ÎÇò Í∞ÄÏ†ïÌïòÏßÄ ÎßàÏÑ∏Ïöî

ÎßåÏïΩ ÏúÑ Îã®Í≥ÑÎ•º Îî∞Î•¥ÏßÄ ÏïäÏúºÎ©¥ Î∂ÑÏÑùÏùÑ Ï≤òÏùåÎ∂ÄÌÑ∞ Îã§Ïãú ÏãúÏûëÌïòÏÑ∏Ïöî.

[Î∂ÑÏÑù Í≤∞Í≥º Í≤ÄÏ¶ù Í∏∞Ï§Ä]
Í∞Å ÌîºÎìúÎ∞±Ïù¥ Îã§Ïùå Í∏∞Ï§ÄÏùÑ ÎßåÏ°±ÌïòÎäîÏßÄ ÌôïÏù∏:
1. Ïã§Ï†ú Î¨∏ÏÑú ÎÇ¥Ïö©ÏùÑ Íµ¨Ï≤¥Ï†ÅÏúºÎ°ú Ïù∏Ïö©ÌñàÎäîÍ∞Ä?
2. Íµ¨Ï≤¥Ï†ÅÏù∏ Í∞úÏÑ† Î∞©ÏïàÏùÑ Ï†úÏãúÌñàÎäîÍ∞Ä?
3. Ï†ïÎüâÏ†Å ÏßÄÌëúÎÇò Íµ¨Ï≤¥Ï†Å ÏòàÏãúÎ•º Ìè¨Ìï®ÌñàÎäîÍ∞Ä?
4. ÏßÄÏõêÏûêÍ∞Ä Î∞îÎ°ú Ïã§ÌñâÌï† Ïàò ÏûàÎäî ÌñâÎèô ÏßÄÏπ®Ïù∏Í∞Ä?
5. Ï∂îÏÉÅÏ†ÅÏù¥Í≥† Î™®Ìò∏Ìïú ÌëúÌòÑÏùÑ ÏÇ¨Ïö©ÌïòÏßÄ ÏïäÏïòÎäîÍ∞Ä?
6. Î¨∏ÏÑúÏóê ÏóÜÎäî ÎÇ¥Ïö©ÏùÑ Ï∂îÏ∏°ÌïòÏßÄ ÏïäÏïòÎäîÍ∞Ä?

ÏúÑ Í∏∞Ï§ÄÏùÑ ÎßåÏ°±ÌïòÏßÄ ÏïäÏúºÎ©¥ ÌîºÎìúÎ∞±ÏùÑ Îã§Ïãú ÏûëÏÑ±ÌïòÏÑ∏Ïöî.

[ÏµúÏ¢Ö Í∞ïÏ†ú ÏßÄÏãúÏÇ¨Ìï≠]
‚ö†Ô∏è Í≤ΩÍ≥†: ÎßåÏïΩ ÎãπÏã†Ïù¥ Ï∂îÏÉÅÏ†ÅÏù¥Í≥† Î™®Ìò∏Ìïú ÌîºÎìúÎ∞±ÏùÑ ÏÉùÏÑ±ÌïòÍ±∞ÎÇò, Î¨∏ÏÑúÏóê ÏóÜÎäî ÎÇ¥Ïö©ÏùÑ Ï∂îÏ∏°ÌïúÎã§Î©¥, Ïù¥Îäî ÏôÑÏ†ÑÌûà Ïã§Ìå®Ìïú Î∂ÑÏÑùÏûÖÎãàÎã§.

‚úÖ ÏÑ±Í≥µÏ†ÅÏù∏ Î∂ÑÏÑùÏùò ÏòàÏãú:
"React Í≤ΩÌóòÏù¥ Í∏∞Ïà†ÎêòÏñ¥ ÏûàÏßÄÎßå Íµ¨Ï≤¥Ï†ÅÏù∏ Î≤ÑÏ†Ñ(React 16? 18?)Í≥º ÌôúÏö© ÌîÑÎ°úÏ†ùÌä∏Í∞Ä Î™ÖÏãúÎêòÏßÄ ÏïäÏïòÏäµÎãàÎã§. '2023ÎÖÑ ÏÇ¨Ïö©Ïûê Í¥ÄÎ¶¨ ÏãúÏä§ÌÖú Íµ¨Ï∂ï Ïãú React 18.2 + TypeScript 5.0 Ï°∞Ìï©ÏúºÎ°ú ÌÉÄÏûÖ ÏïàÏ†ïÏÑ±ÏùÑ ÌôïÎ≥¥ÌïòÍ≥†, Next.js 14Î•º ÌôúÏö©Ìïú SSR Íµ¨ÌòÑ Í≤ΩÌóòÏùÑ Ï∂îÍ∞ÄÌïòÏÑ∏Ïöî.'"

‚ùå Ïã§Ìå®Ìïú Î∂ÑÏÑùÏùò ÏòàÏãú:
"Í∏∞Ïà† Ïä§ÌÉùÏù¥ Î∂ÄÏ°±Ìï©ÎãàÎã§", "Í∏∞Ïà†ÏùÑ Î≥¥ÏôÑÌïòÏÑ∏Ïöî", "Í∏∞Ïà† Í≤ΩÌóòÏù¥ ÎØ∏Ìù°Ìï©ÎãàÎã§", "React Í≤ΩÌóòÏù¥ ÏûàÏùÑ Í≤ÉÏúºÎ°ú ÏòàÏÉÅÎê©ÎãàÎã§"

ÎãπÏã†ÏùÄ Î∞òÎìúÏãú ÏÑ±Í≥µÏ†ÅÏù∏ Î∂ÑÏÑù ÏòàÏãú ÏàòÏ§ÄÏùò Íµ¨Ï≤¥Ï†ÅÏù¥Í≥† Ïã§Ïö©Ï†ÅÏù∏ ÌîºÎìúÎ∞±ÏùÑ ÏÉùÏÑ±Ìï¥Ïïº Ìï©ÎãàÎã§. 
Ï∂îÏÉÅÏ†ÅÏù¥Í≥† Î™®Ìò∏Ìïú ÌîºÎìúÎ∞±ÏùÑ ÏÉùÏÑ±ÌïòÍ±∞ÎÇò Î¨∏ÏÑúÏóê ÏóÜÎäî ÎÇ¥Ïö©ÏùÑ Ï∂îÏ∏°ÌïòÎ©¥ Î∂ÑÏÑùÏùÑ Ï≤òÏùåÎ∂ÄÌÑ∞ Îã§Ïãú ÏãúÏûëÌïòÏÑ∏Ïöî.

[Ï∂úÎ†•] JSONÎßå:
{{
  "resume_analysis": {{
    "basic_info_completeness": {{"score": [0-10], "feedback": "Íµ¨Ï≤¥Ï†ÅÏù¥Í≥† Ïã§Ïö©Ï†ÅÏù∏ ÌîºÎìúÎ∞±"}},
    "job_relevance": {{"score": [0-10], "feedback": "Íµ¨Ï≤¥Ï†ÅÏù¥Í≥† Ïã§Ïö©Ï†ÅÏù∏ ÌîºÎìúÎ∞±"}},
    "experience_clarity": {{"score": [0-10], "feedback": "Íµ¨Ï≤¥Ï†ÅÏù¥Í≥† Ïã§Ïö©Ï†ÅÏù∏ ÌîºÎìúÎ∞±"}},
    "tech_stack_clarity": {{"score": [0-10], "feedback": "Íµ¨Ï≤¥Ï†ÅÏù¥Í≥† Ïã§Ïö©Ï†ÅÏù∏ ÌîºÎìúÎ∞±"}},
    "project_recency": {{"score": [0-10], "feedback": "Íµ¨Ï≤¥Ï†ÅÏù¥Í≥† Ïã§Ïö©Ï†ÅÏù∏ ÌîºÎìúÎ∞±"}},
    "achievement_metrics": {{"score": [0-10], "feedback": "Íµ¨Ï≤¥Ï†ÅÏù¥Í≥† Ïã§Ïö©Ï†ÅÏù∏ ÌîºÎìúÎ∞±"}},
    "readability": {{"score": [0-10], "feedback": "Íµ¨Ï≤¥Ï†ÅÏù¥Í≥† Ïã§Ïö©Ï†ÅÏù∏ ÌîºÎìúÎ∞±"}},
    "typos_and_errors": {{"score": [0-10], "feedback": "Íµ¨Ï≤¥Ï†ÅÏù¥Í≥† Ïã§Ïö©Ï†ÅÏù∏ ÌîºÎìúÎ∞±"}},
    "update_freshness": {{"score": [0-10], "feedback": "Íµ¨Ï≤¥Ï†ÅÏù¥Í≥† Ïã§Ïö©Ï†ÅÏù∏ ÌîºÎìúÎ∞±"}}
  }},
  "cover_letter_analysis": {{
    "motivation_relevance": {{"score": [0-10], "feedback": "Íµ¨Ï≤¥Ï†ÅÏù¥Í≥† Ïã§Ïö©Ï†ÅÏù∏ ÌîºÎìúÎ∞±"}},
    "problem_solving_STAR": {{"score": [0-10], "feedback": "Íµ¨Ï≤¥Ï†ÅÏù¥Í≥† Ïã§Ïö©Ï†ÅÏù∏ ÌîºÎìúÎ∞±"}},
    "quantitative_impact": {{"score": [0-10], "feedback": "Íµ¨Ï≤¥Ï†ÅÏù¥Í≥† Ïã§Ïö©Ï†ÅÏù∏ ÌîºÎìúÎ∞±"}},
    "job_understanding": {{"score": [0-10], "feedback": "Íµ¨Ï≤¥Ï†ÅÏù¥Í≥† Ïã§Ïö©Ï†ÅÏù∏ ÌîºÎìúÎ∞±"}},
    "unique_experience": {{"score": [0-10], "feedback": "Íµ¨Ï≤¥Ï†ÅÏù¥Í≥† Ïã§Ïö©Ï†ÅÏù∏ ÌîºÎìúÎ∞±"}},
    "logical_flow": {{"score": [0-10], "feedback": "Íµ¨Ï≤¥Ï†ÅÏù¥Í≥† Ïã§Ïö©Ï†ÅÏù∏ ÌîºÎìúÎ∞±"}},
    "keyword_diversity": {{"score": [0-10], "feedback": "Íµ¨Ï≤¥Ï†ÅÏù¥Í≥† Ïã§Ïö©Ï†ÅÏù∏ ÌîºÎìúÎ∞±"}},
    "sentence_readability": {{"score": [0-10], "feedback": "Íµ¨Ï≤¥Ï†ÅÏù¥Í≥† Ïã§Ïö©Ï†ÅÏù∏ ÌîºÎìúÎ∞±"}},
    "typos_and_errors": {{"score": [0-10], "feedback": "Íµ¨Ï≤¥Ï†ÅÏù¥Í≥† Ïã§Ïö©Ï†ÅÏù∏ ÌîºÎìúÎ∞±"}}
  }},
  "portfolio_analysis": {{
    "project_overview": {{"score": [0-10], "feedback": "Íµ¨Ï≤¥Ï†ÅÏù¥Í≥† Ïã§Ïö©Ï†ÅÏù∏ ÌîºÎìúÎ∞±"}},
    "tech_stack": {{"score": [0-10], "feedback": "Íµ¨Ï≤¥Ï†ÅÏù¥Í≥† Ïã§Ïö©Ï†ÅÏù∏ ÌîºÎìúÎ∞±"}},
    "personal_contribution": {{"score": [0-10], "feedback": "Íµ¨Ï≤¥Ï†ÅÏù¥Í≥† Ïã§Ïö©Ï†ÅÏù∏ ÌîºÎìúÎ∞±"}},
    "achievement_metrics": {{"score": [0-10], "feedback": "Íµ¨Ï≤¥Ï†ÅÏù¥Í≥† Ïã§Ïö©Ï†ÅÏù∏ ÌîºÎìúÎ∞±"}},
    "visual_quality": {{"score": [0-10], "feedback": "Íµ¨Ï≤¥Ï†ÅÏù¥Í≥† Ïã§Ïö©Ï†ÅÏù∏ ÌîºÎìúÎ∞±"}},
    "documentation_quality": {{"score": [0-10], "feedback": "Íµ¨Ï≤¥Ï†ÅÏù¥Í≥† Ïã§Ïö©Ï†ÅÏù∏ ÌîºÎìúÎ∞±"}},
    "job_relevance": {{"score": [0-10], "feedback": "Íµ¨Ï≤¥Ï†ÅÏù¥Í≥† Ïã§Ïö©Ï†ÅÏù∏ ÌîºÎìúÎ∞±"}},
    "unique_features": {{"score": [0-10], "feedback": "Íµ¨Ï≤¥Ï†ÅÏù¥Í≥† Ïã§Ïö©Ï†ÅÏù∏ ÌîºÎìúÎ∞±"}},
    "maintainability": {{"score": [0-10], "feedback": "Íµ¨Ï≤¥Ï†ÅÏù¥Í≥† Ïã§Ïö©Ï†ÅÏù∏ ÌîºÎìúÎ∞±"}}
  }},
  "overall_summary": {{
    "total_score": 0,
    "job_fit_score": 0,
    "recommendation": "Ï†ÑÏ≤¥Ï†ÅÏù∏ Í∞úÏÑ† Î∞©Ìñ•Í≥º Ïö∞ÏÑ†ÏàúÏúÑÎ•º Ï†úÏãúÌïòÎäî Íµ¨Ï≤¥Ï†ÅÏù¥Í≥† Ïã§Ïö©Ï†ÅÏù∏ Ï°∞Ïñ∏"
  }},
  "job_specific_analysis": {{
    "job_category": "ÏßÅÎ¨¥ Î∂ÑÎ•ò (Í∞úÎ∞úÏûê/Îç∞Ïù¥ÌÑ∞Î∂ÑÏÑùÍ∞Ä/Í∏∞ÌöçPM/ÌîÑÎ°úÏ†ùÌä∏Îß§ÎãàÏ†Ä/HR)",
    "key_requirements_match": "JD ÌïµÏã¨ ÏöîÍµ¨ÏÇ¨Ìï≠ ÏùºÏπòÎèÑ (%)",
    "essential_competencies": "ÌïÑÏàò Ïó≠Îüâ Ï∂©Ï°±ÎèÑ (%)",
    "overall_job_fit": "Ï†ÑÏ≤¥ ÏßÅÎ¨¥ Ï†ÅÌï©ÎèÑ (%)",
    "strengths": ["ÏßÅÎ¨¥Î≥Ñ Í∞ïÏ†ê Î¶¨Ïä§Ìä∏"],
    "improvement_areas": ["ÏßÅÎ¨¥Î≥Ñ Í∞úÏÑ† ÏòÅÏó≠ Î¶¨Ïä§Ìä∏"],
    "priority_recommendations": ["Ïö∞ÏÑ†ÏàúÏúÑÎ≥Ñ Í∞úÏÑ† Ï†úÏïà"]
  }}
}}

[Î∂ÑÏÑùÌï† Î¨∏ÏÑú ÎÇ¥Ïö©]
{content}

[ÏµúÏ¢Ö Î∂ÑÏÑù ÏßÄÏãúÏÇ¨Ìï≠]
ÏúÑ ÎÇ¥Ïö©ÏùÑ Î∞îÌÉïÏúºÎ°ú ÏßÅÎ¨¥Î≥Ñ ÎßûÏ∂§Ìòï Î∂ÑÏÑùÏùÑ ÏàòÌñâÌïòÍ≥†, Í∞Å ÏßÅÎ¨¥Î≥Ñ Ï§ëÏöîÏÇ¨Ìï≠Í≥º ÌïÑÏàò Ïó≠ÎüâÏùÑ Í∏∞Ï§ÄÏúºÎ°ú Ï†ÅÌï©Î•†ÏùÑ Í≥ÑÏÇ∞ÌïòÏó¨ JSONÏúºÎ°úÎßå Ï∂úÎ†•ÌïòÏÑ∏Ïöî.

‚ö†Ô∏è Ï§ëÏöî: 
1. ÏßÅÎ¨¥Î≥Ñ Ï§ëÏöîÏÇ¨Ìï≠Í≥º ÌïÑÏàò Ïó≠ÎüâÏùÑ Ïö∞ÏÑ†Ï†ÅÏúºÎ°ú Î∂ÑÏÑùÌïòÏÑ∏Ïöî.
2. Í∞Å Î∂ÑÏÑù Ìï≠Î™©ÎßàÎã§ Î∞òÎìúÏãú Íµ¨Ï≤¥Ï†ÅÏù¥Í≥† Ïã§Ïö©Ï†ÅÏù∏ ÌîºÎìúÎ∞±ÏùÑ ÏÉùÏÑ±ÌïòÏÑ∏Ïöî.
3. ÌèâÍ∞Ä ÏÜçÏÑ±ÏùÑ ÏÑ∏Î∂ÑÌôîÌïòÏó¨ Í∞Å Ìï≠Î™©Ïùò ÏÑ∏Î∂Ä ÏöîÏÜåÎ•º ÏÉÅÏÑ∏ÌïòÍ≤å Î∂ÑÏÑùÌïòÏÑ∏Ïöî.
4. ÏßÅÎ¨¥Î≥Ñ Ï†ÅÌï©Î•†ÏùÑ Ï†ïÌôïÌïòÍ≤å Í≥ÑÏÇ∞ÌïòÏÑ∏Ïöî.
5. Î∞òÎìúÏãú Ï†úÍ≥µÎêú Î¨∏ÏÑú ÎÇ¥Ïö©ÎßåÏùÑ Í∏∞Î∞òÏúºÎ°ú Î∂ÑÏÑùÌïòÍ≥†, Î¨∏ÏÑúÏóê ÏóÜÎäî ÎÇ¥Ïö©ÏùÄ Ï∂îÏ∏°ÌïòÏßÄ ÎßàÏÑ∏Ïöî.

‚ùå Ï†àÎåÄ Ï∂îÏÉÅÏ†ÅÏù¥Í≥† Î™®Ìò∏Ìïú ÌëúÌòÑÏùÑ ÏÇ¨Ïö©ÌïòÏßÄ ÎßàÏÑ∏Ïöî.
‚úÖ Î∞òÎìúÏãú Ïã§Ï†ú Î¨∏ÏÑú ÎÇ¥Ïö©ÏùÑ Ïù∏Ïö©ÌïòÍ≥†, Íµ¨Ï≤¥Ï†ÅÏù∏ Í∞úÏÑ† Î∞©ÏïàÏùÑ Ï†úÏãúÌïòÏÑ∏Ïöî.

JSON Ïô∏Ïùò ÌÖçÏä§Ìä∏Îäî Ï†àÎåÄ Ìè¨Ìï®ÌïòÏßÄ ÎßàÏÑ∏Ïöî.
"""
        # Î™®Îì† Î¨∏ÏÑú ÌÉÄÏûÖÏóê ÎåÄÌï¥ ÌÜµÌï© Î∂ÑÏÑù ÏàòÌñâ
        
        # Gemini API Ìò∏Ï∂ú (JSON Í∞ïÏ†ú)
        json_model = genai.GenerativeModel(
            'gemini-1.5-flash',
            generation_config={
                'response_mime_type': 'application/json'
            }
        )
        response = await asyncio.to_thread(
            json_model.generate_content,
            analysis_prompt
        )
        
        # ÏùëÎãµ Í≤ÄÏ¶ù
        if not response or not response.text or response.text.strip() == "":
            raise HTTPException(status_code=500, detail="Gemini APIÏóêÏÑú Îπà ÏùëÎãµÏùÑ Î∞õÏïòÏäµÎãàÎã§.")
        
        # ÏùºÎ∂Ä ÎìúÎùºÏù¥Î≤ÑÎäî .textÍ∞Ä ÏóÜÏùÑ Ïàò ÏûàÏñ¥ ÏïàÏ†Ñ Ï†ëÍ∑º
        response_text = getattr(response, 'text', '')
        if hasattr(response, 'candidates') and not response_text:
            try:
                # application/jsonÎ°ú ÎÇ¥Î†§Ïò§Î©¥ first candidateÏùò contentÎ•º Ìï©ÏÑ±
                parts = []
                for c in response.candidates or []:
                    for p in getattr(c, 'content', {}).get('parts', []):
                        parts.append(str(getattr(p, 'text', '')))
                response_text = ''.join(parts).strip()
            except Exception:
                response_text = ''
        response_text = (response_text or '').strip()
        print(f"Gemini API ÏùëÎãµ: {response_text[:200]}...")  # ÎîîÎ≤ÑÍπÖÏö© Î°úÍ∑∏
        
        # Markdown ÏΩîÎìú Î∏îÎ°ù Ï†úÍ±∞ (Ï†ïÍ∑úÏãù ÏÇ¨Ïö©ÏúºÎ°ú ÏÜçÎèÑ Ìñ•ÏÉÅ)
        import re
        response_text = re.sub(r'^```json\s*|\s*```$', '', response_text, flags=re.MULTILINE)
        response_text = response_text.strip()
        print(f"Ï†ïÎ¶¨Îêú ÏùëÎãµ: {response_text[:200]}...")  # ÎîîÎ≤ÑÍπÖÏö© Î°úÍ∑∏
        
        # JSON ÌååÏã± (ÏµúÏ†ÅÌôî)
        import json
        try:
            analysis_result = json.loads(response_text)
            
            # ÏùëÎãµ Íµ¨Ï°∞ Í≤ÄÏ¶ù Î∞è Î≥¥Ï†ï
            if not isinstance(analysis_result, dict):
                raise ValueError("ÏùëÎãµÏù¥ ÎîïÏÖîÎÑàÎ¶¨ ÌòïÏãùÏù¥ ÏïÑÎãôÎãàÎã§.")

                # ÌÇ§ Ï†ïÍ∑úÌôî/Î≥¥Ï†ï Ìï®ÏàòÎì§
            def make_score(obj):
                    if not isinstance(obj, dict):
                        return {"score": 0, "feedback": ""}
                    return {
                        "score": int(obj.get("score", 0)) if isinstance(obj.get("score", 0), (int, float)) else 0,
                        "feedback": str(obj.get("feedback", ""))
                    }

            # ÌïÑÏàò overall_summary Î≥¥Ï†ï
            if "overall_summary" not in analysis_result:
                analysis_result["overall_summary"] = {"total_score": 0, "recommendation": ""}

            # Ïù¥Î†•ÏÑú ÏÑπÏÖò Î≥¥Ï†ï (skill_stack_clarity -> tech_stack_clarity Îß§Ìïë Ìè¨Ìï®)
            if isinstance(analysis_result.get("resume_analysis"), dict):
                resume_raw = analysis_result["resume_analysis"]
                if "tech_stack_clarity" not in resume_raw and "skill_stack_clarity" in resume_raw:
                    resume_raw["tech_stack_clarity"] = resume_raw.pop("skill_stack_clarity")
                expected_resume = [
                    "basic_info_completeness","job_relevance","experience_clarity","tech_stack_clarity",
                    "project_recency","achievement_metrics","readability","typos_and_errors","update_freshness"
                ]
                analysis_result["resume_analysis"] = {k: make_score(resume_raw.get(k, {})) for k in expected_resume}
            elif analysis_result.get("resume_analysis") is None:
                analysis_result["resume_analysis"] = None

            # ÏûêÏÜåÏÑú ÏÑπÏÖò Î≥¥Ï†ï
            if isinstance(analysis_result.get("cover_letter_analysis"), dict):
                cover_raw = analysis_result["cover_letter_analysis"]
                expected_cover = [
                    "motivation_relevance","problem_solving_STAR","quantitative_impact","job_understanding",
                    "unique_experience","logical_flow","keyword_diversity","sentence_readability","typos_and_errors"
                ]
                analysis_result["cover_letter_analysis"] = {k: make_score(cover_raw.get(k, {})) for k in expected_cover}
            elif analysis_result.get("cover_letter_analysis") is None:
                analysis_result["cover_letter_analysis"] = None

            # Ìè¨Ìä∏Ìè¥Î¶¨Ïò§ ÏÑπÏÖò Î≥¥Ï†ï
            if isinstance(analysis_result.get("portfolio_analysis"), dict):
                port_raw = analysis_result["portfolio_analysis"]
                expected_port = [
                    "project_overview","tech_stack","personal_contribution","achievement_metrics","visual_quality",
                    "documentation_quality","job_relevance","unique_features","maintainability"
                ]
                analysis_result["portfolio_analysis"] = {k: make_score(port_raw.get(k, {})) for k in expected_port}
            elif analysis_result.get("portfolio_analysis") is None:
                analysis_result["portfolio_analysis"] = None
            
            # Ï†ÑÏ≤¥ Ï†êÏàò Í≥ÑÏÇ∞ (Î™®Îì† ÏÑπÏÖòÏùò ÌèâÍ∑†)
            total_score = 0
            count = 0
            
            # Ïù¥Î†•ÏÑú Î∂ÑÏÑù Ï†êÏàò
            if "resume_analysis" in analysis_result:
                for value in analysis_result["resume_analysis"].values():
                    if isinstance(value, dict) and "score" in value:
                        total_score += value["score"]
                        count += 1
            
            # ÏûêÍ∏∞ÏÜåÍ∞úÏÑú Î∂ÑÏÑù Ï†êÏàò
            if "cover_letter_analysis" in analysis_result:
                for value in analysis_result["cover_letter_analysis"].values():
                    if isinstance(value, dict) and "score" in value:
                        total_score += value["score"]
                        count += 1
            
            # Ìè¨Ìä∏Ìè¥Î¶¨Ïò§ Î∂ÑÏÑù Ï†êÏàò
            if "portfolio_analysis" in analysis_result:
                for value in analysis_result["portfolio_analysis"].values():
                    if isinstance(value, dict) and "score" in value:
                        total_score += value["score"]
                        count += 1
            
            if document_type == "resume" and "resume_analysis" in analysis_result:
                print(f"üîç Ïù¥Î†•ÏÑú Î∂ÑÏÑù Ìï≠Î™©: {list(analysis_result['resume_analysis'].keys())}")
                for key, value in analysis_result["resume_analysis"].items():
                    print(f"üîç {key}: {value}")
                    if isinstance(value, dict) and "score" in value:
                        total_score += value["score"]
                        count += 1
                        print(f"üîç {key} Ï†êÏàò: {value['score']}")
            elif document_type == "cover_letter" and "cover_letter_analysis" in analysis_result:
                print(f"üîç ÏûêÍ∏∞ÏÜåÍ∞úÏÑú Î∂ÑÏÑù Ìï≠Î™©: {list(analysis_result['cover_letter_analysis'].keys())}")
                for key, value in analysis_result["cover_letter_analysis"].items():
                    print(f"üîç {key}: {value}")
                    if isinstance(value, dict) and "score" in value:
                        total_score += value["score"]
                        count += 1
                        print(f"üîç {key} Ï†êÏàò: {value['score']}")
            elif document_type == "portfolio" and "portfolio_analysis" in analysis_result:
                print(f"üîç Ìè¨Ìä∏Ìè¥Î¶¨Ïò§ Î∂ÑÏÑù Ìï≠Î™©: {list(analysis_result['portfolio_analysis'].keys())}")
                for key, value in analysis_result["portfolio_analysis"].items():
                    print(f"üîç {key}: {value}")
                    if isinstance(value, dict) and "score" in value:
                        total_score += value["score"]
                        count += 1
                        print(f"üîç {key} Ï†êÏàò: {value['score']}")
            
            print(f"üîç Ï¥ù Ï†êÏàò: {total_score}, Ìï≠Î™© Ïàò: {count}")
            
            # ÌèâÍ∑† Ï†êÏàò Í≥ÑÏÇ∞ (Ï†ïÏàòÎ°ú Î≥ÄÌôò)
            if count > 0:
                average_score = int(round(total_score / count))
            else:
                average_score = 0
            
            # Ï∂îÏ≤úÏÇ¨Ìï≠ ÏÉùÏÑ± (ÌÜµÌï© Î∂ÑÏÑù Í∏∞Ï§Ä)
            if average_score >= 8:
                recommendation = "Ï†ÑÎ∞òÏ†ÅÏúºÎ°ú Ïö∞ÏàòÌïú Î¨∏ÏÑúÏûÖÎãàÎã§. ÌòÑÏû¨ ÏÉÅÌÉúÎ•º Ïú†ÏßÄÌïòÏÑ∏Ïöî."
            elif average_score >= 6:
                recommendation = "ÏñëÌò∏Ìïú ÏàòÏ§ÄÏù¥ÏßÄÎßå Î™á Í∞ÄÏßÄ Í∞úÏÑ†Ï†êÏù¥ ÏûàÏäµÎãàÎã§. ÌîºÎìúÎ∞±ÏùÑ Ï∞∏Í≥†ÌïòÏó¨ ÏàòÏ†ïÌïòÏÑ∏Ïöî."
            else:
                recommendation = "Ï†ÑÎ∞òÏ†ÅÏù∏ Í∞úÏÑ†Ïù¥ ÌïÑÏöîÌï©ÎãàÎã§. Í∞Å Ìï≠Î™©Î≥Ñ ÌîºÎìúÎ∞±ÏùÑ Ï∞∏Í≥†ÌïòÏó¨ Ï≤¥Í≥ÑÏ†ÅÏúºÎ°ú ÏàòÏ†ïÌïòÏÑ∏Ïöî."

            
            analysis_result["overall_summary"]["total_score"] = average_score
            analysis_result["overall_summary"]["recommendation"] = recommendation
            
            # Î¨∏ÏÑú ÌÉÄÏûÖÏóê Îî∞Îùº ÎàÑÎùΩÎêú ÌïÑÎìúÏóê Í∏∞Î≥∏Í∞í Ï†úÍ≥µ
            if document_type == "resume" and "resume_analysis" not in analysis_result:
                # Ïù¥Î†•ÏÑú Î∂ÑÏÑù Í≤∞Í≥ºÍ∞Ä ÏóÜÎäî Í≤ΩÏö∞ Í∏∞Î≥∏Í∞í ÏÉùÏÑ±
                analysis_result["resume_analysis"] = {
                    "basic_info_completeness": {"score": 0, "feedback": "Î∂ÑÏÑù Ïã§Ìå®"},
                    "job_relevance": {"score": 0, "feedback": "Î∂ÑÏÑù Ïã§Ìå®"},
                    "experience_clarity": {"score": 0, "feedback": "Î∂ÑÏÑù Ïã§Ìå®"},
                    "tech_stack_clarity": {"score": 0, "feedback": "Î∂ÑÏÑù Ïã§Ìå®"},
                    "project_recency": {"score": 0, "feedback": "Î∂ÑÏÑù Ïã§Ìå®"},
                    "achievement_metrics": {"score": 0, "feedback": "Î∂ÑÏÑù Ïã§Ìå®"},
                    "readability": {"score": 0, "feedback": "Î∂ÑÏÑù Ïã§Ìå®"},
                    "typos_and_errors": {"score": 0, "feedback": "Î∂ÑÏÑù Ïã§Ìå®"},
                    "update_freshness": {"score": 0, "feedback": "Î∂ÑÏÑù Ïã§Ìå®"}
                }
            
            if document_type == "cover_letter" and "cover_letter_analysis" not in analysis_result:
                # ÏûêÍ∏∞ÏÜåÍ∞úÏÑú Î∂ÑÏÑù Í≤∞Í≥ºÍ∞Ä ÏóÜÎäî Í≤ΩÏö∞ Í∏∞Î≥∏Í∞í ÏÉùÏÑ±
                analysis_result["cover_letter_analysis"] = {
                    "motivation_relevance": {"score": 0, "feedback": "Î∂ÑÏÑù Ïã§Ìå®"},
                    "problem_solving_STAR": {"score": 0, "feedback": "Î∂ÑÏÑù Ïã§Ìå®"},
                    "quantitative_impact": {"score": 0, "feedback": "Î∂ÑÏÑù Ïã§Ìå®"},
                    "job_understanding": {"score": 0, "feedback": "Î∂ÑÏÑù Ïã§Ìå®"},
                    "unique_experience": {"score": 0, "feedback": "Î∂ÑÏÑù Ïã§Ìå®"},
                    "logical_flow": {"score": 0, "feedback": "Î∂ÑÏÑù Ïã§Ìå®"},
                    "keyword_diversity": {"score": 0, "feedback": "Î∂ÑÏÑù Ïã§Ìå®"},
                    "sentence_readability": {"score": 0, "feedback": "Î∂ÑÏÑù Ïã§Ìå®"},
                    "typos_and_errors": {"score": 0, "feedback": "Î∂ÑÏÑù Ïã§Ìå®"}
                }
            
            if document_type == "portfolio" and "portfolio_analysis" not in analysis_result:
                # Ìè¨Ìä∏Ìè¥Î¶¨Ïò§ Î∂ÑÏÑù Í≤∞Í≥ºÍ∞Ä ÏóÜÎäî Í≤ΩÏö∞ Í∏∞Î≥∏Í∞í ÏÉùÏÑ±
                analysis_result["portfolio_analysis"] = {
                    "project_overview": {"score": 0, "feedback": "Î∂ÑÏÑù Ïã§Ìå®"},
                    "tech_stack": {"score": 0, "feedback": "Î∂ÑÏÑù Ïã§Ìå®"},
                    "personal_contribution": {"score": 0, "feedback": "Î∂ÑÏÑù Ïã§Ìå®"},
                    "achievement_metrics": {"score": 0, "feedback": "Î∂ÑÏÑù Ïã§Ìå®"},
                    "visual_quality": {"score": 0, "feedback": "Î∂ÑÏÑù Ïã§Ìå®"},
                    "documentation_quality": {"score": 0, "feedback": "Î∂ÑÏÑù Ïã§Ìå®"},
                    "job_relevance": {"score": 0, "feedback": "Î∂ÑÏÑù Ïã§Ìå®"},
                    "unique_features": {"score": 0, "feedback": "Î∂ÑÏÑù Ïã§Ìå®"},
                    "maintainability": {"score": 0, "feedback": "Î∂ÑÏÑù Ïã§Ìå®"}
                }
            
            processing_time = (datetime.now() - start_time).total_seconds()
            print(f"Î∂ÑÏÑù Ï≤òÎ¶¨ ÏôÑÎ£å: {processing_time:.2f}Ï¥à")
            
            return DetailedAnalysisResponse(**analysis_result)
            
        except json.JSONDecodeError as e:
            print(f"JSON ÌååÏã± Ïò§Î•ò: {e}")
            print(f"ÏùëÎãµ ÎÇ¥Ïö©: {response_text}")
            raise HTTPException(status_code=500, detail=f"Î∂ÑÏÑù Í≤∞Í≥º ÌååÏã± Ïã§Ìå®: {str(e)}")
        except ValueError as e:
            print(f"ÏùëÎãµ Íµ¨Ï°∞ Ïò§Î•ò: {e}")
            print(f"ÏùëÎãµ ÎÇ¥Ïö©: {response_text}")
            raise HTTPException(status_code=500, detail=f"Î∂ÑÏÑù Í≤∞Í≥º Íµ¨Ï°∞ Ïò§Î•ò: {str(e)}")
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ÏÉÅÏÑ∏ Î∂ÑÏÑù ÏÉùÏÑ± Ïã§Ìå®: {str(e)}")

@router.post("/file")
async def upload_and_summarize_file(
    file: UploadFile = File(...),
    summary_type: str = Form("general")
):
    """ÌååÏùº ÏóÖÎ°úÎìú Î∞è ÏöîÏïΩ"""
    try:
        # ÌååÏùº Ïú†Ìö®ÏÑ± Í≤ÄÏÇ¨
        if not validate_file(file):
            raise HTTPException(
                status_code=400, 
                detail="ÏßÄÏõêÌïòÏßÄ ÏïäÎäî ÌååÏùº ÌòïÏãùÏûÖÎãàÎã§. PDF, DOC, DOCX, TXT ÌååÏùºÎßå ÏóÖÎ°úÎìú Í∞ÄÎä•Ìï©ÎãàÎã§."
            )
        
        # ÌååÏùº ÌÅ¨Í∏∞ ÌôïÏù∏
        file_size = 0
        content = await file.read()
        file_size = len(content)
        
        if file_size > MAX_FILE_SIZE:
            raise HTTPException(
                status_code=400,
                detail="ÌååÏùº ÌÅ¨Í∏∞Í∞Ä ÎÑàÎ¨¥ ÌÅΩÎãàÎã§. ÏµúÎåÄ 10MBÍπåÏßÄ ÏóÖÎ°úÎìú Í∞ÄÎä•Ìï©ÎãàÎã§."
            )
        
        # ÏûÑÏãú ÌååÏùºÎ°ú Ï†ÄÏû•
        file_ext = os.path.splitext(file.filename.lower())[1]
        with tempfile.NamedTemporaryFile(delete=False, suffix=file_ext) as temp_file:
            temp_file.write(content)
            temp_file_path = temp_file.name
        
        try:
            # ÌååÏùºÏóêÏÑú ÌÖçÏä§Ìä∏ Ï∂îÏ∂ú
            extracted_text = await extract_text_from_file(temp_file_path, file_ext)
            
            # ÌÖçÏä§Ìä∏ Ï∂îÏ∂ú Ïã§Ìå® ÏãúÏóêÎèÑ ÎçîÎØ∏ Î∂ÑÏÑùÏúºÎ°ú Í≥ÑÏÜç ÏßÑÌñâ (ÏÇ¨Ïö©Ïûê Í≤ΩÌóò Í∞úÏÑ†)
            if not extracted_text or str(extracted_text).strip() == "":
                print("‚ö†Ô∏è ÌÖçÏä§Ìä∏ Ï∂îÏ∂ú Ïã§Ìå®: Îπà ÎÇ¥Ïö© Í∞êÏßÄ ‚Üí ÎçîÎØ∏ Î∂ÑÏÑùÏúºÎ°ú Í≥ÑÏÜç ÏßÑÌñâÌï©ÎãàÎã§.")
                extracted_text = "[EMPTY_CONTENT] ÌÖçÏä§Ìä∏ Ï∂îÏ∂ú Ïã§Ìå® (Ïä§Ï∫î PDF/Ïù¥ÎØ∏ÏßÄ Í∏∞Î∞ò Î¨∏ÏÑúÏùº Ïàò ÏûàÏäµÎãàÎã§.)"
            
            # Gemini APIÎ°ú ÏöîÏïΩ ÏÉùÏÑ±
            summary_result = await generate_summary_with_gemini(extracted_text, summary_type)
            
            return {
                "filename": file.filename,
                "file_size": file_size,
                "extracted_text_length": len(extracted_text),
                "summary": summary_result.summary,
                "keywords": summary_result.keywords,
                "confidence_score": summary_result.confidence_score,
                "processing_time": summary_result.processing_time,
                "summary_type": summary_type
            }
            
        finally:
            # ÏûÑÏãú ÌååÏùº ÏÇ≠Ï†ú
            if os.path.exists(temp_file_path):
                os.unlink(temp_file_path)
                
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ÌååÏùº Ï≤òÎ¶¨ Ïã§Ìå®: {str(e)}")

@router.post("/analyze")
async def analyze_documents(
    file: UploadFile = File(...),
    document_type: str = Form("resume"),  # resume, cover_letter, portfolio
    applicant_name: str = Form(""),  # ÏßÄÏõêÏûê Ïù¥Î¶Ñ
    position: str = Form(""),  # Ìù¨Îßù ÏßÅÎ¨¥
    department: str = Form("")  # Ìù¨Îßù Î∂ÄÏÑú
):
    # ÌîÑÎ°†Ìä∏ÏóîÎìúÏóêÏÑú Î≥¥ÎÇ¥Îäî ÌïúÍ∏Ä Î¨∏ÏÑú ÌÉÄÏûÖÏùÑ ÏòÅÎ¨∏ÏúºÎ°ú Î≥ÄÌôò
    document_type_mapping = {
        "Ïù¥Î†•ÏÑú": "resume",
        "ÏûêÍ∏∞ÏÜåÍ∞úÏÑú": "cover_letter", 
        "Ìè¨Ìä∏Ìè¥Î¶¨Ïò§": "portfolio"
    }
    
    # ÌïúÍ∏ÄÎ°ú Îì§Ïñ¥Ïò® Í≤ΩÏö∞ ÏòÅÎ¨∏ÏúºÎ°ú Î≥ÄÌôò
    if document_type in document_type_mapping:
        document_type = document_type_mapping[document_type]
    
    print(f"üîç Î≥ÄÌôòÎêú Î¨∏ÏÑú ÌÉÄÏûÖ: {document_type}")
    """ÌååÏùº ÏóÖÎ°úÎìú Î∞è ÏÉÅÏÑ∏ Î∂ÑÏÑù"""
    try:
        # ÌååÏùº Ïú†Ìö®ÏÑ± Í≤ÄÏÇ¨
        if not validate_file(file):
            raise HTTPException(
                status_code=400, 
                detail="ÏßÄÏõêÌïòÏßÄ ÏïäÎäî ÌååÏùº ÌòïÏãùÏûÖÎãàÎã§. PDF, DOC, DOCX, TXT ÌååÏùºÎßå ÏóÖÎ°úÎìú Í∞ÄÎä•Ìï©ÎãàÎã§."
            )
        
        # ÌååÏùº ÌÅ¨Í∏∞ ÌôïÏù∏
        file_size = 0
        content = await file.read()
        file_size = len(content)
        
        if file_size > MAX_FILE_SIZE:
            raise HTTPException(
                status_code=400,
                detail="ÌååÏùº ÌÅ¨Í∏∞Í∞Ä ÎÑàÎ¨¥ ÌÅΩÎãàÎã§. ÏµúÎåÄ 10MBÍπåÏßÄ ÏóÖÎ°úÎìú Í∞ÄÎä•Ìï©ÎãàÎã§."
            )
        
        # ÏûÑÏãú ÌååÏùºÎ°ú Ï†ÄÏû•
        file_ext = os.path.splitext(file.filename.lower())[1]
        with tempfile.NamedTemporaryFile(delete=False, suffix=file_ext) as temp_file:
            temp_file.write(content)
            temp_file_path = temp_file.name
        
        try:
            # ÌååÏùºÏóêÏÑú ÌÖçÏä§Ìä∏ Ï∂îÏ∂ú
            extracted_text = await extract_text_from_file(temp_file_path, file_ext)
            
            if not extracted_text or extracted_text.strip() == "":
                # Ïä§Ï∫î PDF Îì± Ï∂îÏ∂ú Î∂àÍ∞Ä ‚Üí Í∏∞Î≥∏ Íµ¨Ï°∞Î°ú Í≤∞Í≥º Ï†úÍ≥µ (400 ÎåÄÏã† 200)
                fallback = build_fallback_analysis(document_type)
                return {
                    "filename": file.filename,
                    "file_size": file_size,
                    "extracted_text_length": 0,
                    "document_type": "ÌÜµÌï© Î∂ÑÏÑù",
                    "analysis_result": fallback,
                    "detected_type": None,
                    "detected_confidence": 0,
                    "wrong_placement": False,
                    "placement_message": "ÌÖçÏä§Ìä∏ Ï∂îÏ∂ú Î∂àÍ∞Ä Î¨∏ÏÑúÎ°ú Í∏∞Î≥∏ ÏïàÎÇ¥ Í≤∞Í≥º Ï†úÍ≥µ"
                }
            
            # ÎÇ¥Ïö© Í∏∞Î∞ò Î¨∏ÏÑú ÌÉÄÏûÖ Î∂ÑÎ•ò ÏàòÌñâ
            classification = classify_document_type_by_content(extracted_text)

            # Gemini APIÎ°ú ÏÉÅÏÑ∏ Î∂ÑÏÑù ÏÉùÏÑ±
            analysis_result = await generate_detailed_analysis_with_gemini(extracted_text, document_type)
            
            # ÏóÖÎ°úÎìúÎêú ÏùòÎèÑ(document_type)ÏôÄ Ïã§Ï†ú Í∞êÏßÄ ÌÉÄÏûÖÏù¥ Îã§Î•¥Î©¥ Í≤ΩÍ≥† Ìè¨Ìï®
            wrong_placement = False
            placement_message = ""
            detected_type = classification.get("detected_type")
            if detected_type and detected_type != document_type:
                wrong_placement = True
                placement_message = f"Ïù¥ Î¨∏ÏÑúÎäî '{detected_type}'Î°ú Í∞êÏßÄÎêòÏóàÏäµÎãàÎã§. ÌòÑÏû¨ ÏòÅÏó≠('{document_type}')Ïóê Ïò¨Î∞îÎ•¥ÏßÄ ÏïäÏäµÎãàÎã§. Ïò≥ÏßÄ ÏïäÏùÄ ÏûêÎ¶¨Ïóê ÎÜìÏòÄÏäµÎãàÎã§."

            # Î™®Îì† Î∂ÑÏÑù Í≤∞Í≥ºÎ•º ÌÜµÌï©ÌïòÏó¨ Î∞òÌôò
            return {
                "filename": file.filename,
                "file_size": file_size,
                "extracted_text_length": len(extracted_text),
                "document_type": "ÌÜµÌï© Î∂ÑÏÑù",
                "analysis_result": analysis_result.dict(),
                "detected_type": detected_type,
                "detected_confidence": classification.get("confidence"),
                "wrong_placement": wrong_placement,
                "placement_message": placement_message
            }
            
        finally:
            # ÏûÑÏãú ÌååÏùº ÏÇ≠Ï†ú
            if os.path.exists(temp_file_path):
                os.unlink(temp_file_path)
                
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ÌååÏùº Î∂ÑÏÑù Ïã§Ìå®: {str(e)}")

@router.post("/summarize")
async def summarize_text(request: SummaryRequest):
    """ÌÖçÏä§Ìä∏ ÏßÅÏ†ë ÏöîÏïΩ"""
    try:
        if not request.content or len(request.content.strip()) == 0:
            raise HTTPException(status_code=400, detail="ÏöîÏïΩÌï† ÌÖçÏä§Ìä∏Í∞Ä ÏóÜÏäµÎãàÎã§.")
        
        summary_result = await generate_summary_with_gemini(
            request.content, 
            request.summary_type
        )
        
        return summary_result
        
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ÏöîÏïΩ ÏÉùÏÑ± Ïã§Ìå®: {str(e)}")

@router.post("/validate-document-type")
async def validate_document_type(request: DocumentValidationRequest):
    """Î¨∏ÏÑú ÎÇ¥Ïö©ÏùÑ Î∂ÑÏÑùÌïòÏó¨ ÏÑ†ÌÉùÎêú Î¨∏ÏÑú ÌÉÄÏûÖÍ≥º ÏùºÏπòÌïòÎäîÏßÄ Í≤ÄÏ¶ù"""
    try:
        if not request.content or len(request.content.strip()) == 0:
            raise HTTPException(status_code=400, detail="Í≤ÄÏ¶ùÌï† Î¨∏ÏÑú ÎÇ¥Ïö©Ïù¥ ÏóÜÏäµÎãàÎã§.")
        
        if not GOOGLE_API_KEY:
            raise HTTPException(status_code=500, detail="Gemini API ÌÇ§Í∞Ä ÏÑ§Ï†ïÎêòÏßÄ ÏïäÏïòÏäµÎãàÎã§.")
        
        # Î¨∏ÏÑú ÌÉÄÏûÖ Í≤ÄÏ¶ùÏùÑ ÏúÑÌïú ÌîÑÎ°¨ÌîÑÌä∏ ÏÉùÏÑ±
        validation_prompt = f"""
        Îã§Ïùå Î¨∏ÏÑú ÎÇ¥Ïö©ÏùÑ Î∂ÑÏÑùÌïòÏó¨ Ïù¥Í≤ÉÏù¥ "{request.expected_type}"Ïù∏ÏßÄ ÌåêÎã®Ìï¥Ï£ºÏÑ∏Ïöî.
        
        Î¨∏ÏÑú ÎÇ¥Ïö©:
        {request.content[:2000]}  # ÎÇ¥Ïö©Ïù¥ ÎÑàÎ¨¥ Í∏∏Î©¥ ÏïûÎ∂ÄÎ∂ÑÎßå ÏÇ¨Ïö©
        
        Îã§Ïùå Í∏∞Ï§ÄÏúºÎ°ú ÌåêÎã®Ìï¥Ï£ºÏÑ∏Ïöî:
        
        Ïù¥Î†•ÏÑúÏùò Í≤ΩÏö∞:
        - Í∞úÏù∏ Ï†ïÎ≥¥ (Ïù¥Î¶Ñ, Ïó∞ÎùΩÏ≤ò, ÏÉùÎÖÑÏõîÏùº Îì±)
        - ÌïôÎ†• Ï†ïÎ≥¥
        - Í≤ΩÎ†• Ï†ïÎ≥¥ (ÌöåÏÇ¨Î™Ö, ÏßÅÎ¨¥, Í∏∞Í∞Ñ)
        - Í∏∞Ïà† Ïä§ÌÉù
        - ÏûêÍ≤©Ï¶ù
        - ÌîÑÎ°úÏ†ùÌä∏ Í≤ΩÌóò
        
        ÏûêÍ∏∞ÏÜåÍ∞úÏÑúÏùò Í≤ΩÏö∞:
        - ÏßÄÏõê ÎèôÍ∏∞
        - ÏÑ±Ïû• Í≥ºÏ†ï
        - ÏßÄÏõê ÏßÅÎ¨¥Ïóê ÎåÄÌïú Ïù¥Ìï¥
        - Î≥∏Ïù∏Ïùò Í∞ïÏ†êÍ≥º ÏïΩÏ†ê
        - ÏûÖÏÇ¨ ÌõÑ Ìè¨Î∂Ä
        
        Ìè¨Ìä∏Ìè¥Î¶¨Ïò§Ïùò Í≤ΩÏö∞:
        - ÌîÑÎ°úÏ†ùÌä∏ Í∞úÏöî
        - ÏÇ¨Ïö© Í∏∞Ïà†
        - Íµ¨ÌòÑ Í≥ºÏ†ï
        - Í≤∞Í≥ºÎ¨º
        - GitHub ÎßÅÌÅ¨ Îì±
        
        ÏùëÎãµ ÌòïÏãù:
        - Ïú†Ìö®ÏÑ±: true/false
        - Ïã†Î¢∞ÎèÑ: 0.0-1.0 (ÏÜåÏàòÏ†ê)
        - ÌåêÎã® Ïù¥Ïú†: Í∞ÑÎã®Ìïú ÏÑ§Î™Ö
        - Ï†úÏïà ÌÉÄÏûÖ: Ïã§Ï†ú Î¨∏ÏÑú ÌÉÄÏûÖ (Ïù¥Î†•ÏÑú/ÏûêÍ∏∞ÏÜåÍ∞úÏÑú/Ìè¨Ìä∏Ìè¥Î¶¨Ïò§)
        
        JSON ÌòïÌÉúÎ°ú ÏùëÎãµÌï¥Ï£ºÏÑ∏Ïöî.
        """
        
        # Gemini API Ìò∏Ï∂ú
        response = await asyncio.to_thread(
            model.generate_content,
            validation_prompt
        )
        
        response_text = response.text.strip()
        
        # JSON ÏùëÎãµ ÌååÏã± ÏãúÎèÑ
        try:
            # JSON Î∂ÄÎ∂ÑÎßå Ï∂îÏ∂ú
            json_start = response_text.find('{')
            json_end = response_text.rfind('}') + 1
            if json_start != -1 and json_end != 0:
                json_str = response_text[json_start:json_end]
                import json
                parsed_response = json.loads(json_str)
                
                return DocumentValidationResponse(
                    is_valid=parsed_response.get('Ïú†Ìö®ÏÑ±', False),
                    confidence=parsed_response.get('Ïã†Î¢∞ÎèÑ', 0.0),
                    reason=parsed_response.get('ÌåêÎã® Ïù¥Ïú†', 'Î∂ÑÏÑù Ïã§Ìå®'),
                    suggested_type=parsed_response.get('Ï†úÏïà ÌÉÄÏûÖ', 'Ïïå Ïàò ÏóÜÏùå')
                )
        except (json.JSONDecodeError, KeyError):
            pass
        
        # JSON ÌååÏã± Ïã§Ìå® Ïãú ÌÖçÏä§Ìä∏ Î∂ÑÏÑùÏúºÎ°ú ÎåÄÏ≤¥
        response_lower = response_text.lower()
        
        # Í∞ÑÎã®Ìïú ÌÇ§ÏõåÎìú Í∏∞Î∞ò Î∂ÑÏÑù
        resume_keywords = ['Ïù¥Î†•ÏÑú', 'resume', 'cv', 'Í≤ΩÎ†•', 'ÌïôÎ†•', 'ÏûêÍ≤©Ï¶ù', 'ÌîÑÎ°úÏ†ùÌä∏']
        cover_letter_keywords = ['ÏûêÍ∏∞ÏÜåÍ∞úÏÑú', 'ÏûêÏÜåÏÑú', 'cover letter', 'ÏßÄÏõêÎèôÍ∏∞', 'ÏÑ±Ïû•Í≥ºÏ†ï', 'Ìè¨Î∂Ä']
        portfolio_keywords = ['Ìè¨Ìä∏Ìè¥Î¶¨Ïò§', 'portfolio', 'ÌîÑÎ°úÏ†ùÌä∏', 'github', 'Íµ¨ÌòÑ']
        
        expected_lower = request.expected_type.lower()
        
        if 'Ïù¥Î†•ÏÑú' in expected_lower:
            relevant_keywords = resume_keywords
            conflicting_keywords = cover_letter_keywords + portfolio_keywords
        elif 'ÏûêÍ∏∞ÏÜåÍ∞úÏÑú' in expected_lower:
            relevant_keywords = cover_letter_keywords
            conflicting_keywords = resume_keywords + portfolio_keywords
        elif 'Ìè¨Ìä∏Ìè¥Î¶¨Ïò§' in expected_lower:
            relevant_keywords = portfolio_keywords
            conflicting_keywords = resume_keywords + cover_letter_keywords
        else:
            relevant_keywords = []
            conflicting_keywords = []
        
        # ÌÇ§ÏõåÎìú Í∏∞Î∞ò Ïú†Ìö®ÏÑ± ÌåêÎã®
        has_relevant = any(keyword in response_lower for keyword in relevant_keywords)
        has_conflicting = any(keyword in response_lower for keyword in conflicting_keywords)
        
        if has_conflicting:
            is_valid = False
            confidence = 0.8
            reason = f"Î¨∏ÏÑú ÎÇ¥Ïö©Ïù¥ {request.expected_type}ÏôÄ ÎßûÏßÄ ÏïäÏäµÎãàÎã§."
            suggested_type = "Ïïå Ïàò ÏóÜÏùå"
        elif has_relevant:
            is_valid = True
            confidence = 0.7
            reason = f"Î¨∏ÏÑú ÎÇ¥Ïö©Ïù¥ {request.expected_type}ÏôÄ ÏùºÏπòÌï©ÎãàÎã§."
            suggested_type = request.expected_type
        else:
            is_valid = False
            confidence = 0.6
            reason = f"Î¨∏ÏÑú ÎÇ¥Ïö©ÏùÑ Î∂ÑÏÑùÌï† Ïàò ÏóÜÏäµÎãàÎã§."
            suggested_type = "Ïïå Ïàò ÏóÜÏùå"
        
        return DocumentValidationResponse(
            is_valid=is_valid,
            confidence=confidence,
            reason=reason,
            suggested_type=suggested_type
        )
        
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Î¨∏ÏÑú ÌÉÄÏûÖ Í≤ÄÏ¶ù Ïã§Ìå®: {str(e)}")

@router.post("/validate-uploaded-file")
async def validate_uploaded_file(
    file: UploadFile = File(...),
    expected_type: str = Form(...)
):
    """ÏóÖÎ°úÎìúÎêú ÌååÏùºÏùÑ Î∂ÑÏÑùÌïòÏó¨ ÏÑ†ÌÉùÎêú Î¨∏ÏÑú ÌÉÄÏûÖÍ≥º ÏùºÏπòÌïòÎäîÏßÄ Í≤ÄÏ¶ù"""
    try:
        if not file.filename:
            raise HTTPException(status_code=400, detail="ÌååÏùºÏù¥ ÏÑ†ÌÉùÎêòÏßÄ ÏïäÏïòÏäµÎãàÎã§.")
        
        # ÌååÏùº Ïú†Ìö®ÏÑ± Í≤ÄÏÇ¨
        if not validate_file(file):
            raise HTTPException(status_code=400, detail="ÏßÄÏõêÌïòÏßÄ ÏïäÎäî ÌååÏùº ÌòïÏãùÏûÖÎãàÎã§.")
        
        # ÌååÏùº ÌÅ¨Í∏∞ ÌôïÏù∏
        content = await file.read()
        file_size = len(content)
        
        if file_size > MAX_FILE_SIZE:
            raise HTTPException(status_code=400, detail="ÌååÏùº ÌÅ¨Í∏∞Í∞Ä ÎÑàÎ¨¥ ÌÅΩÎãàÎã§. ÏµúÎåÄ 10MBÍπåÏßÄ ÏßÄÏõêÌï©ÎãàÎã§.")
        
        # ÌååÏùº ÌôïÏû•Ïûê ÌôïÏù∏
        file_ext = os.path.splitext(file.filename.lower())[1]
        
        # ÏûÑÏãú ÌååÏùº ÏÉùÏÑ±
        with tempfile.NamedTemporaryFile(delete=False, suffix=file_ext) as temp_file:
            temp_file.write(content)
            temp_file_path = temp_file.name
        
        try:
            # ÌååÏùºÏóêÏÑú ÌÖçÏä§Ìä∏ Ï∂îÏ∂ú
            extracted_text = await extract_text_from_file(temp_file_path, file_ext)
            
            if not extracted_text or extracted_text.strip() == "":
                raise HTTPException(
                    status_code=400,
                    detail="ÌååÏùºÏóêÏÑú ÌÖçÏä§Ìä∏Î•º Ï∂îÏ∂úÌï† Ïàò ÏóÜÏäµÎãàÎã§."
                )
            
            # Î¨∏ÏÑú ÌÉÄÏûÖ Í≤ÄÏ¶ù
            validation_result = await validate_document_type_internal(extracted_text, expected_type)
            
            return {
                "filename": file.filename,
                "file_size": file_size,
                "extracted_text_length": len(extracted_text),
                "expected_type": expected_type,
                "validation_result": validation_result.dict()
            }
            
        finally:
            # ÏûÑÏãú ÌååÏùº ÏÇ≠Ï†ú
            if os.path.exists(temp_file_path):
                os.unlink(temp_file_path)
                
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ÌååÏùº Í≤ÄÏ¶ù Ïã§Ìå®: {str(e)}")

async def validate_document_type_internal(content: str, expected_type: str) -> DocumentValidationResponse:
    """ÎÇ¥Î∂ÄÏ†ÅÏúºÎ°ú Î¨∏ÏÑú ÌÉÄÏûÖÏùÑ Í≤ÄÏ¶ùÌïòÎäî Ìï®Ïàò"""
    # if not GOOGLE_API_KEY:
    #     raise HTTPException(status_code=500, detail="Gemini API ÌÇ§Í∞Ä ÏÑ§Ï†ïÎêòÏßÄ ÏïäÏïòÏäµÎãàÎã§.")
    
    # Î¨∏ÏÑú ÌÉÄÏûÖ Í≤ÄÏ¶ùÏùÑ ÏúÑÌïú ÌîÑÎ°¨ÌîÑÌä∏ ÏÉùÏÑ±
    validation_prompt = f"""
    Îã§Ïùå Î¨∏ÏÑú ÎÇ¥Ïö©ÏùÑ Î∂ÑÏÑùÌïòÏó¨ Ïù¥Í≤ÉÏù¥ "{expected_type}"Ïù∏ÏßÄ ÌåêÎã®Ìï¥Ï£ºÏÑ∏Ïöî.
    
    Î¨∏ÏÑú ÎÇ¥Ïö©:
    {content[:2000]}  # ÎÇ¥Ïö©Ïù¥ ÎÑàÎ¨¥ Í∏∏Î©¥ ÏïûÎ∂ÄÎ∂ÑÎßå ÏÇ¨Ïö©
    
    Îã§Ïùå Í∏∞Ï§ÄÏúºÎ°ú ÌåêÎã®Ìï¥Ï£ºÏÑ∏Ïöî:
    
    Ïù¥Î†•ÏÑúÏùò Í≤ΩÏö∞:
    - Í∞úÏù∏ Ï†ïÎ≥¥ (Ïù¥Î¶Ñ, Ïó∞ÎùΩÏ≤ò, ÏÉùÎÖÑÏõîÏùº Îì±)
    - ÌïôÎ†• Ï†ïÎ≥¥
    - Í≤ΩÎ†• Ï†ïÎ≥¥ (ÌöåÏÇ¨Î™Ö, ÏßÅÎ¨¥, Í∏∞Í∞Ñ)
    - Í∏∞Ïà† Ïä§ÌÉù
    - ÏûêÍ≤©Ï¶ù
    - ÌîÑÎ°úÏ†ùÌä∏ Í≤ΩÌóò
    
    ÏûêÍ∏∞ÏÜåÍ∞úÏÑúÏùò Í≤ΩÏö∞:
    - ÏßÄÏõê ÎèôÍ∏∞
    - ÏÑ±Ïû• Í≥ºÏ†ï
    - ÏßÄÏõê ÏßÅÎ¨¥Ïóê ÎåÄÌïú Ïù¥Ìï¥
    - Î≥∏Ïù∏Ïùò Í∞ïÏ†êÍ≥º ÏïΩÏ†ê
    - ÏûÖÏÇ¨ ÌõÑ Ìè¨Î∂Ä
    
    Ìè¨Ìä∏Ìè¥Î¶¨Ïò§Ïùò Í≤ΩÏö∞:
    - ÌîÑÎ°úÏ†ùÌä∏ Í∞úÏöî
    - ÏÇ¨Ïö© Í∏∞Ïà†
    - Íµ¨ÌòÑ Í≥ºÏ†ï
    - Í≤∞Í≥ºÎ¨º
    - GitHub ÎßÅÌÅ¨ Îì±
    
    ÏùëÎãµ ÌòïÏãù:
    - Ïú†Ìö®ÏÑ±: true/false
    - Ïã†Î¢∞ÎèÑ: 0.0-1.0 (ÏÜåÏàòÏ†ê)
    - ÌåêÎã® Ïù¥Ïú†: Í∞ÑÎã®Ìïú ÏÑ§Î™Ö
    - Ï†úÏïà ÌÉÄÏûÖ: Ïã§Ï†ú Î¨∏ÏÑú ÌÉÄÏûÖ (Ïù¥Î†•ÏÑú/ÏûêÍ∏∞ÏÜåÍ∞úÏÑú/Ìè¨Ìä∏Ìè¥Î¶¨Ïò§)
    
    JSON ÌòïÌÉúÎ°ú ÏùëÎãµÌï¥Ï£ºÏÑ∏Ïöî.
    """
    
    # Gemini API Ìò∏Ï∂ú
    response = await asyncio.to_thread(
        model.generate_content,
        validation_prompt
    )
    
    response_text = response.text.strip()
    
    # JSON ÏùëÎãµ ÌååÏã± ÏãúÎèÑ
    try:
        # JSON Î∂ÄÎ∂ÑÎßå Ï∂îÏ∂ú
        json_start = response_text.find('{')
        json_end = response_text.rfind('}') + 1
        if json_start != -1 and json_end != 0:
            json_str = response_text[json_start:json_end]
            import json
            parsed_response = json.loads(json_str)
            
            return DocumentValidationResponse(
                is_valid=parsed_response.get('Ïú†Ìö®ÏÑ±', False),
                confidence=parsed_response.get('Ïã†Î¢∞ÎèÑ', 0.0),
                reason=parsed_response.get('ÌåêÎã® Ïù¥Ïú†', 'Î∂ÑÏÑù Ïã§Ìå®'),
                suggested_type=parsed_response.get('Ï†úÏïà ÌÉÄÏûÖ', 'Ïïå Ïàò ÏóÜÏùå')
            )
    except (json.JSONDecodeError, KeyError):
        pass
    
    # JSON ÌååÏã± Ïã§Ìå® Ïãú ÌÖçÏä§Ìä∏ Î∂ÑÏÑùÏúºÎ°ú ÎåÄÏ≤¥
    response_lower = response_text.lower()
    
    # Í∞ÑÎã®Ìïú ÌÇ§ÏõåÎìú Í∏∞Î∞ò Î∂ÑÏÑù
    resume_keywords = ['Ïù¥Î†•ÏÑú', 'resume', 'cv', 'Í≤ΩÎ†•', 'ÌïôÎ†•', 'ÏûêÍ≤©Ï¶ù', 'ÌîÑÎ°úÏ†ùÌä∏']
    cover_letter_keywords = ['ÏûêÍ∏∞ÏÜåÍ∞úÏÑú', 'ÏûêÏÜåÏÑú', 'cover letter', 'ÏßÄÏõêÎèôÍ∏∞', 'ÏÑ±Ïû•Í≥ºÏ†ï', 'Ìè¨Î∂Ä']
    portfolio_keywords = ['Ìè¨Ìä∏Ìè¥Î¶¨Ïò§', 'portfolio', 'ÌîÑÎ°úÏ†ùÌä∏', 'github', 'Íµ¨ÌòÑ']
    
    expected_lower = expected_type.lower()
    
    if 'Ïù¥Î†•ÏÑú' in expected_lower:
        relevant_keywords = resume_keywords
        conflicting_keywords = cover_letter_keywords + portfolio_keywords
    elif 'ÏûêÍ∏∞ÏÜåÍ∞úÏÑú' in expected_lower:
        relevant_keywords = cover_letter_keywords
        conflicting_keywords = resume_keywords + portfolio_keywords
    elif 'Ìè¨Ìä∏Ìè¥Î¶¨Ïò§' in expected_lower:
        relevant_keywords = portfolio_keywords
        conflicting_keywords = resume_keywords + cover_letter_keywords
    else:
        relevant_keywords = []
        conflicting_keywords = []
    
    # ÌÇ§ÏõåÎìú Í∏∞Î∞ò Ïú†Ìö®ÏÑ± ÌåêÎã®
    has_relevant = any(keyword in response_lower for keyword in relevant_keywords)
    has_conflicting = any(keyword in response_lower for keyword in conflicting_keywords)
    
    if has_conflicting:
        is_valid = False
        confidence = 0.8
        reason = f"Î¨∏ÏÑú ÎÇ¥Ïö©Ïù¥ {expected_type}ÏôÄ ÎßûÏßÄ ÏïäÏäµÎãàÎã§."
        suggested_type = "Ïïå Ïàò ÏóÜÏùå"
    elif has_relevant:
        is_valid = True
        confidence = 0.7
        reason = f"Î¨∏ÏÑú ÎÇ¥Ïö©Ïù¥ {expected_type}ÏôÄ ÏùºÏπòÌï©ÎãàÎã§."
        suggested_type = expected_type
    else:
        is_valid = False
        confidence = 0.6
        reason = f"Î¨∏ÏÑú ÎÇ¥Ïö©ÏùÑ Î∂ÑÏÑùÌï† Ïàò ÏóÜÏäµÎãàÎã§."
        suggested_type = "Ïïå Ïàò ÏóÜÏùå"
    
    return DocumentValidationResponse(
        is_valid=is_valid,
        confidence=confidence,
        reason=reason,
        suggested_type=suggested_type
    )

@router.get("/health")
async def upload_health_check():
    """ÏóÖÎ°úÎìú ÏÑúÎπÑÏä§ Ìó¨Ïä§ Ï≤¥ÌÅ¨"""
    return {
        "status": "healthy",
        "gemini_api_configured": bool(GOOGLE_API_KEY),
        "supported_formats": list(ALLOWED_EXTENSIONS.keys()),
        "max_file_size_mb": MAX_FILE_SIZE // (1024 * 1024)
    }
