from fastapi import APIRouter, HTTPException, UploadFile, File, Form
from fastapi.responses import JSONResponse
from typing import Optional, Dict, List
import os
from dotenv import load_dotenv
import tempfile
import asyncio
import aiofiles
from datetime import datetime
import sys
sys.path.append('..')  # ÏÉÅÏúÑ ÎîîÎ†âÌÜ†Î¶¨Ïùò openai_service.py ÏÇ¨Ïö©
from openai_service import OpenAIService
from pydantic import BaseModel
import re
import json # Added for JSON parsing

# .env ÌååÏùº Î°úÎìú (ÌòÑÏû¨ ÎîîÎ†âÌÜ†Î¶¨ÏóêÏÑú)
print(f"üîç upload.py ÌòÑÏû¨ ÏûëÏóÖ ÎîîÎ†âÌÜ†Î¶¨: {os.getcwd()}")
print(f"üîç upload.py .env ÌååÏùº Ï°¥Ïû¨ Ïó¨Î∂Ä: {os.path.exists('.env')}")
load_dotenv('.env')
print(f"üîç upload.py OPENAI_API_KEY Î°úÎìú ÌõÑ: {os.getenv('OPENAI_API_KEY')}")

# OpenAI API ÏÑ§Ï†ï
try:
    openai_service = OpenAIService(model_name="gpt-4o")  # gpt-4oÎ°ú Î≥ÄÍ≤Ω
    print("OpenAI ÏÑúÎπÑÏä§ Ï¥àÍ∏∞Ìôî ÏÑ±Í≥µ (GPT-4o)")
except Exception as e:
    print(f"OpenAI ÏÑúÎπÑÏä§ Ï¥àÍ∏∞Ìôî Ïã§Ìå®: {e}")
    openai_service = None

router = APIRouter(tags=["upload"])

class SummaryRequest(BaseModel):
    content: str
    summary_type: str = "general"  # general, technical, experience

class SummaryResponse(BaseModel):
    summary: str
    keywords: list[str]
    confidence_score: float
    processing_time: float

# ÏÉàÎ°úÏö¥ ÏÉÅÏÑ∏ Î∂ÑÏÑù Î™®Îç∏Îì§
class AnalysisScore(BaseModel):
    score: int  # 0-10
    feedback: str

class DocumentValidationRequest(BaseModel):
    content: str
    expected_type: str  # "Ïù¥Î†•ÏÑú", "ÏûêÍ∏∞ÏÜåÍ∞úÏÑú", "Ìè¨Ìä∏Ìè¥Î¶¨Ïò§"

class DocumentValidationResponse(BaseModel):
    is_valid: bool
    confidence: float
    reason: str
    suggested_type: str

class ResumeAnalysis(BaseModel):
    basic_info_completeness: AnalysisScore
    job_relevance: AnalysisScore
    experience_clarity: AnalysisScore
    tech_stack_clarity: AnalysisScore
    project_recency: AnalysisScore
    achievement_metrics: AnalysisScore
    readability: AnalysisScore
    typos_and_errors: AnalysisScore
    update_freshness: AnalysisScore

class CoverLetterAnalysis(BaseModel):
    motivation_relevance: AnalysisScore
    problem_solving_STAR: AnalysisScore
    quantitative_impact: AnalysisScore
    job_understanding: AnalysisScore
    unique_experience: AnalysisScore
    logical_flow: AnalysisScore
    keyword_diversity: AnalysisScore
    sentence_readability: AnalysisScore
    typos_and_errors: AnalysisScore

class PortfolioAnalysis(BaseModel):
    project_overview: AnalysisScore
    tech_stack: AnalysisScore
    personal_contribution: AnalysisScore
    achievement_metrics: AnalysisScore
    visual_quality: AnalysisScore
    documentation_quality: AnalysisScore
    job_relevance: AnalysisScore
    unique_features: AnalysisScore
    maintainability: AnalysisScore

class OverallSummary(BaseModel):
    total_score: float
    recommendation: str

class DetailedAnalysisResponse(BaseModel):
    resume_analysis: Optional[ResumeAnalysis] = None
    cover_letter_analysis: Optional[CoverLetterAnalysis] = None
    portfolio_analysis: Optional[PortfolioAnalysis] = None
    overall_summary: OverallSummary

# ===== Î∂ÑÏÑù Ïã§Ìå® Ïãú Í∏∞Î≥∏ Íµ¨Ï°∞ ÏÉùÏÑ± Ïú†Ìã∏ =====
def _build_score(msg: str) -> Dict[str, object]:
    return {"score": 0, "feedback": msg}

def build_fallback_analysis(document_type: str) -> Dict[str, object]:
    reason = "Î¨∏ÏÑúÏóêÏÑú ÌÖçÏä§Ìä∏Î•º Ï∂îÏ∂úÌï† Ïàò ÏóÜÏñ¥ ÌèâÍ∞ÄÌïòÏßÄ Î™ªÌñàÏäµÎãàÎã§. Ìé∏Ïßë Í∞ÄÎä•Ìïú PDF/DOCXÎ°ú Ïû¨ÏóÖÎ°úÎìúÌï¥Ï£ºÏÑ∏Ïöî."
    resume = {
        "basic_info_completeness": _build_score(reason),
        "job_relevance": _build_score(reason),
        "experience_clarity": _build_score(reason),
        "tech_stack_clarity": _build_score(reason),
        "project_recency": _build_score(reason),
        "achievement_metrics": _build_score(reason),
        "readability": _build_score(reason),
        "typos_and_errors": _build_score(reason),
        "update_freshness": _build_score(reason),
    }
    cover = {
        "motivation_relevance": _build_score(reason),
        "problem_solving_STAR": _build_score(reason),
        "quantitative_impact": _build_score(reason),
        "job_understanding": _build_score(reason),
        "unique_experience": _build_score(reason),
        "logical_flow": _build_score(reason),
        "keyword_diversity": _build_score(reason),
        "sentence_readability": _build_score(reason),
        "typos_and_errors": _build_score(reason),
    }
    portfolio = {
        "project_overview": _build_score(reason),
        "tech_stack": _build_score(reason),
        "personal_contribution": _build_score(reason),
        "achievement_metrics": _build_score(reason),
        "visual_quality": _build_score(reason),
        "documentation_quality": _build_score(reason),
        "job_relevance": _build_score(reason),
        "unique_features": _build_score(reason),
        "maintainability": _build_score(reason),
    }
    return {
        "resume_analysis": resume,
        "cover_letter_analysis": cover,
        "portfolio_analysis": portfolio,
        "overall_summary": {"total_score": 0, "recommendation": reason},
    }

# ===== ÎÇ¥Ïö© Í∏∞Î∞ò Î¨∏ÏÑú Ïú†Ìòï Î∂ÑÎ•òÍ∏∞ =====
def classify_document_type_by_content(text: str) -> Dict[str, object]:
    """Í∞ÑÎã®Ìïú Í∑úÏπô Í∏∞Î∞òÏúºÎ°ú Î¨∏ÏÑú Ïú†Ìòï(resume/cover_letter/portfolio)ÏùÑ Î∂ÑÎ•òÌï©ÎãàÎã§."""
    text_lower = text.lower()

    # ÌïúÍµ≠Ïñ¥/ÏòÅÏñ¥ ÌÇ§ÏõåÎìú ÏÑ∏Ìä∏
    resume_keywords = [
        "Í≤ΩÎ†•", "Ïù¥Î†•", "ÌîÑÎ°úÏ†ùÌä∏", "ÌïôÎ†•", "Í∏∞Ïà†", "Ïä§ÌÇ¨", "ÏûêÍ≤©Ï¶ù", "Í∑ºÎ¨¥", "Îã¥Îãπ", "ÏÑ±Í≥º",
        "Í≤ΩÌóò", "ÏöîÏïΩ", "ÌïµÏã¨Ïó≠Îüâ", "phone", "email", "github", "linkedin",
        "experience", "education", "skills", "projects", "certificate"
    ]
    cover_letter_keywords = [
        "ÏßÄÏõêÎèôÍ∏∞", "ÏÑ±Ïû•Î∞∞Í≤Ω", "ÏûÖÏÇ¨", "Ìè¨Î∂Ä", "Ï†ÄÎäî", "Î∞∞Ïö∞Î©∞", "ÌïòÍ≥†Ïûê", "Í∏∞Ïó¨", "Í¥ÄÏã¨",
        "ÎèôÍ∏∞", "Ïó¥Ï†ï", "Ïôú", "Ïôú Ïö∞Î¶¨", "motiv", "cover letter", "passion"
    ]
    portfolio_keywords = [
        "Ìè¨Ìä∏Ìè¥Î¶¨Ïò§", "ÏûëÌíà", "ÏãúÏó∞", "Îç∞Î™®", "ÎßÅÌÅ¨", "Ïù¥ÎØ∏ÏßÄ", "Ïä§ÏÉ∑", "Ï∫°Ï≤ò", "Î†àÌè¨ÏßÄÌÜ†Î¶¨",
        "repository", "demo", "screenshot", "figma", "behance", "dribbble"
    ]

    def score_keywords(keywords: List[str]) -> float:
        score = 0.0
        for kw in keywords:
            # Îã®Ïñ¥ Í≤ΩÍ≥Ñ Ïö∞ÏÑ†, ÏóÜÏúºÎ©¥ Ìè¨Ìï® Í≤ÄÏÇ¨
            if re.search(rf"\b{re.escape(kw)}\b", text_lower) or kw in text_lower:
                score += 1.0
        # ÏÑπÏÖò Ìó§Îçî Î≥¥ÎÑàÏä§
        section_headers = ["Í≤ΩÎ†•", "ÌïôÎ†•", "ÌîÑÎ°úÏ†ùÌä∏", "skills", "experience", "education"]
        if any(h in text for h in section_headers):
            score += 0.5
        # Ïó∞ÎùΩÏ≤ò Ìå®ÌÑ¥ Î≥¥ÎÑàÏä§ (Ïù¥Î†•ÏÑú ÏßÄÌëú)
        if re.search(r"[0-9]{2,3}-[0-9]{3,4}-[0-9]{4}", text) or re.search(r"\b[a-z0-9._%+-]+@[a-z0-9.-]+\.[a-z]{2,}\b", text_lower):
            score += 0.7
        return score

    resume_score = score_keywords(resume_keywords)
    cover_letter_score = sum(1.0 for kw in cover_letter_keywords if kw in text_lower)
    portfolio_score = sum(1.0 for kw in portfolio_keywords if kw in text_lower)

    scores = {
        "resume": resume_score,
        "cover_letter": cover_letter_score,
        "portfolio": portfolio_score,
    }

    detected_type = max(scores.items(), key=lambda x: x[1])[0]
    max_score = scores[detected_type]
    # Í∞ÑÎã®Ìïú Ïã†Î¢∞ÎèÑ Ï†ïÍ∑úÌôî (ÏµúÎåÄ 10Ï†ê Í∞ÄÏ†ï)
    confidence = min(round(max_score / 10.0, 2), 1.0)

    return {"detected_type": detected_type, "confidence": confidence, "scores": scores}

# ÌóàÏö©Îêú ÌååÏùº ÌÉÄÏûÖ
ALLOWED_EXTENSIONS = {
    '.pdf': 'application/pdf',
    '.doc': 'application/msword',
    '.docx': 'application/vnd.openxmlformats-officedocument.wordprocessingml.document',
    '.txt': 'text/plain'
}

# ÌååÏùº ÌÅ¨Í∏∞ Ï†úÌïú (50MB)
MAX_FILE_SIZE = 50 * 1024 * 1024

def validate_file(file: UploadFile) -> bool:
    """ÌååÏùº Ïú†Ìö®ÏÑ± Í≤ÄÏÇ¨"""
    if not file.filename:
        return False
    
    # ÌååÏùº ÌôïÏû•Ïûê ÌôïÏù∏
    file_ext = os.path.splitext(file.filename.lower())[1]
    if file_ext not in ALLOWED_EXTENSIONS:
        return False
    
    return True

async def extract_text_from_file(file_path: str, file_ext: str) -> str:
    """ÌååÏùºÏóêÏÑú ÌÖçÏä§Ìä∏ Ï∂îÏ∂ú (Îã§Ï§ë Î∞±ÏóÖ Ï†ÑÎûµ)"""
    try:
        if file_ext == '.txt':
            async with aiofiles.open(file_path, 'r', encoding='utf-8') as f:
                return await f.read()
        elif file_ext == '.pdf':
            # 1Ï∞®: PyPDF2
            try:
                import PyPDF2
                text = ""
                with open(file_path, 'rb') as pdf_file:
                    pdf_reader = PyPDF2.PdfReader(pdf_file)
                    for page in pdf_reader.pages:
                        extracted = page.extract_text() or ""
                        text += extracted + ("\n" if extracted else "")
                    if text.strip():
                        return text
            except Exception:
                pass
            # 2Ï∞®: pdfplumber
            try:
                import pdfplumber  # type: ignore
                text = ""
                with pdfplumber.open(file_path) as pdf:
                    for p in pdf.pages:
                        extracted = p.extract_text() or ""
                        text += extracted + ("\n" if extracted else "")
                if text.strip():
                    return text
            except Exception:
                pass
            # Ïã§Ìå® Ïãú Îπà Î¨∏ÏûêÏó¥ Î∞òÌôò
            return ""
        elif file_ext in ['.doc', '.docx']:
            # 1Ï∞®: python-docx
            try:
                from docx import Document  # type: ignore
                doc = Document(file_path)
                text = "\n".join([p.text for p in doc.paragraphs if p.text])
                if text.strip():
                    return text
            except Exception:
                pass
            # 2Ï∞®: docx2txt
            try:
                import docx2txt  # type: ignore
                text = docx2txt.process(file_path) or ""
                return text
            except Exception:
                pass
            return ""
        else:
            return ""
    except Exception as e:
        return ""

async def generate_summary_with_openai(content: str, summary_type: str = "general") -> SummaryResponse:
    """OpenAI APIÎ•º ÏÇ¨Ïö©ÌïòÏó¨ ÏöîÏïΩ ÏÉùÏÑ±"""
    if not openai_service:
        raise HTTPException(status_code=500, detail="OpenAI API ÌÇ§Í∞Ä ÏÑ§Ï†ïÎêòÏßÄ ÏïäÏïòÏäµÎãàÎã§.")
    
    start_time = datetime.now()
    
    try:
        # ÏöîÏïΩ ÌÉÄÏûÖÏóê Îî∞Î•∏ ÌîÑÎ°¨ÌîÑÌä∏ ÏÑ§Ï†ï
        prompts = {
            "general": f"""
            Îã§Ïùå Ïù¥Î†•ÏÑú/ÏûêÍ∏∞ÏÜåÍ∞úÏÑú ÎÇ¥Ïö©ÏùÑ Í∞ÑÍ≤∞ÌïòÍ≤å ÏöîÏïΩÌï¥Ï£ºÏÑ∏Ïöî:
            
            {content}
            
            ÏöîÏïΩ Ïãú Îã§Ïùå ÏÇ¨Ìï≠ÏùÑ Ìè¨Ìï®Ìï¥Ï£ºÏÑ∏Ïöî:
            1. Ï£ºÏöî Í≤ΩÎ†• Î∞è Í≤ΩÌóò
            2. ÌïµÏã¨ Í∏∞Ïà† Ïä§ÌÉù
            3. Ï£ºÏöî ÏÑ±Í≥ºÎÇò ÌîÑÎ°úÏ†ùÌä∏
            4. ÏßÄÏõê ÏßÅÎ¨¥ÏôÄÏùò Ïó∞Í¥ÄÏÑ±
            
            ÏöîÏïΩÏùÄ 200Ïûê Ïù¥ÎÇ¥Î°ú ÏûëÏÑ±Ìï¥Ï£ºÏÑ∏Ïöî.
            """,
            "technical": f"""
            Îã§Ïùå ÎÇ¥Ïö©ÏóêÏÑú Í∏∞Ïà†Ï†Å Ïó≠ÎüâÏùÑ Ï§ëÏã¨ÏúºÎ°ú ÏöîÏïΩÌï¥Ï£ºÏÑ∏Ïöî:
            
            {content}
            
            Îã§Ïùå Ìï≠Î™©Îì§ÏùÑ Ìè¨Ìï®Ìï¥Ï£ºÏÑ∏Ïöî:
            1. ÌîÑÎ°úÍ∑∏ÎûòÎ∞ç Ïñ∏Ïñ¥ Î∞è ÌîÑÎ†àÏûÑÏõåÌÅ¨
            2. Í∞úÎ∞ú ÎèÑÍµ¨ Î∞è ÌîåÎû´Ìèº
            3. ÌîÑÎ°úÏ†ùÌä∏ Í≤ΩÌóò
            4. Í∏∞Ïà†Ï†Å ÏÑ±Í≥º
            
            ÏöîÏïΩÏùÄ 150Ïûê Ïù¥ÎÇ¥Î°ú ÏûëÏÑ±Ìï¥Ï£ºÏÑ∏Ïöî.
            """,
            "experience": f"""
            Îã§Ïùå ÎÇ¥Ïö©ÏóêÏÑú Í≤ΩÎ†•Í≥º Í≤ΩÌóòÏùÑ Ï§ëÏã¨ÏúºÎ°ú ÏöîÏïΩÌï¥Ï£ºÏÑ∏Ïöî:
            
            {content}
            
            Îã§Ïùå Ìï≠Î™©Îì§ÏùÑ Ìè¨Ìï®Ìï¥Ï£ºÏÑ∏Ïöî:
            1. Ï¥ù Í≤ΩÎ†• Í∏∞Í∞Ñ
            2. Ï£ºÏöî ÌöåÏÇ¨ Î∞è ÏßÅÎ¨¥
            3. ÌïµÏã¨ ÌîÑÎ°úÏ†ùÌä∏ Í≤ΩÌóò
            4. Ï£ºÏöî ÏÑ±Í≥º Î∞è ÏóÖÏ†Å
            
            ÏöîÏïΩÏùÄ 150Ïûê Ïù¥ÎÇ¥Î°ú ÏûëÏÑ±Ìï¥Ï£ºÏÑ∏Ïöî.
            """
        }
        
        prompt = prompts.get(summary_type, prompts["general"])
        
        # OpenAI API Ìò∏Ï∂ú
        summary = await openai_service.generate_response(prompt)
        
        # ÌÇ§ÏõåÎìú Ï∂îÏ∂úÏùÑ ÏúÑÌïú Ï∂îÍ∞Ä ÏöîÏ≤≠
        keyword_prompt = f"""
        Îã§Ïùå ÏöîÏïΩÏóêÏÑú ÌïµÏã¨ ÌÇ§ÏõåÎìú 5Í∞úÎ•º Ï∂îÏ∂úÌï¥Ï£ºÏÑ∏Ïöî:
        
        {summary}
        
        ÌÇ§ÏõåÎìúÎäî ÏâºÌëúÎ°ú Íµ¨Î∂ÑÌïòÏó¨ ÎÇòÏó¥Ìï¥Ï£ºÏÑ∏Ïöî.
        """
        
        keyword_response = await openai_service.generate_response(keyword_prompt)
        
        keywords = [kw.strip() for kw in keyword_response.split(',')]
        
        processing_time = (datetime.now() - start_time).total_seconds()
        
        return SummaryResponse(
            summary=summary,
            keywords=keywords[:5],  # ÏµúÎåÄ 5Í∞ú ÌÇ§ÏõåÎìú
            confidence_score=0.85,  # Í∏∞Î≥∏ Ïã†Î¢∞ÎèÑ Ï†êÏàò
            processing_time=processing_time
        )
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ÏöîÏïΩ ÏÉùÏÑ± Ïã§Ìå®: {str(e)}")

async def generate_detailed_analysis_with_gpt4o(content: str, document_type: str = "resume") -> DetailedAnalysisResponse:
    """GPT-4oÎ•º ÏÇ¨Ïö©ÌïòÏó¨ Ïù¥Î†•ÏÑú/ÏûêÏÜåÏÑú ÏÉÅÏÑ∏ Î∂ÑÏÑù ÏÉùÏÑ±"""
    if not openai_service:
        raise HTTPException(status_code=500, detail="OpenAI API ÌÇ§Í∞Ä ÏÑ§Ï†ïÎêòÏßÄ ÏïäÏïòÏäµÎãàÎã§.")
    
    start_time = datetime.now()
    
    try:
        # Î¨∏ÏÑú ÌÉÄÏûÖÏóê Îî∞Î•∏ ÏÉÅÏÑ∏ Î∂ÑÏÑù ÌîÑÎ°¨ÌîÑÌä∏ ÏÉùÏÑ±
        if document_type == "resume":
            analysis_prompt = f"""
ÎãπÏã†ÏùÄ 15ÎÖÑ Í≤ΩÎ†•Ïùò HR Ï†ÑÎ¨∏Í∞ÄÏûÖÎãàÎã§. Îã§Ïùå Ïù¥Î†•ÏÑúÎ•º ÏÉÅÏÑ∏Ìûà Î∂ÑÏÑùÌïòÏó¨ ÌèâÍ∞ÄÌï¥Ï£ºÏÑ∏Ïöî.

[Î∂ÑÏÑùÌï† Ïù¥Î†•ÏÑú ÎÇ¥Ïö©]
{content}

[Î∂ÑÏÑù Ìï≠Î™© Î∞è ÌèâÍ∞Ä Í∏∞Ï§Ä]
1. basic_info_completeness (Í∏∞Î≥∏Ï†ïÎ≥¥ ÏôÑÏÑ±ÎèÑ): Ïó∞ÎùΩÏ≤ò, ÌïôÎ†•, Í≤ΩÎ†• Îì± Í∏∞Î≥∏ Ï†ïÎ≥¥Ïùò ÏôÑÏÑ±ÎèÑ (0-10Ï†ê)
2. job_relevance (ÏßÅÎ¨¥ Ï†ÅÌï©ÏÑ±): ÏßÄÏõê ÏßÅÎ¨¥ÏôÄÏùò Ïó∞Í¥ÄÏÑ± Î∞è Ï†ÅÌï©ÏÑ± (0-10Ï†ê)
3. experience_clarity (Í≤ΩÎ†• Î™ÖÌôïÏÑ±): Í≤ΩÎ†• ÏÇ¨Ìï≠Ïùò Íµ¨Ï≤¥ÏÑ±Í≥º Î™ÖÌôïÏÑ± (0-10Ï†ê)
4. tech_stack_clarity (Í∏∞Ïà† Ïä§ÌÉù Î™ÖÌôïÏÑ±): Í∏∞Ïà† Ïä§ÌÇ¨Ïùò Íµ¨Ï≤¥ÏÑ±Í≥º ÏàòÏ§Ä (0-10Ï†ê)
5. project_recency (ÌîÑÎ°úÏ†ùÌä∏ ÏµúÏã†ÏÑ±): ÏµúÍ∑º ÌîÑÎ°úÏ†ùÌä∏ Í≤ΩÌóòÏùò Ï†ÅÏ†àÏÑ± (0-10Ï†ê)
6. achievement_metrics (ÏÑ±Í≥º ÏßÄÌëú): Íµ¨Ï≤¥Ï†Å ÏÑ±Í≥ºÏôÄ ÏàòÏπòÌôî Ï†ïÎèÑ (0-10Ï†ê)
7. readability (Í∞ÄÎèÖÏÑ±): Î¨∏ÏÑú Íµ¨Ï°∞ÏôÄ ÏùΩÍ∏∞ Ïâ¨Ïö¥ Ï†ïÎèÑ (0-10Ï†ê)
8. typos_and_errors (Ïò§Î•ò Ï†ïÎèÑ): ÎßûÏ∂§Î≤ï, Î¨∏Î≤ï Ïò§Î•òÏùò Ï†ïÎèÑ (0-10Ï†ê)
9. update_freshness (ÏµúÏã†ÏÑ±): Ï†ïÎ≥¥Ïùò ÏµúÏã†ÏÑ±Í≥º ÏóÖÎç∞Ïù¥Ìä∏ Ï†ïÎèÑ (0-10Ï†ê)

[ÏùëÎãµ ÌòïÏãù]
Î∞òÎìúÏãú Îã§Ïùå JSON ÌòïÏãùÏúºÎ°úÎßå ÏùëÎãµÌïòÏÑ∏Ïöî:
{{
  "resume_analysis": {{
        "basic_info_completeness": {{"score": 8, "feedback": "Ïó∞ÎùΩÏ≤òÏôÄ ÌïôÎ†• Ï†ïÎ≥¥Í∞Ä ÏôÑÎ≤ΩÌïòÍ≤å Í∏∞Ïû¨Îê®"}},
        "job_relevance": {{"score": 7, "feedback": "ÏßÄÏõê ÏßÅÎ¨¥ÏôÄ Í¥ÄÎ†®Îêú Í≤ΩÌóòÏù¥ Ï†ÅÏ†àÌï®"}},
        "experience_clarity": {{"score": 8, "feedback": "Í≤ΩÎ†• ÏÇ¨Ìï≠Ïù¥ Íµ¨Ï≤¥Ï†ÅÏúºÎ°ú Í∏∞Ïà†Îê®"}},
        "tech_stack_clarity": {{"score": 9, "feedback": "Í∏∞Ïà† Ïä§ÌÇ¨Ïù¥ Î™ÖÌôïÌïòÍ≤å Ï†ïÎ¶¨Îê®"}},
        "project_recency": {{"score": 7, "feedback": "ÏµúÍ∑º 2ÎÖÑ ÎÇ¥ ÌîÑÎ°úÏ†ùÌä∏ Í≤ΩÌóò ÏûàÏùå"}},
        "achievement_metrics": {{"score": 6, "feedback": "ÏùºÎ∂Ä ÏÑ±Í≥ºÍ∞Ä ÏàòÏπòÌôîÎêòÏñ¥ ÏûàÏùå"}},
        "readability": {{"score": 8, "feedback": "Î¨∏ÏÑú Íµ¨Ï°∞Í∞Ä Ï≤¥Í≥ÑÏ†ÅÏù¥Í≥† ÏùΩÍ∏∞ Ïâ¨ÏõÄ"}},
        "typos_and_errors": {{"score": 9, "feedback": "Ïò§Î•òÍ∞Ä Í±∞Ïùò ÏóÜÏùå"}},
        "update_freshness": {{"score": 8, "feedback": "ÏµúÏã† Ï†ïÎ≥¥Î°ú ÏóÖÎç∞Ïù¥Ìä∏Îê®"}}
    }},
    "overall_summary": {{
        "total_score": 7.8,
        "recommendation": "Ï†ÑÎ∞òÏ†ÅÏúºÎ°ú Ïö∞ÏàòÌïú Ïù¥Î†•ÏÑúÏù¥ÎÇò, ÏÑ±Í≥º ÏßÄÌëúÏùò Íµ¨Ï≤¥ÌôîÍ∞Ä ÌïÑÏöîÌï©ÎãàÎã§."
    }}
}}
"""
        elif document_type == "cover_letter":
            analysis_prompt = f"""
ÎãπÏã†ÏùÄ 15ÎÖÑ Í≤ΩÎ†•Ïùò HR Ï†ÑÎ¨∏Í∞ÄÏûÖÎãàÎã§. Îã§Ïùå ÏûêÍ∏∞ÏÜåÍ∞úÏÑúÎ•º ÏÉÅÏÑ∏Ìûà Î∂ÑÏÑùÌïòÏó¨ ÌèâÍ∞ÄÌï¥Ï£ºÏÑ∏Ïöî.

[Î∂ÑÏÑùÌï† ÏûêÍ∏∞ÏÜåÍ∞úÏÑú ÎÇ¥Ïö©]
{content}

[Î∂ÑÏÑù Ìï≠Î™© Î∞è ÌèâÍ∞Ä Í∏∞Ï§Ä]
1. motivation_relevance (ÏßÄÏõêÎèôÍ∏∞ Ïó∞Í¥ÄÏÑ±): ÏßÄÏõê ÌöåÏÇ¨/ÏßÅÎ¨¥ÏôÄÏùò Ïó∞Í¥ÄÏÑ± (0-10Ï†ê)
2. problem_solving_STAR (Î¨∏Ï†úÌï¥Í≤∞ STAR): Íµ¨Ï≤¥Ï†Å ÏÇ¨Î°ÄÏôÄ STAR Íµ¨Ï°∞Ïùò ÏôÑÏÑ±ÎèÑ (0-10Ï†ê)
3. quantitative_impact (Ï†ïÎüâÏ†Å ÏÑ±Í≥º): ÏàòÏπòÌôîÎêú ÏÑ±Í≥ºÏôÄ ÏûÑÌå©Ìä∏ (0-10Ï†ê)
4. job_understanding (ÏßÅÎ¨¥ Ïù¥Ìï¥ÎèÑ): ÏßÄÏõê ÏßÅÎ¨¥Ïóê ÎåÄÌïú Ïù¥Ìï¥ÎèÑ (0-10Ï†ê)
5. unique_experience (ÎèÖÌäπÌïú Í≤ΩÌóò): Ï∞®Î≥ÑÌôîÎêú Í≤ΩÌóòÍ≥º Ïä§ÌÜ†Î¶¨ (0-10Ï†ê)
6. logical_flow (ÎÖºÎ¶¨Ï†Å ÌùêÎ¶Ñ): Î¨∏Îã® Í∞Ñ ÎÖºÎ¶¨Ï†Å Ïó∞Í≤∞ÏÑ± (0-10Ï†ê)
7. keyword_diversity (ÌÇ§ÏõåÎìú Îã§ÏñëÏÑ±): ÏßÅÎ¨¥ Í¥ÄÎ†® ÌÇ§ÏõåÎìúÏùò Ï†ÅÏ†àÏÑ± (0-10Ï†ê)
8. sentence_readability (Î¨∏Ïû• Í∞ÄÎèÖÏÑ±): Î¨∏Ïû•Ïùò Î™ÖÌôïÏÑ±Í≥º Ïù¥Ìï¥ÎèÑ (0-10Ï†ê)
9. typos_and_errors (Ïò§Î•ò Ï†ïÎèÑ): ÎßûÏ∂§Î≤ï, Î¨∏Î≤ï Ïò§Î•òÏùò Ï†ïÎèÑ (0-10Ï†ê)

[ÏùëÎãµ ÌòïÏãù]
Î∞òÎìúÏãú Îã§Ïùå JSON ÌòïÏãùÏúºÎ°úÎßå ÏùëÎãµÌïòÏÑ∏Ïöî:
{{
  "cover_letter_analysis": {{
        "motivation_relevance": {{"score": 8, "feedback": "ÏßÄÏõê ÎèôÍ∏∞Í∞Ä Î™ÖÌôïÌïòÍ≥† ÏÑ§ÎìùÎ†• ÏûàÏùå"}},
        "problem_solving_STAR": {{"score": 7, "feedback": "STAR Íµ¨Ï°∞Í∞Ä Ï†ÅÏ†àÌïòÍ≤å Íµ¨ÏÑ±Îê®"}},
        "quantitative_impact": {{"score": 6, "feedback": "ÏùºÎ∂Ä ÏÑ±Í≥ºÍ∞Ä ÏàòÏπòÌôîÎêòÏñ¥ ÏûàÏùå"}},
        "job_understanding": {{"score": 8, "feedback": "ÏßÅÎ¨¥Ïóê ÎåÄÌïú Ïù¥Ìï¥ÎèÑÍ∞Ä ÎÜíÏùå"}},
        "unique_experience": {{"score": 7, "feedback": "Ï∞®Î≥ÑÌôîÎêú Í≤ΩÌóòÏù¥ Ïûò ÎìúÎü¨ÎÇ®"}},
        "logical_flow": {{"score": 8, "feedback": "ÎÖºÎ¶¨Ï†Å ÌùêÎ¶ÑÏù¥ ÏûêÏó∞Ïä§Îü¨ÏõÄ"}},
        "keyword_diversity": {{"score": 7, "feedback": "ÏßÅÎ¨¥ Í¥ÄÎ†® ÌÇ§ÏõåÎìúÍ∞Ä Ï†ÅÏ†àÌï®"}},
        "sentence_readability": {{"score": 8, "feedback": "Î¨∏Ïû•Ïù¥ Î™ÖÌôïÌïòÍ≥† Ïù¥Ìï¥ÌïòÍ∏∞ Ïâ¨ÏõÄ"}},
        "typos_and_errors": {{"score": 9, "feedback": "Ïò§Î•òÍ∞Ä Í±∞Ïùò ÏóÜÏùå"}}
  }},
  "overall_summary": {{
        "total_score": 7.5,
        "recommendation": "Ï†ÑÎ∞òÏ†ÅÏúºÎ°ú Ïö∞ÏàòÌïú ÏûêÍ∏∞ÏÜåÍ∞úÏÑúÏù¥ÎÇò, Ï†ïÎüâÏ†Å ÏÑ±Í≥º ÌëúÌòÑÏùÑ Í∞ïÌôîÌïòÎ©¥ ÎçîÏö± Ï¢ãÍ≤†ÏäµÎãàÎã§."
    }}
}}
"""
        else:
            analysis_prompt = f"""
ÎãπÏã†ÏùÄ 15ÎÖÑ Í≤ΩÎ†•Ïùò HR Ï†ÑÎ¨∏Í∞ÄÏûÖÎãàÎã§. Îã§Ïùå Î¨∏ÏÑúÎ•º ÏÉÅÏÑ∏Ìûà Î∂ÑÏÑùÌïòÏó¨ ÌèâÍ∞ÄÌï¥Ï£ºÏÑ∏Ïöî.

[Î∂ÑÏÑùÌï† Î¨∏ÏÑú ÎÇ¥Ïö©]
{content}

[Î∂ÑÏÑù Ìï≠Î™© Î∞è ÌèâÍ∞Ä Í∏∞Ï§Ä]
1. content_completeness (ÎÇ¥Ïö© ÏôÑÏÑ±ÎèÑ): Î¨∏ÏÑú ÎÇ¥Ïö©Ïùò ÏôÑÏÑ±ÎèÑÏôÄ Ï∂©Ïã§ÏÑ± (0-10Ï†ê)
2. clarity (Î™ÖÌôïÏÑ±): ÎÇ¥Ïö©Ïùò Î™ÖÌôïÏÑ±Í≥º Ïù¥Ìï¥ÎèÑ (0-10Ï†ê)
3. relevance (Í¥ÄÎ†®ÏÑ±): Ï£ºÏ†úÏôÄÏùò Í¥ÄÎ†®ÏÑ± (0-10Ï†ê)
4. quality (ÌíàÏßà): Ï†ÑÎ∞òÏ†ÅÏù∏ Î¨∏ÏÑú ÌíàÏßà (0-10Ï†ê)

[ÏùëÎãµ ÌòïÏãù]
Î∞òÎìúÏãú Îã§Ïùå JSON ÌòïÏãùÏúºÎ°úÎßå ÏùëÎãµÌïòÏÑ∏Ïöî:
{{
  "overall_summary": {{
        "total_score": 7.5,
        "recommendation": "Ï†ÑÎ∞òÏ†ÅÏúºÎ°ú Ïö∞ÏàòÌïú Î¨∏ÏÑúÏûÖÎãàÎã§."
    }}
}}
"""

        print(f"üöÄ GPT-4o ÏÉÅÏÑ∏ Î∂ÑÏÑù ÏãúÏûë...")
        
        # GPT-4o API Ìò∏Ï∂ú
        response = await openai_service.generate_response(analysis_prompt)
        
        print(f"‚úÖ GPT-4o ÏùëÎãµ ÏàòÏã† ÏôÑÎ£å")
        
        # JSON ÌååÏã± ÏãúÎèÑ
        try:
            # JSON ÏùëÎãµÏóêÏÑú Ï∂îÏ∂ú
            json_match = re.search(r'\{.*\}', response, re.DOTALL)
            if json_match:
                json_str = json_match.group()
                result = json.loads(json_str)
                
                # Ï†ÑÏ≤¥ Ï†êÏàò Í≥ÑÏÇ∞
                if document_type == "resume" and "resume_analysis" in result:
                    scores = []
                    for field, data in result["resume_analysis"].items():
                        if isinstance(data, dict) and "score" in data:
                            scores.append(data["score"])
                    if scores:
                        result["overall_summary"]["total_score"] = sum(scores) / len(scores)
                
                elif document_type == "cover_letter" and "cover_letter_analysis" in result:
                    scores = []
                    for field, data in result["cover_letter_analysis"].items():
                        if isinstance(data, dict) and "score" in data:
                            scores.append(data["score"])
                    if scores:
                        result["overall_summary"]["total_score"] = sum(scores) / len(scores)
                
                end_time = datetime.now()
                processing_time = (end_time - start_time).total_seconds()
                print(f"‚ö° ÏÉÅÏÑ∏ Î∂ÑÏÑù ÏôÑÎ£å: {processing_time:.2f}Ï¥à")
                
                return DetailedAnalysisResponse(**result)
                
        except Exception as parse_error:
            print(f"‚ö†Ô∏è JSON ÌååÏã± Ïã§Ìå®, Í∏∞Î≥∏ ÏùëÎãµ ÏÉùÏÑ±: {parse_error}")
        
        # JSON ÌååÏã± Ïã§Ìå® Ïãú Í∏∞Î≥∏ ÏùëÎãµ ÏÉùÏÑ±
        end_time = datetime.now()
        processing_time = (end_time - start_time).total_seconds()
        print(f"‚ö° Í∏∞Î≥∏ Î∂ÑÏÑù ÏôÑÎ£å: {processing_time:.2f}Ï¥à")
        
        if document_type == "resume":
            return DetailedAnalysisResponse(
                resume_analysis=ResumeAnalysis(
                    basic_info_completeness=AnalysisScore(score=7, feedback="Í∏∞Î≥∏ Î∂ÑÏÑù ÏôÑÎ£å"),
                    job_relevance=AnalysisScore(score=7, feedback="Í∏∞Î≥∏ Î∂ÑÏÑù ÏôÑÎ£å"),
                    experience_clarity=AnalysisScore(score=7, feedback="Í∏∞Î≥∏ Î∂ÑÏÑù ÏôÑÎ£å"),
                    tech_stack_clarity=AnalysisScore(score=7, feedback="Í∏∞Î≥∏ Î∂ÑÏÑù ÏôÑÎ£å"),
                    project_recency=AnalysisScore(score=7, feedback="Í∏∞Î≥∏ Î∂ÑÏÑù ÏôÑÎ£å"),
                    achievement_metrics=AnalysisScore(score=7, feedback="Í∏∞Î≥∏ Î∂ÑÏÑù ÏôÑÎ£å"),
                    readability=AnalysisScore(score=7, feedback="Í∏∞Î≥∏ Î∂ÑÏÑù ÏôÑÎ£å"),
                    typos_and_errors=AnalysisScore(score=7, feedback="Í∏∞Î≥∏ Î∂ÑÏÑù ÏôÑÎ£å"),
                    update_freshness=AnalysisScore(score=7, feedback="Í∏∞Î≥∏ Î∂ÑÏÑù ÏôÑÎ£å")
                ),
                overall_summary=OverallSummary(total_score=7.0, recommendation="GPT-4o Î∂ÑÏÑùÏù¥ ÏôÑÎ£åÎêòÏóàÏäµÎãàÎã§.")
            )
        elif document_type == "cover_letter":
            return DetailedAnalysisResponse(
                cover_letter_analysis=CoverLetterAnalysis(
                    motivation_relevance=AnalysisScore(score=7, feedback="Í∏∞Î≥∏ Î∂ÑÏÑù ÏôÑÎ£å"),
                    problem_solving_STAR=AnalysisScore(score=7, feedback="Í∏∞Î≥∏ Î∂ÑÏÑù ÏôÑÎ£å"),
                    quantitative_impact=AnalysisScore(score=7, feedback="Í∏∞Î≥∏ Î∂ÑÏÑù ÏôÑÎ£å"),
                    job_understanding=AnalysisScore(score=7, feedback="Í∏∞Î≥∏ Î∂ÑÏÑù ÏôÑÎ£å"),
                    unique_experience=AnalysisScore(score=7, feedback="Í∏∞Î≥∏ Î∂ÑÏÑù ÏôÑÎ£å"),
                    logical_flow=AnalysisScore(score=7, feedback="Í∏∞Î≥∏ Î∂ÑÏÑù ÏôÑÎ£å"),
                    keyword_diversity=AnalysisScore(score=7, feedback="Í∏∞Î≥∏ Î∂ÑÏÑù ÏôÑÎ£å"),
                    sentence_readability=AnalysisScore(score=7, feedback="Í∏∞Î≥∏ Î∂ÑÏÑù ÏôÑÎ£å"),
                    typos_and_errors=AnalysisScore(score=7, feedback="Í∏∞Î≥∏ Î∂ÑÏÑù ÏôÑÎ£å")
                ),
                overall_summary=OverallSummary(total_score=7.0, recommendation="GPT-4o Î∂ÑÏÑùÏù¥ ÏôÑÎ£åÎêòÏóàÏäµÎãàÎã§.")
            )
        else:
            return DetailedAnalysisResponse(
                overall_summary=OverallSummary(total_score=7.0, recommendation="GPT-4o Î∂ÑÏÑùÏù¥ ÏôÑÎ£åÎêòÏóàÏäµÎãàÎã§.")
            )
        
    except Exception as e:
        end_time = datetime.now()
        processing_time = (end_time - start_time).total_seconds()
        print(f"‚ùå ÏÉÅÏÑ∏ Î∂ÑÏÑù Ïã§Ìå® ({processing_time:.2f}Ï¥à): {e}")
        raise HTTPException(status_code=500, detail=f"ÏÉÅÏÑ∏ Î∂ÑÏÑù Ïã§Ìå®: {str(e)}")

@router.post("/file")
async def upload_and_summarize_file(
    file: UploadFile = File(...),
    summary_type: str = Form("general")
):
    """ÌååÏùº ÏóÖÎ°úÎìú Î∞è ÏöîÏïΩ"""
    try:
        # ÌååÏùº Ïú†Ìö®ÏÑ± Í≤ÄÏÇ¨
        if not validate_file(file):
            raise HTTPException(
                status_code=400, 
                detail="ÏßÄÏõêÌïòÏßÄ ÏïäÎäî ÌååÏùº ÌòïÏãùÏûÖÎãàÎã§. PDF, DOC, DOCX, TXT ÌååÏùºÎßå ÏóÖÎ°úÎìú Í∞ÄÎä•Ìï©ÎãàÎã§."
            )
        
        # ÌååÏùº ÌÅ¨Í∏∞ ÌôïÏù∏
        file_size = 0
        content = await file.read()
        file_size = len(content)
        
        if file_size > MAX_FILE_SIZE:
            raise HTTPException(
                status_code=400,
                detail="ÌååÏùº ÌÅ¨Í∏∞Í∞Ä ÎÑàÎ¨¥ ÌÅΩÎãàÎã§. ÏµúÎåÄ 50MBÍπåÏßÄ ÏóÖÎ°úÎìú Í∞ÄÎä•Ìï©ÎãàÎã§."
            )
        
        # ÏûÑÏãú ÌååÏùºÎ°ú Ï†ÄÏû•
        file_ext = os.path.splitext(file.filename.lower())[1]
        with tempfile.NamedTemporaryFile(delete=False, suffix=file_ext) as temp_file:
            temp_file.write(content)
            temp_file_path = temp_file.name
        
        try:
            # ÌååÏùºÏóêÏÑú ÌÖçÏä§Ìä∏ Ï∂îÏ∂ú
            extracted_text = await extract_text_from_file(temp_file_path, file_ext)
            
            # ÌÖçÏä§Ìä∏ Ï∂îÏ∂ú Ïã§Ìå® ÏãúÏóêÎèÑ ÎçîÎØ∏ Î∂ÑÏÑùÏúºÎ°ú Í≥ÑÏÜç ÏßÑÌñâ (ÏÇ¨Ïö©Ïûê Í≤ΩÌóò Í∞úÏÑ†)
            if not extracted_text or str(extracted_text).strip() == "":
                print("‚ö†Ô∏è ÌÖçÏä§Ìä∏ Ï∂îÏ∂ú Ïã§Ìå®: Îπà ÎÇ¥Ïö© Í∞êÏßÄ ‚Üí ÎçîÎØ∏ Î∂ÑÏÑùÏúºÎ°ú Í≥ÑÏÜç ÏßÑÌñâÌï©ÎãàÎã§.")
                extracted_text = "[EMPTY_CONTENT] ÌÖçÏä§Ìä∏ Ï∂îÏ∂ú Ïã§Ìå® (Ïä§Ï∫î PDF/Ïù¥ÎØ∏ÏßÄ Í∏∞Î∞ò Î¨∏ÏÑúÏùº Ïàò ÏûàÏäµÎãàÎã§.)"
            
            # OpenAI APIÎ°ú ÏöîÏïΩ ÏÉùÏÑ±
            summary_result = await generate_summary_with_openai(extracted_text, summary_type)
            
            return {
                "filename": file.filename,
                "file_size": file_size,
                "extracted_text_length": len(extracted_text),
                "summary": summary_result.summary,
                "keywords": summary_result.keywords,
                "confidence_score": summary_result.confidence_score,
                "processing_time": summary_result.processing_time,
                "summary_type": summary_type
            }
            
        finally:
            # ÏûÑÏãú ÌååÏùº ÏÇ≠Ï†ú
            if os.path.exists(temp_file_path):
                os.unlink(temp_file_path)
                
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ÌååÏùº Ï≤òÎ¶¨ Ïã§Ìå®: {str(e)}")

@router.post("/summarize")
async def summarize_text(request: SummaryRequest):
    """ÌÖçÏä§Ìä∏ ÏßÅÏ†ë ÏöîÏïΩ"""
    try:
        if not request.content or len(request.content.strip()) == 0:
            raise HTTPException(status_code=400, detail="ÏöîÏïΩÌï† ÌÖçÏä§Ìä∏Í∞Ä ÏóÜÏäµÎãàÎã§.")
        
        summary_result = await generate_summary_with_openai(
            request.content, 
            request.summary_type
        )
        
        return summary_result
        
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ÏöîÏïΩ ÏÉùÏÑ± Ïã§Ìå®: {str(e)}")

@router.get("/health")
async def upload_health_check():
    """ÏóÖÎ°úÎìú ÏÑúÎπÑÏä§ Ìó¨Ïä§ Ï≤¥ÌÅ¨"""
    return {
        "status": "healthy",
        "openai_api_configured": bool(openai_service),
        "supported_formats": list(ALLOWED_EXTENSIONS.keys()),
        "max_file_size_mb": MAX_FILE_SIZE // (1024 * 1024)
    }

@router.post("/analyze")
async def analyze_document(
    file: UploadFile = File(...),
    analysis_type: str = Form("resume")  # resume, cover_letter, portfolio
):
    """Î¨∏ÏÑú ÏÉÅÏÑ∏ Î∂ÑÏÑù (GPT-4o ÏÇ¨Ïö©)"""
    try:
        # ÌååÏùº Ïú†Ìö®ÏÑ± Í≤ÄÏÇ¨
        if not validate_file(file):
            raise HTTPException(
                status_code=400, 
                detail="ÏßÄÏõêÌïòÏßÄ ÏïäÎäî ÌååÏùº ÌòïÏãùÏûÖÎãàÎã§. PDF, DOC, DOCX, TXT ÌååÏùºÎßå ÏóÖÎ°úÎìú Í∞ÄÎä•Ìï©ÎãàÎã§."
            )
        
        # ÌååÏùº ÌÅ¨Í∏∞ ÌôïÏù∏
        file_size = 0
        content = await file.read()
        file_size = len(content)
        
        if file_size > MAX_FILE_SIZE:
            raise HTTPException(
                status_code=400,
                detail="ÌååÏùº ÌÅ¨Í∏∞Í∞Ä ÎÑàÎ¨¥ ÌÅΩÎãàÎã§. ÏµúÎåÄ 50MBÍπåÏßÄ ÏóÖÎ°úÎìú Í∞ÄÎä•Ìï©ÎãàÎã§."
            )
        
        # ÏûÑÏãú ÌååÏùºÎ°ú Ï†ÄÏû•
        file_ext = os.path.splitext(file.filename.lower())[1]
        with tempfile.NamedTemporaryFile(delete=False, suffix=file_ext) as temp_file:
            temp_file.write(content)
            temp_file_path = temp_file.name
        
        try:
            # ÌååÏùºÏóêÏÑú ÌÖçÏä§Ìä∏ Ï∂îÏ∂ú
            extracted_text = await extract_text_from_file(temp_file_path, file_ext)
            
            # ÌÖçÏä§Ìä∏ Ï∂îÏ∂ú Ïã§Ìå® ÏãúÏóêÎèÑ ÎçîÎØ∏ Î∂ÑÏÑùÏúºÎ°ú Í≥ÑÏÜç ÏßÑÌñâ
            if not extracted_text or str(extracted_text).strip() == "":
                print("‚ö†Ô∏è ÌÖçÏä§Ìä∏ Ï∂îÏ∂ú Ïã§Ìå®: Îπà ÎÇ¥Ïö© Í∞êÏßÄ ‚Üí ÎçîÎØ∏ Î∂ÑÏÑùÏúºÎ°ú Í≥ÑÏÜç ÏßÑÌñâÌï©ÎãàÎã§.")
                extracted_text = "[EMPTY_CONTENT] ÌÖçÏä§Ìä∏ Ï∂îÏ∂ú Ïã§Ìå® (Ïä§Ï∫î PDF/Ïù¥ÎØ∏ÏßÄ Í∏∞Î∞ò Î¨∏ÏÑúÏùº Ïàò ÏûàÏäµÎãàÎã§.)"
            
            # GPT-4oÎ°ú ÏÉÅÏÑ∏ Î∂ÑÏÑù ÏÉùÏÑ±
            analysis_result = await generate_detailed_analysis_with_gpt4o(extracted_text, analysis_type)
            
            return {
                "filename": file.filename,
                "file_size": file_size,
                "extracted_text_length": len(extracted_text),
                "analysis_type": analysis_type,
                "analysis_result": analysis_result.dict()
            }
            
        finally:
            # ÏûÑÏãú ÌååÏùº ÏÇ≠Ï†ú
            if os.path.exists(temp_file_path):
                os.unlink(temp_file_path)
                
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Î¨∏ÏÑú Î∂ÑÏÑù Ïã§Ìå®: {str(e)}")

@router.post("/analyze-text")
async def analyze_text_content(
    request: SummaryRequest,
    analysis_type: str = Form("resume")
):
    """ÌÖçÏä§Ìä∏ ÎÇ¥Ïö© ÏÉÅÏÑ∏ Î∂ÑÏÑù (GPT-4o ÏÇ¨Ïö©)"""
    try:
        if not request.content or len(request.content.strip()) == 0:
            raise HTTPException(status_code=400, detail="Î∂ÑÏÑùÌï† ÌÖçÏä§Ìä∏Í∞Ä ÏóÜÏäµÎãàÎã§.")
        
        # GPT-4oÎ°ú ÏÉÅÏÑ∏ Î∂ÑÏÑù ÏÉùÏÑ±
        analysis_result = await generate_detailed_analysis_with_gpt4o(
            request.content, 
            analysis_type
        )
        
        return {
            "content_length": len(request.content),
            "analysis_type": analysis_type,
            "analysis_result": analysis_result.dict()
        }
        
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ÌÖçÏä§Ìä∏ Î∂ÑÏÑù Ïã§Ìå®: {str(e)}")